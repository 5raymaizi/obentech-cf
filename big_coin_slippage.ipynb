{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# df显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.8f}'.format)\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from dcdl_amps_csv import *\n",
    "def analyze_slippage_backtest(file_path):\n",
    "    \"\"\"\n",
    "    Analyze slippage distribution from a JSON file containing trading data.\n",
    "    Each line of the file is a separate JSON object.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSON file\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Lists to store slippage values and related data\n",
    "    slippage_values = []\n",
    "    slippage_by_type = defaultdict(list)\n",
    "    slippage_by_volume = defaultdict(list)\n",
    "    \n",
    "    # Read and process the file line by line\n",
    "    line_count = 0\n",
    "    valid_slippage_count = 0\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_count += 1\n",
    "            \n",
    "            try:\n",
    "                # Parse JSON object\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                # Check if slippage is available and not null\n",
    "                if 'slippage' in data and data['slippage'] is not None:\n",
    "                    slippage = data['slippage']\n",
    "                    trade_type = data['type']\n",
    "                    volume = data['volume']\n",
    "                    \n",
    "                    # Store slippage value\n",
    "                    slippage_values.append(slippage)\n",
    "                    slippage_by_type[trade_type].append(slippage)\n",
    "                    slippage_by_volume[volume].append(slippage)\n",
    "                    \n",
    "                    valid_slippage_count += 1\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error parsing JSON at line {line_count}\")\n",
    "                continue\n",
    "    \n",
    "    if not slippage_values:\n",
    "        print(\"No valid slippage values found.\")\n",
    "        return\n",
    "    \n",
    "    # Convert to numpy array for analysis\n",
    "    slippage_array = np.array(slippage_values)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'count': len(slippage_array),\n",
    "        'mean': np.mean(slippage_array),\n",
    "        'median': np.median(slippage_array),\n",
    "        'std': np.std(slippage_array),\n",
    "        'min': np.min(slippage_array),\n",
    "        'max': np.max(slippage_array),\n",
    "        'percentile_1': np.percentile(slippage_array, 1),\n",
    "        'percentile_5': np.percentile(slippage_array, 5),\n",
    "        'percentile_10': np.percentile(slippage_array, 10),\n",
    "        'percentile_25': np.percentile(slippage_array, 25),\n",
    "        'percentile_50': np.percentile(slippage_array, 50),\n",
    "        'percentile_55': np.percentile(slippage_array, 55),        \n",
    "        'percentile_60': np.percentile(slippage_array, 60),\n",
    "        'percentile_65': np.percentile(slippage_array, 65),\n",
    "        'percentile_75': np.percentile(slippage_array, 75),\n",
    "        'percentile_95': np.percentile(slippage_array, 95),\n",
    "        'percentile_99': np.percentile(slippage_array, 99)\n",
    "    }\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"Slippage Statistics:\")\n",
    "    print(f\"Count: {stats['count']}\")\n",
    "    print(f\"Mean: {stats['mean']:.8f}\")\n",
    "    print(f\"Median: {stats['median']:.8f}\")\n",
    "    print(f\"Standard Deviation: {stats['std']:.8f}\")\n",
    "    print(f\"Min: {stats['min']:.8f}\")\n",
    "    print(f\"Max: {stats['max']:.8f}\")\n",
    "    print(f\"1st Percentile: {stats['percentile_1']:.8f}\")\n",
    "    print(f\"5th Percentile: {stats['percentile_5']:.8f}\")\n",
    "    print(f\"10th Percentile: {stats['percentile_10']:.8f}\")\n",
    "    print(f\"25th Percentile: {stats['percentile_25']:.8f}\")\n",
    "    print(f\"50th Percentile: {stats['percentile_50']:.8f}\")\n",
    "    print(f\"55th Percentile: {stats['percentile_55']:.8f}\")\n",
    "    print(f\"60th Percentile: {stats['percentile_60']:.8f}\")\n",
    "    print(f\"65th Percentile: {stats['percentile_65']:.8f}\")\n",
    "    print(f\"75th Percentile: {stats['percentile_75']:.8f}\")\n",
    "    print(f\"95th Percentile: {stats['percentile_95']:.8f}\")\n",
    "    print(f\"99th Percentile: {stats['percentile_99']:.8f}\")\n",
    "    \n",
    "    # Statistics by trade type\n",
    "    print(\"\\nSlippage by Trade Type:\")\n",
    "    for trade_type, values in slippage_by_type.items():\n",
    "        values_array = np.array(values)\n",
    "        print(f\"{trade_type}:\")\n",
    "        print(f\"  Count: {len(values_array)}\")\n",
    "        print(f\"  Mean: {np.mean(values_array):.8f}\")\n",
    "        print(f\"  Median: {np.median(values_array):.8f}\")\n",
    "        print(f\"  Standard Deviation: {np.std(values_array):.8f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualizations(slippage_array, slippage_by_type, slippage_by_volume)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def create_visualizations(slippage_array, slippage_by_type, slippage_by_volume):\n",
    "    \"\"\"\n",
    "    Create three visualizations in one row:\n",
    "    1. Histogram of all slippage values\n",
    "    2. Histogram of slippage for trade type = Maker_ask\n",
    "    3. Histogram of slippage for trade type = Maker_bid\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    type_labels = {\n",
    "        \"all\": \"All Slippage Distribution\",\n",
    "        \"Maker_ask\": \"Type = Maker_ask (主动买单/挂卖)\",\n",
    "        \"Maker_bid\": \"Type = Maker_bid (主动卖单/挂买)\"\n",
    "    }\n",
    "    colors = {\n",
    "        \"all\": \"blue\",\n",
    "        \"Maker_ask\": \"green\",\n",
    "        \"Maker_bid\": \"red\"\n",
    "    }\n",
    "\n",
    "    # Create a single row with 3 subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # 1. All slippage\n",
    "    axes[0].hist(slippage_array, bins=50, alpha=0.7, color=colors[\"all\"])\n",
    "    axes[0].set_title(type_labels[\"all\"])\n",
    "    axes[0].set_xlabel('Slippage')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Maker_ask\n",
    "    if \"Maker_ask\" in slippage_by_type and len(slippage_by_type[\"Maker_ask\"]) > 0:\n",
    "        axes[1].hist(slippage_by_type[\"Maker_ask\"], bins=50, alpha=0.7, color=colors[\"Maker_ask\"])\n",
    "    axes[1].set_title('Ask Slippage Distribution')\n",
    "    axes[1].set_xlabel('Slippage')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Maker_bid\n",
    "    if \"Maker_bid\" in slippage_by_type and len(slippage_by_type[\"Maker_bid\"]) > 0:\n",
    "        axes[2].hist(slippage_by_type[\"Maker_bid\"], bins=50, alpha=0.7, color=colors[\"Maker_bid\"])\n",
    "    axes[2].set_title('Bid Slippage Distribution')\n",
    "    axes[2].set_xlabel('Slippage')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('slippage_hist_row.png')\n",
    "    plt.show()\n",
    "def analyze_slippage(file_path, file_path2 = None,file_path3 = None,  starttime=None, endtime=None):\n",
    "    \"\"\"\n",
    "    分析滑点数据的函数\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): CSV文件路径\n",
    "    starttime (str or pd.Timestamp, optional): 筛选大于该时间的数据，格式如'2025-07-08 00:00:00'\n",
    "    endtime (str or pd.Timestamp, optional): 筛选小于该时间的数据，格式如'2025-07-08 23:59:59'\n",
    "\n",
    "    Returns:\n",
    "    tuple: (slippage_stats, sell_stats, buy_stats, df_processed, slippage_percentiles)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(file_path)\n",
    "    if file_path2 is not None:\n",
    "        df2 = pd.read_csv(file_path2)\n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "\n",
    "    if file_path3 is not None:\n",
    "        df3 = pd.read_csv(file_path3)\n",
    "        df = pd.concat([df, df3], ignore_index=True)\n",
    "    df = df[df.Order2FilledPrice!=0]\n",
    "\n",
    "    # 计算滑点\n",
    "    df['SR'] = df['Price']/df['Order2FilledPrice']-1\n",
    "    df['slippage'] = df['SR']-df['ESR']\n",
    "    df['sign'] = df['Side'].apply(lambda x: 1 if x == 'sell' else -1)\n",
    "    df['slippage'] = df['slippage']*df['sign']\n",
    "    df['TimeUsed'] = (pd.to_datetime(df['Order2Timestamp']) - pd.to_datetime(df['Timestamp'])).dt.total_seconds()\n",
    "    df['HedgingTimeUsed'] = (pd.to_datetime(df['Order2Timestamp']) - pd.to_datetime(df['Timestamp'])).dt.total_seconds()    \n",
    "    df['Amount'] = df['AmountFilled']*df['AveragePrice']\n",
    "    df = df.drop_duplicates(subset=['OrderID'])\n",
    "\n",
    "    # 筛选大于starttime和小于endtime的数据，精确到秒\n",
    "    if starttime is not None or endtime is not None:\n",
    "        df['Timestamp_dt'] = pd.to_datetime(df['Timestamp'])\n",
    "        if starttime is not None:\n",
    "            if not isinstance(starttime, pd.Timestamp):\n",
    "                starttime = pd.to_datetime(starttime)\n",
    "            starttime = starttime.replace(microsecond=0)\n",
    "            df = df[df['Timestamp_dt'] > starttime]\n",
    "        if endtime is not None:\n",
    "            if not isinstance(endtime, pd.Timestamp):\n",
    "                endtime = pd.to_datetime(endtime)\n",
    "            endtime = endtime.replace(microsecond=0)\n",
    "            df = df[df['Timestamp_dt'] < endtime]\n",
    "        df = df.drop(columns=['Timestamp_dt'])\n",
    "\n",
    "    # 获取滑点统计信息\n",
    "    slippage_stats = df['slippage'].describe()\n",
    "    df= df[df['slippage'].notna()]\n",
    "    # 增加百分位统计\n",
    "    slippage_array = df['slippage'].values\n",
    "    stats = {\n",
    "        'count': len(slippage_array),\n",
    "        'mean': np.mean(slippage_array),\n",
    "        'median': np.median(slippage_array),\n",
    "        'std': np.std(slippage_array),\n",
    "        'min': np.min(slippage_array),\n",
    "        'max': np.max(slippage_array),\n",
    "        'percentile_1': np.percentile(slippage_array, 1),\n",
    "        'percentile_5': np.percentile(slippage_array, 5),\n",
    "        'percentile_10': np.percentile(slippage_array, 10),\n",
    "        'percentile_25': np.percentile(slippage_array, 25),\n",
    "        'percentile_50': np.percentile(slippage_array, 50),\n",
    "        'percentile_55': np.percentile(slippage_array, 55),        \n",
    "        'percentile_60': np.percentile(slippage_array, 60),\n",
    "        'percentile_65': np.percentile(slippage_array, 65),\n",
    "        'percentile_75': np.percentile(slippage_array, 75),\n",
    "        'percentile_95': np.percentile(slippage_array, 95),\n",
    "        'percentile_99': np.percentile(slippage_array, 99)\n",
    "    }\n",
    "    \n",
    "    # 创建子图\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 8))\n",
    "\n",
    "    # 分析卖出滑点\n",
    "    sell_slippage = df[df.Side=='sell']['slippage']\n",
    "    sell_slippage.hist(ax=ax1, bins=30, alpha=0.7, color='red')\n",
    "    ax1.set_title('Sell Slippage Distribution')\n",
    "    ax1.set_xlabel('Slippage')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    sell_mean = sell_slippage.mean()\n",
    "    sell_std = sell_slippage.std()\n",
    "    ax1.axvline(sell_mean, color='red', linestyle='--', label=f'Mean: {sell_mean:.6f}')\n",
    "    ax1.legend()\n",
    "    # print(f\"Sell slippage - Mean: {sell_mean:.6f}, Std: {sell_std:.6f}\")\n",
    "\n",
    "    # 卖方向分位数\n",
    "    sell_percentiles = {\n",
    "        'count': len(sell_slippage),\n",
    "        'mean': np.mean(sell_slippage) if len(sell_slippage) > 0 else np.nan,\n",
    "        'median': np.median(sell_slippage) if len(sell_slippage) > 0 else np.nan,\n",
    "        'std': np.std(sell_slippage) if len(sell_slippage) > 0 else np.nan,\n",
    "        'min': np.min(sell_slippage) if len(sell_slippage) > 0 else np.nan,\n",
    "        'max': np.max(sell_slippage) if len(sell_slippage) > 0 else np.nan,\n",
    "        'percentile_1': np.percentile(sell_slippage, 1) if len(sell_slippage) > 0 else np.nan,\n",
    "        'percentile_5': np.percentile(sell_slippage, 5) if len(sell_slippage) > 0 else np.nan,\n",
    "        'percentile_10': np.percentile(sell_slippage, 10) if len(sell_slippage) > 0 else np.nan,\n",
    "        'percentile_25': np.percentile(sell_slippage, 25) if len(sell_slippage) > 0 else np.nan,\n",
    "        'percentile_50': np.percentile(sell_slippage, 50) if len(sell_slippage) > 0 else np.nan,\n",
    "        'percentile_55': np.percentile(sell_slippage, 55) if len(sell_slippage) > 0 else np.nan,\n",
    "        'percentile_60': np.percentile(sell_slippage, 60) if len(sell_slippage) > 0 else np.nan,\n",
    "        'percentile_65': np.percentile(sell_slippage, 65) if len(sell_slippage) > 0 else np.nan,\n",
    "        'percentile_75': np.percentile(sell_slippage, 75) if len(sell_slippage) > 0 else np.nan,\n",
    "        'percentile_95': np.percentile(sell_slippage, 95) if len(sell_slippage) > 0 else np.nan,\n",
    "        'percentile_99': np.percentile(sell_slippage, 99) if len(sell_slippage) > 0 else np.nan,\n",
    "    }\n",
    "    print(\"Sell slippage percentiles:\")\n",
    "    for k, v in sell_percentiles.items():\n",
    "        print(f\"  {k}: {v:.8f}\")\n",
    "\n",
    "    # 分析买入滑点\n",
    "    buy_slippage = df[df.Side=='buy']['slippage']\n",
    "    buy_slippage.hist(ax=ax2, bins=30, alpha=0.7, color='blue')\n",
    "    ax2.set_title('Buy Slippage Distribution')\n",
    "    ax2.set_xlabel('Slippage')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    buy_mean = buy_slippage.mean()\n",
    "    buy_std = buy_slippage.std()\n",
    "    ax2.axvline(buy_mean, color='blue', linestyle='--', label=f'Mean: {buy_mean:.6f}')\n",
    "    ax2.legend()\n",
    "    # print(f\"Buy slippage - Mean: {buy_mean:.6f}, Std: {buy_std:.6f}\")\n",
    "\n",
    "    # 买方向分位数\n",
    "    buy_percentiles = {\n",
    "        'count': len(buy_slippage),\n",
    "        'mean': np.mean(buy_slippage) if len(buy_slippage) > 0 else np.nan,\n",
    "        'median': np.median(buy_slippage) if len(buy_slippage) > 0 else np.nan,\n",
    "        'std': np.std(buy_slippage) if len(buy_slippage) > 0 else np.nan,\n",
    "        'min': np.min(buy_slippage) if len(buy_slippage) > 0 else np.nan,\n",
    "        'max': np.max(buy_slippage) if len(buy_slippage) > 0 else np.nan,\n",
    "        'percentile_1': np.percentile(buy_slippage, 1) if len(buy_slippage) > 0 else np.nan,\n",
    "        'percentile_5': np.percentile(buy_slippage, 5) if len(buy_slippage) > 0 else np.nan,\n",
    "        'percentile_10': np.percentile(buy_slippage, 10) if len(buy_slippage) > 0 else np.nan,\n",
    "        'percentile_25': np.percentile(buy_slippage, 25) if len(buy_slippage) > 0 else np.nan,\n",
    "        'percentile_50': np.percentile(buy_slippage, 50) if len(buy_slippage) > 0 else np.nan,\n",
    "        'percentile_55': np.percentile(buy_slippage, 55) if len(buy_slippage) > 0 else np.nan,\n",
    "        'percentile_60': np.percentile(buy_slippage, 60) if len(buy_slippage) > 0 else np.nan,\n",
    "        'percentile_65': np.percentile(buy_slippage, 65) if len(buy_slippage) > 0 else np.nan,\n",
    "        'percentile_75': np.percentile(buy_slippage, 75) if len(buy_slippage) > 0 else np.nan,\n",
    "        'percentile_95': np.percentile(buy_slippage, 95) if len(buy_slippage) > 0 else np.nan,\n",
    "        'percentile_99': np.percentile(buy_slippage, 99) if len(buy_slippage) > 0 else np.nan,\n",
    "    }\n",
    "    print(\"Buy slippage percentiles:\")\n",
    "    for k, v in buy_percentiles.items():\n",
    "        print(f\"  {k}: {v:.8f}\")\n",
    "\n",
    "    # overall滑点分布\n",
    "    df['slippage'].hist(ax=ax3, bins=30, alpha=0.7, color='green')\n",
    "    ax3.set_title('Overall Slippage Distribution')\n",
    "    ax3.set_xlabel('Slippage')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    overall_mean = df['slippage'].mean()\n",
    "    ax3.axvline(overall_mean, color='green', linestyle='--', label=f'Mean: {overall_mean:.6f}')\n",
    "    ax3.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Overall slippage percentiles:\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"  {k}: {v:.8f}\")\n",
    "    return stats, sell_percentiles, buy_percentiles, df\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def analyze_slippage_compare(\n",
    "    file_path_a: str,\n",
    "    file_path_b: str,\n",
    "    starttime=None,\n",
    "    endtime=None,\n",
    "    label_a: str = \"FileA\",\n",
    "    label_b: str = \"FileB\",\n",
    "    bins: int = 50,\n",
    "    xlim_quantile=(0.01, 0.99),\n",
    "    show: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    输入两个路径，对比两个文件的滑点：\n",
    "    - 输出一个对比表格 compare_table\n",
    "    - 输出一个 2x3 图（每行一个文件：Sell / Buy / Overall）\n",
    "\n",
    "    返回:\n",
    "        compare_table (pd.DataFrame): MultiIndex 行(side, metric)，列含 label_a, label_b, diff(B-A), diff_bps(B-A)*1e4\n",
    "        fig (matplotlib.figure.Figure)\n",
    "        df_a (pd.DataFrame): 处理后的 A 数据\n",
    "        df_b (pd.DataFrame): 处理后的 B 数据\n",
    "    \"\"\"\n",
    "\n",
    "    def _to_ts(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "        return x if isinstance(x, pd.Timestamp) else pd.to_datetime(x)\n",
    "\n",
    "    def _load_and_process(path: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(path)\n",
    "        # 基本健壮性检查\n",
    "        required_cols = [\n",
    "            \"Order2FilledPrice\", \"Price\", \"ESR\", \"Side\",\n",
    "            \"Order2Timestamp\", \"Timestamp\",\n",
    "            \"AmountFilled\", \"AveragePrice\", \"OrderID\"\n",
    "        ]\n",
    "        missing = [c for c in required_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"[{path}] 缺少必要列: {missing}\")\n",
    "\n",
    "        df = df[df[\"Order2FilledPrice\"] != 0].copy()\n",
    "\n",
    "        # 计算滑点（保持你原来的定义与符号处理）\n",
    "        df[\"SR\"] = df[\"Price\"] / df[\"Order2FilledPrice\"] - 1\n",
    "        df[\"slippage\"] = df[\"SR\"] - df[\"ESR\"]\n",
    "        df[\"sign\"] = df[\"Side\"].apply(lambda x: 1 if x == \"sell\" else -1)\n",
    "        df[\"slippage\"] = df[\"slippage\"] * df[\"sign\"]\n",
    "\n",
    "        # 时间消耗\n",
    "        t2 = pd.to_datetime(df[\"Order2Timestamp\"])\n",
    "        t1 = pd.to_datetime(df[\"Timestamp\"])\n",
    "        df[\"TimeUsed\"] = (t2 - t1).dt.total_seconds()\n",
    "        df[\"HedgingTimeUsed\"] = df[\"TimeUsed\"]\n",
    "\n",
    "        # 额外字段\n",
    "        df[\"Amount\"] = df[\"AmountFilled\"] * df[\"AveragePrice\"]\n",
    "\n",
    "        # 去重\n",
    "        df = df.drop_duplicates(subset=[\"OrderID\"]).copy()\n",
    "\n",
    "        # 时间筛选（精确到秒）\n",
    "        st = _to_ts(starttime)\n",
    "        et = _to_ts(endtime)\n",
    "        if st is not None or et is not None:\n",
    "            ts = pd.to_datetime(df[\"Timestamp\"]).dt.floor(\"S\")\n",
    "            if st is not None:\n",
    "                st = st.floor(\"S\")\n",
    "                df = df[ts > st]\n",
    "            if et is not None:\n",
    "                et = et.floor(\"S\")\n",
    "                df = df[ts < et]\n",
    "\n",
    "        df = df[df[\"slippage\"].notna()].copy()\n",
    "        return df\n",
    "\n",
    "    def _summarize(series: pd.Series, percentiles=(1, 5, 10, 25, 50, 75, 90, 95, 99)) -> dict:\n",
    "        arr = series.dropna().to_numpy()\n",
    "        if len(arr) == 0:\n",
    "            out = {\"count\": 0, \"mean\": np.nan, \"std\": np.nan, \"median\": np.nan, \"min\": np.nan, \"max\": np.nan}\n",
    "            for p in percentiles:\n",
    "                out[f\"p{p}\"] = np.nan\n",
    "            return out\n",
    "\n",
    "        out = {\n",
    "            \"count\": int(len(arr)),\n",
    "            \"mean\": float(np.mean(arr)),\n",
    "            \"std\": float(np.std(arr)),\n",
    "            \"median\": float(np.median(arr)),\n",
    "            \"min\": float(np.min(arr)),\n",
    "            \"max\": float(np.max(arr)),\n",
    "        }\n",
    "        for p in percentiles:\n",
    "            out[f\"p{p}\"] = float(np.percentile(arr, p))\n",
    "        return out\n",
    "\n",
    "    # --- 读 + 处理 ---\n",
    "    df_a = _load_and_process(file_path_a)\n",
    "    df_b = _load_and_process(file_path_b)\n",
    "\n",
    "    # --- 统计表（overall / sell / buy） ---\n",
    "    sides = {\n",
    "        \"sell\": lambda d: d[d[\"Side\"] == \"sell\"][\"slippage\"],\n",
    "        \"buy\":  lambda d: d[d[\"Side\"] == \"buy\"][\"slippage\"],\n",
    "        \"overall\": lambda d: d[\"slippage\"],\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for side_name, getter in sides.items():\n",
    "        sa = _summarize(getter(df_a))\n",
    "        sb = _summarize(getter(df_b))\n",
    "        for k in sa.keys():  # 两边key一致\n",
    "            rows.append((side_name, k, sa[k], sb[k]))\n",
    "\n",
    "    summary_df = pd.DataFrame(rows, columns=[\"side\", \"metric\", label_a, label_b]).set_index([\"side\", \"metric\"])\n",
    "    summary_df[\"diff\"] = summary_df[label_b] - summary_df[label_a]\n",
    "    compare_table = summary_df[[label_a, label_b, \"diff\"]]\n",
    "\n",
    "    # --- 画 2x3 分布图 ---\n",
    "    # 为了两行可比，统一 xlim：用 combined 的 overall 分位数范围\n",
    "    def _q_range(s1, s2, ql, qh):\n",
    "        x = pd.concat([s1.dropna(), s2.dropna()], ignore_index=True)\n",
    "        if len(x) == 0:\n",
    "            return None, None\n",
    "        lo = x.quantile(ql) if ql is not None else x.min()\n",
    "        hi = x.quantile(qh) if qh is not None else x.max()\n",
    "        if np.isfinite(lo) and np.isfinite(hi) and lo < hi:\n",
    "            return float(lo), float(hi)\n",
    "        return None, None\n",
    "\n",
    "    ql, qh = xlim_quantile if xlim_quantile is not None else (None, None)\n",
    "    x_lo, x_hi = _q_range(df_a[\"slippage\"], df_b[\"slippage\"], ql, qh)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(22, 10), sharex=True)\n",
    "    col_defs = [(\"sell\", \"Sell\"), (\"buy\", \"Buy\"), (\"overall\", \"Overall\")]\n",
    "    row_defs = [(df_a, label_a), (df_b, label_b)]\n",
    "\n",
    "    for r, (dff, lab) in enumerate(row_defs):\n",
    "        for c, (side_key, side_title) in enumerate(col_defs):\n",
    "            ax = axes[r, c]\n",
    "            s = sides[side_key](dff).dropna()\n",
    "\n",
    "            ax.hist(s, bins=bins, alpha=0.7)\n",
    "            mu = s.mean() if len(s) else np.nan\n",
    "            sd = s.std() if len(s) else np.nan\n",
    "            if np.isfinite(mu):\n",
    "                ax.axvline(mu, linestyle=\"--\", label=f\"mean={mu:.6g}\")\n",
    "\n",
    "            ax.set_title(f\"{lab} - {side_title} (n={len(s)}, std={sd:.6g})\")\n",
    "            ax.set_xlabel(\"slippage\")\n",
    "            ax.set_ylabel(\"freq\")\n",
    "            ax.legend()\n",
    "\n",
    "            if x_lo is not None and x_hi is not None:\n",
    "                ax.set_xlim(x_lo, x_hi)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return compare_table, fig, df_a, df_b\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def analyze_slippage_compare(\n",
    "    file_paths,                 # <- 改：传入 list\n",
    "    starttime=None,\n",
    "    endtime=None,\n",
    "    labels=None,                # <- 改：labels 也用 list\n",
    "    bins: int = 50,\n",
    "    xlim_quantile=(0.01, 0.99),\n",
    "    show: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    输入多个路径(list)，对比多个文件的滑点：\n",
    "    - 输出一个对比表格 compare_table\n",
    "    - 输出一个 N*3 图（每行一个文件：Sell / Buy / Overall）\n",
    "\n",
    "    返回:\n",
    "        compare_table (pd.DataFrame): MultiIndex 行(side, metric)，列含各 label 以及 diff(相对第一个)\n",
    "        fig (matplotlib.figure.Figure)\n",
    "        *dfs (pd.DataFrame...): 处理后的各文件数据（按传入顺序展开返回）\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 新增：参数检查 & label 默认 ---\n",
    "    if not isinstance(file_paths, (list, tuple)) or len(file_paths) < 2:\n",
    "        raise ValueError(\"file_paths 必须是 list/tuple 且长度>=2，例如 ['a.csv','b.csv'] 或 ['a','b','c']\")\n",
    "    n = len(file_paths)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"File{i+1}\" for i in range(n)]\n",
    "    if not isinstance(labels, (list, tuple)) or len(labels) != n:\n",
    "        raise ValueError(\"labels 必须为 list/tuple 且长度与 file_paths 一致\")\n",
    "\n",
    "    def _to_ts(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "        return x if isinstance(x, pd.Timestamp) else pd.to_datetime(x)\n",
    "\n",
    "    def _load_and_process(path: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(path)\n",
    "        # 基本健壮性检查\n",
    "        required_cols = [\n",
    "            \"Order2FilledPrice\", \"Price\", \"ESR\", \"Side\",\n",
    "            \"Order2Timestamp\", \"Timestamp\",\n",
    "            \"AmountFilled\", \"AveragePrice\", \"OrderID\"\n",
    "        ]\n",
    "        missing = [c for c in required_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"[{path}] 缺少必要列: {missing}\")\n",
    "\n",
    "        df = df[df[\"Order2FilledPrice\"] != 0].copy()\n",
    "\n",
    "        # 计算滑点（保持你原来的定义与符号处理）\n",
    "        df[\"SR\"] = df[\"Price\"] / df[\"Order2FilledPrice\"] - 1\n",
    "        df[\"slippage\"] = df[\"SR\"] - df[\"ESR\"]\n",
    "        df[\"sign\"] = df[\"Side\"].apply(lambda x: 1 if x == \"sell\" else -1)\n",
    "        df[\"slippage\"] = df[\"slippage\"] * df[\"sign\"]\n",
    "\n",
    "        # 时间消耗\n",
    "        t2 = pd.to_datetime(df[\"Order2Timestamp\"])\n",
    "        t1 = pd.to_datetime(df[\"Timestamp\"])\n",
    "        df[\"TimeUsed\"] = (t2 - t1).dt.total_seconds()\n",
    "        df[\"HedgingTimeUsed\"] = df[\"TimeUsed\"]\n",
    "\n",
    "        # 额外字段\n",
    "        df[\"Amount\"] = df[\"AmountFilled\"] * df[\"AveragePrice\"]\n",
    "\n",
    "        # 去重\n",
    "        df = df.drop_duplicates(subset=[\"OrderID\"]).copy()\n",
    "\n",
    "        # 时间筛选（精确到秒）\n",
    "        st = _to_ts(starttime)\n",
    "        et = _to_ts(endtime)\n",
    "        if st is not None or et is not None:\n",
    "            ts = pd.to_datetime(df[\"Timestamp\"]).dt.floor(\"S\")\n",
    "            if st is not None:\n",
    "                st = st.floor(\"S\")\n",
    "                df = df[ts > st]\n",
    "            if et is not None:\n",
    "                et = et.floor(\"S\")\n",
    "                df = df[ts < et]\n",
    "\n",
    "        df = df[df[\"slippage\"].notna()].copy()\n",
    "        return df\n",
    "\n",
    "    def _summarize(series: pd.Series, percentiles=(1, 5, 10, 25, 50, 75, 90, 95, 99)) -> dict:\n",
    "        arr = series.dropna().to_numpy()\n",
    "        if len(arr) == 0:\n",
    "            out = {\"count\": 0, \"mean\": np.nan, \"std\": np.nan, \"median\": np.nan, \"min\": np.nan, \"max\": np.nan}\n",
    "            for p in percentiles:\n",
    "                out[f\"p{p}\"] = np.nan\n",
    "            return out\n",
    "\n",
    "        out = {\n",
    "            \"count\": int(len(arr)),\n",
    "            \"mean\": float(np.mean(arr)),\n",
    "            \"std\": float(np.std(arr)),\n",
    "            \"median\": float(np.median(arr)),\n",
    "            \"min\": float(np.min(arr)),\n",
    "            \"max\": float(np.max(arr)),\n",
    "        }\n",
    "        for p in percentiles:\n",
    "            out[f\"p{p}\"] = float(np.percentile(arr, p))\n",
    "        return out\n",
    "\n",
    "    # --- 读 + 处理（从2个变成N个）---\n",
    "    dfs = [_load_and_process(p) for p in file_paths]\n",
    "\n",
    "    # --- 统计表（overall / sell / buy） ---\n",
    "    sides = {\n",
    "        \"sell\": lambda d: d[d[\"Side\"] == \"sell\"][\"slippage\"],\n",
    "        \"buy\":  lambda d: d[d[\"Side\"] == \"buy\"][\"slippage\"],\n",
    "        \"overall\": lambda d: d[\"slippage\"],\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for side_name, getter in sides.items():\n",
    "        stats_list = [_summarize(getter(df)) for df in dfs]\n",
    "        keys = stats_list[0].keys()\n",
    "        for k in keys:\n",
    "            rows.append((side_name, k, *[st[k] for st in stats_list]))\n",
    "\n",
    "    summary_df = pd.DataFrame(rows, columns=[\"side\", \"metric\", *labels]).set_index([\"side\", \"metric\"])\n",
    "\n",
    "    # 保持原逻辑：diff = (第2个 - 第1个)；现在扩展为：每个(第i个 - 第1个)\n",
    "    base = labels[0]\n",
    "    for lab in labels[1:]:\n",
    "        summary_df[f\"diff({lab}-{base})\"] = summary_df[lab] - summary_df[base]\n",
    "\n",
    "    compare_table = summary_df  # 原来是挑列；这里为了不丢信息，直接返回全表（含diff列）\n",
    "\n",
    "    # --- 画 N*3 分布图（原来2*3） ---\n",
    "    def _q_range_many(series_list, ql, qh):\n",
    "        x = pd.concat([s.dropna() for s in series_list], ignore_index=True)\n",
    "        if len(x) == 0:\n",
    "            return None, None\n",
    "        lo = x.quantile(ql) if ql is not None else x.min()\n",
    "        hi = x.quantile(qh) if qh is not None else x.max()\n",
    "        if np.isfinite(lo) and np.isfinite(hi) and lo < hi:\n",
    "            return float(lo), float(hi)\n",
    "        return None, None\n",
    "\n",
    "    ql, qh = xlim_quantile if xlim_quantile is not None else (None, None)\n",
    "    x_lo, x_hi = _q_range_many([df[\"slippage\"] for df in dfs], ql, qh)\n",
    "\n",
    "    fig, axes = plt.subplots(n, 3, figsize=(22, 5 * n), sharex=True)\n",
    "    if n == 1:\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    col_defs = [(\"sell\", \"Sell\"), (\"buy\", \"Buy\"), (\"overall\", \"Overall\")]\n",
    "\n",
    "    for r, (dff, lab) in enumerate(zip(dfs, labels)):\n",
    "        for c, (side_key, side_title) in enumerate(col_defs):\n",
    "            ax = axes[r, c]\n",
    "            s = sides[side_key](dff).dropna()\n",
    "\n",
    "            ax.hist(s, bins=bins, alpha=0.7)\n",
    "            mu = s.mean() if len(s) else np.nan\n",
    "            sd = s.std() if len(s) else np.nan\n",
    "            if np.isfinite(mu):\n",
    "                ax.axvline(mu, linestyle=\"--\", label=f\"mean={mu:.6g}\")\n",
    "\n",
    "            ax.set_title(f\"{lab} - {side_title} (n={len(s)}, std={sd:.6g})\")\n",
    "            ax.set_xlabel(\"slippage\")\n",
    "            ax.set_ylabel(\"freq\")\n",
    "            ax.legend()\n",
    "\n",
    "            if x_lo is not None and x_hi is not None:\n",
    "                ax.set_xlim(x_lo, x_hi)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    # 保持原返回风格：表格 + fig + df们（只是从2个变成可变长）\n",
    "    return (compare_table, fig, *dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['slippage']>0][['Createtime','Timestamp','Symbol','OrderID','AveragePrice','ESR','Order2FilledPrice','Order2CreateTime','Order2Timestamp','SR','slippage','HedgingTimeUsed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_09_2 (9).csv')\n",
    "df[df['OrderID']==2894534748356714496.00000000 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.05/1.04-1)*365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.05-1.04)*365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['slippage']>0]['HedgingTimeUsed'].quantile(0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TimeUsed'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_,_,_,df = analyze_slippage('/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_09_2 (9).csv')\n",
    "len(df[df['slippage']>=0])/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,df = analyze_slippage('/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_09_2_1103.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_depth_BTC = pd.read_csv('/Users/rayxu/Desktop/Obentech/cf_depth_BTC.csv')\n",
    "beijing_time = pd.to_datetime(cf_depth_BTC['T']).dt.tz_localize('UTC').dt.tz_convert('Asia/Shanghai')\n",
    "cf_depth_BTC['beijing_time'] = beijing_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beijing_time = pd.to_datetime(cf_depth_BTC['T']).dt.tz_localize('UTC').dt.tz_convert('Asia/Shanghai')\n",
    "cf_depth_BTC['beijing_time'] = beijing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/rayxu/Downloads/order.arbitrage_btc_okx_binance_09_2.csv')\n",
    "df[df.OrderID == 2673679677252673536.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('/Users/rayxu/Downloads/order.arbitrage_btc_okx_binance_09_2.csv').loc[3673]\n",
    "2673679677252673536.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTC09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/rayxu/Downloads/order.arbitrage_btc_okx_binance_09_2 (1).csv'\n",
    "stats, sell_stats, buy_stats, processed_df = analyze_slippage(file_path, starttime='2025-07-10 09:00:00')\n",
    "print(\"\\nOverall slippage statistics:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 cf_depth 和 processed_df根据cf_depth的 beijing_time和processed_df的Createtime合并， 保留cf_depth的'ret_mid_1.0s', 'logret_mid_1.0s', 'ret_mid_10.0s', 'logret_mid_10.0s','ret_mid_30.0s', 'logret_mid_30.0s', 'ret_mid_60.0s','logret_mid_60.0s'\n",
    "processed_df['Createtime'] = pd.to_datetime(processed_df['Createtime'])\n",
    "cf_depth_BTC = cf_depth_BTC[['beijing_time', 'ret_mid_1.0s', 'logret_mid_1.0s', 'ret_mid_10.0s','ret_mid_5.0s','logret_mid_10.0s','ret_mid_30.0s', 'logret_mid_30.0s', 'ret_mid_60.0s','logret_mid_60.0s']]\n",
    "cf_depth_BTC = cf_depth_BTC.dropna()\n",
    "cf_depth_BTC = cf_depth_BTC.drop_duplicates(subset=['beijing_time'])\n",
    "\n",
    "# Ensure both are tz-aware and in the same timezone (Asia/Shanghai)\n",
    "if processed_df['Createtime'].dt.tz is None:\n",
    "    processed_df['Createtime'] = processed_df['Createtime'].dt.tz_localize('Asia/Shanghai')\n",
    "else:\n",
    "    processed_df['Createtime'] = processed_df['Createtime'].dt.tz_convert('Asia/Shanghai')\n",
    "\n",
    "# asof merge: need to sort by time\n",
    "cf_depth = cf_depth_BTC.sort_values('beijing_time')\n",
    "processed_df = processed_df.sort_values('Createtime')\n",
    "\n",
    "# asof merge, allow_nearest for closest match\n",
    "merged_df = pd.merge_asof(\n",
    "    processed_df.reset_index(),\n",
    "    cf_depth_BTC,\n",
    "    left_on='Createtime',\n",
    "    right_on='beijing_time',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "# Restore index if needed\n",
    "processed_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 先定义正确的分档区间，区间必须单调递增\n",
    "# # 档位：slippage > -0.0001, -0.0001 >= slippage > -0.0003, -0.0003 >= slippage > -0.0005, slippage <= -0.0005\n",
    "# # 所以bins应该是 [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "# # labels顺序要和bins顺序一致\n",
    "# slippage_bins = [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "# slippage_labels = ['<= -0.0005', '-0.0005 ~ -0.0003', '-0.0003 ~ -0.0001', '> -0.0001']\n",
    "# merged_df['slippage_bin'] = pd.cut(merged_df['slippage'], bins=slippage_bins, labels=slippage_labels, right=True, include_lowest=True, ordered=True)\n",
    "\n",
    "# # 统计每档的ret_mid_1.0s, logret_mid_1.0s, ret_mid_10.0s, logret_mid_10.0s, ret_mid_30.0s, logret_mid_30.0s, ret_mid_60.0s, logret_mid_60.0s的平均值\n",
    "# cols = ['ret_mid_1.0s', 'ret_mid_10.0s',  'ret_mid_30.0s', 'ret_mid_60.0s']\n",
    "# slippage_stats = merged_df.groupby('slippage_bin')[cols].mean()\n",
    "\n",
    "# # 画均值图\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 统计每档的ret的绝对值均值\n",
    "# abs_cols = [c for c in cols if c.startswith('ret_mid_') or c.startswith('logret_mid_')]\n",
    "# for c in abs_cols:\n",
    "#     merged_df[f'abs_{c}'] = merged_df[c].abs()\n",
    "# abs_cols_abs = [f'abs_{c}' for c in abs_cols]\n",
    "# slippage_stats_abs = merged_df.groupby('slippage_bin')[abs_cols_abs].mean()\n",
    "\n",
    "# # 画绝对值均值图\n",
    "# fig, ax = plt.subplots(figsize=(20, 8))\n",
    "# slippage_stats_abs.plot(kind='bar', ax=ax)\n",
    "# plt.title('Mean absolute returns (and log returns) by slippage bin')\n",
    "# plt.ylabel('Mean absolute value')\n",
    "# plt.xlabel('Slippage bin')\n",
    "# plt.xticks(rotation=0)\n",
    "# plt.grid(axis='y')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 先定义正确的分档区间，区间必须单调递增\n",
    "# # 档位：slippage > -0.0001, -0.0001 >= slippage > -0.0003, -0.0003 >= slippage > -0.0005, slippage <= -0.0005\n",
    "# # 所以bins应该是 [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "# # labels顺序要和bins顺序一致\n",
    "# slippage_bins = [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "# slippage_labels = ['<= -0.0005', '-0.0005 ~ -0.0003', '-0.0003 ~ -0.0001', '> -0.0001']\n",
    "# merged_df['slippage_bin'] = pd.cut(merged_df['slippage'], bins=slippage_bins, labels=slippage_labels, right=True, include_lowest=True, ordered=True)\n",
    "\n",
    "# # 统计每档的ret_mid_1.0s, logret_mid_1.0s, ret_mid_10.0s, logret_mid_10.0s, ret_mid_30.0s, logret_mid_30.0s, ret_mid_60.0s, logret_mid_60.0s的平均值\n",
    "# cols = ['ret_mid_1.0s', 'ret_mid_10.0s', 'ret_mid_30.0s', 'ret_mid_60.0s']\n",
    "# slippage_stats = merged_df.groupby('slippage_bin')[cols].mean()\n",
    "\n",
    "# # 画图\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(20, 8))\n",
    "# slippage_stats.plot(kind='bar', ax=ax)\n",
    "# plt.title('Mean returns by slippage bin')\n",
    "# plt.ylabel('Mean value')\n",
    "# plt.xlabel('Slippage bin')\n",
    "# plt.xticks(rotation=0)\n",
    "# plt.grid(axis='y')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[processed_df.slippage < -0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 先定义正确的分档区间，区间必须单调递增\n",
    "# # 档位：slippage > -0.0001, -0.0001 >= slippage > -0.0003, -0.0003 >= slippage > -0.0005, slippage <= -0.0005\n",
    "# # 所以bins应该是 [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "# # labels顺序要和bins顺序一致\n",
    "# slippage_bins = [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "# slippage_labels = ['<= -0.0005', '-0.0005 ~ -0.0003', '-0.0003 ~ -0.0001', '> -0.0001']\n",
    "# merged_df['slippage_bin'] = pd.cut(merged_df['slippage'], bins=slippage_bins, labels=slippage_labels, right=True, include_lowest=True, ordered=True)\n",
    "\n",
    "# # 新建signed方向，sell为-1，其余为1\n",
    "# merged_df['signed'] = merged_df['Side'].apply(lambda x: 1 if x == 'sell' else -1)\n",
    "\n",
    "# # 新建signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s\n",
    "# for col in ['ret_mid_1.0s', 'ret_mid_10.0s', 'ret_mid_30.0s', 'ret_mid_60.0s']:\n",
    "#     merged_df[f'signed_{col}'] = merged_df[col] * merged_df['signed']\n",
    "\n",
    "# # 统计每档的signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s的平均值\n",
    "# signed_cols = [f'signed_ret_mid_{t}' for t in ['1.0s', '10.0s', '30.0s', '60.0s']]\n",
    "# slippage_stats_signed = merged_df.groupby('slippage_bin')[signed_cols].mean()\n",
    "\n",
    "# # 画图\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(20, 8))\n",
    "# slippage_stats_signed.plot(kind='bar', ax=ax)\n",
    "# plt.title('Mean signed returns by slippage bin')\n",
    "# plt.ylabel('Mean signed value')\n",
    "# plt.xlabel('Slippage bin')\n",
    "# plt.xticks(rotation=0)\n",
    "# plt.grid(axis='y')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[(merged_df.Side == 'buy') & (merged_df['ret_mid_5.0s'] < 0)]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(224*0.000226+0.000197*93+0.000177*155+0.000178*118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.000197*93+0.000178*118)/(93+118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.000226*224+0.000177*155)/(224+155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(245*0.000171+191*0.000192)/(245+191)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(167*0.000203+142*0.000208)/(167+142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "118*0.000178/(118+155)+(0.000197)*155/(118+155)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "224*0.000226/(224+93)+(0.000197)*93/(224+93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义分组条件，分不同的ret_{t}s区间\n",
    "time_windows = [1, 5, 10, 30, 60]\n",
    "conditions = {}\n",
    "for side in ['Sell', 'Buy']:\n",
    "    for t in time_windows:\n",
    "        col = f'ret_mid_{t}.0s'\n",
    "        conditions[(side, t, f'Ret{t}s > 0')] = (merged_df.Side.str.lower() == side.lower()) & (merged_df[col] > 0)\n",
    "        conditions[(side, t, f'Ret{t}s < 0')] = (merged_df.Side.str.lower() == side.lower()) & (merged_df[col] < 0)\n",
    "\n",
    "# 计算均值和数量\n",
    "results = []\n",
    "for (side, t, ret), cond in conditions.items():\n",
    "    slippage = merged_df.loc[cond, 'slippage']\n",
    "    mean = slippage.mean()\n",
    "    count = slippage.count()\n",
    "    results.append({'方向': side, '窗口': t, '条件': ret, '均值': mean, '数量': count})\n",
    "\n",
    "df_result = pd.DataFrame(results)\n",
    "\n",
    "# 透视表格\n",
    "pivot = df_result.pivot(index=['方向'], columns=['窗口', '条件'], values=['均值', '数量'])\n",
    "\n",
    "# 每个t分别做数量加权均值配对计算\n",
    "# Sell&Ret>0 和 Buy&Ret<0 配对，Sell&Ret<0 和 Buy&Ret>0 配对，分别针对每个t\n",
    "weighted_means_pair = {}\n",
    "for t in time_windows:\n",
    "    # Sell&Ret>0 和 Buy&Ret<0\n",
    "    sell_pos = df_result[(df_result['方向']=='Sell') & (df_result['窗口']==t) & (df_result['条件']==f'Ret{t}s > 0')]\n",
    "    buy_neg = df_result[(df_result['方向']=='Buy') & (df_result['窗口']==t) & (df_result['条件']==f'Ret{t}s < 0')]\n",
    "    if not sell_pos.empty and not buy_neg.empty:\n",
    "        weighted1 = (sell_pos['均值'].values[0]*sell_pos['数量'].values[0] + buy_neg['均值'].values[0]*buy_neg['数量'].values[0]) / (sell_pos['数量'].values[0] + buy_neg['数量'].values[0])\n",
    "    else:\n",
    "        weighted1 = float('nan')\n",
    "    weighted_means_pair[(f'Sell>0+Buy<0', t)] = weighted1\n",
    "\n",
    "    # Sell&Ret<0 和 Buy&Ret>0\n",
    "    sell_neg = df_result[(df_result['方向']=='Sell') & (df_result['窗口']==t) & (df_result['条件']==f'Ret{t}s < 0')]\n",
    "    buy_pos = df_result[(df_result['方向']=='Buy') & (df_result['窗口']==t) & (df_result['条件']==f'Ret{t}s > 0')]\n",
    "    if not sell_neg.empty and not buy_pos.empty:\n",
    "        weighted2 = (sell_neg['均值'].values[0]*sell_neg['数量'].values[0] + buy_pos['均值'].values[0]*buy_pos['数量'].values[0]) / (sell_neg['数量'].values[0] + buy_pos['数量'].values[0])\n",
    "    else:\n",
    "        weighted2 = float('nan')\n",
    "    weighted_means_pair[(f'Sell<0+Buy>0', t)] = weighted2\n",
    "\n",
    "# 打印表格\n",
    "print(\"BTC (对冲, 7/10-7/15实盘数据)\")\n",
    "header = \"         \" + \"   \".join([f\"Ret{t}s > 0      Ret{t}s < 0\" for t in time_windows])\n",
    "print(header)\n",
    "for side in ['Sell', 'Buy']:\n",
    "    row = []\n",
    "    for t in time_windows:\n",
    "        for cond in [f'Ret{t}s > 0', f'Ret{t}s < 0']:\n",
    "            mean = df_result[(df_result['方向']==side) & (df_result['窗口']==t) & (df_result['条件']==cond)]['均值'].values[0]\n",
    "            count = df_result[(df_result['方向']==side) & (df_result['窗口']==t) & (df_result['条件']==cond)]['数量'].values[0]\n",
    "            row.append(f\"{mean:.6f}({int(count)})\")\n",
    "    print(f\"方向{side:<4} {'   '.join(row)}\")\n",
    "print(\"数量加权(配对: Sell&Ret>0+Buy&Ret<0 | Sell&Ret<0+Buy&Ret>0)\")\n",
    "weighted_row1 = []\n",
    "weighted_row2 = []\n",
    "for t in time_windows:\n",
    "    weighted1 = weighted_means_pair[(f'Sell>0+Buy<0', t)]\n",
    "    weighted2 = weighted_means_pair[(f'Sell<0+Buy>0', t)]\n",
    "    weighted_row1.append(f\"{weighted1:.6f}\")\n",
    "    weighted_row2.append(f\"{weighted2:.6f}\")\n",
    "print(\"Sell>0+Buy<0:   \" + \"   \".join(weighted_row1))\n",
    "print(\"Sell<0+Buy>0:   \" + \"   \".join(weighted_row2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] < 0]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] < -0.0008]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] > 0.0015]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] > -0.0015]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] < -0.0015]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_10.0s'] < -0.0015]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[(merged_df.Side == 'sell') & (merged_df['ret_mid_5.0s'] < 0)]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[(merged_df.Side == 'sell') & (merged_df['ret_mid_5.0s'] > 0)]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[(merged_df.Side == 'sell') & (merged_df['ret_mid_10.0s'] > 0)]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_10.0s'] > 0]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_10.0s'] < 0]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-2.029804e-04+1.961758e-04)/-1.961758e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先定义更细的分档区间：在0到-0.001之间分成10档，最两边为一类\n",
    "import numpy as np\n",
    "\n",
    "# 生成-0.001到0的10个等距分割点\n",
    "num_bins = 10\n",
    "inner_bins = np.linspace(-0.001, 0, num_bins + 1)\n",
    "\n",
    "# 拼接最两边\n",
    "slippage_bins = [float('-inf')] + list(inner_bins) + [float('inf')]\n",
    "\n",
    "# 构造labels\n",
    "slippage_labels = []\n",
    "slippage_labels.append(f'<= {inner_bins[0]:.6f}')\n",
    "for i in range(num_bins):\n",
    "    left = inner_bins[i]\n",
    "    right = inner_bins[i+1]\n",
    "    slippage_labels.append(f'{left:.6f} ~ {right:.6f}')\n",
    "slippage_labels.append(f'> {inner_bins[-1]:.6f}')\n",
    "\n",
    "merged_df['slippage_bin'] = pd.cut(\n",
    "    merged_df['slippage'],\n",
    "    bins=slippage_bins,\n",
    "    labels=slippage_labels,\n",
    "    right=True,\n",
    "    include_lowest=True,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# 新建signed方向，sell为-1，其余为1\n",
    "merged_df['signed'] = merged_df['Side'].apply(lambda x: -1 if x == 'sell' else 1)\n",
    "\n",
    "# 新建signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s\n",
    "for col in ['ret_mid_1.0s','ret_mid_5.0s', 'ret_mid_10.0s', 'ret_mid_30.0s', 'ret_mid_60.0s']:\n",
    "    merged_df[f'signed_{col}'] = merged_df[col] * merged_df['signed']\n",
    "\n",
    "# 统计每档的signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s的平均值\n",
    "signed_cols = [f'signed_ret_mid_{t}' for t in ['1.0s', '5.0s', '10.0s', '30.0s', '60.0s']]\n",
    "slippage_stats_signed = merged_df.groupby('slippage_bin')[signed_cols].mean()\n",
    "\n",
    "# 统计每个bin的数量\n",
    "bin_counts = merged_df['slippage_bin'].value_counts().sort_index()\n",
    "\n",
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "slippage_stats_signed.plot(kind='bar', ax=ax)\n",
    "plt.title('Mean signed returns by slippage bin')\n",
    "plt.ylabel('Mean signed value')\n",
    "plt.xlabel('Slippage bin')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# 在每个bar上方标注数量\n",
    "for i, count in enumerate(bin_counts):\n",
    "    # 取每个bar的最高点\n",
    "    y = slippage_stats_signed.max().max()\n",
    "    # 取当前bin的所有均值的最大值（以便标注在bar上方）\n",
    "    y_bin = slippage_stats_signed.iloc[i].max()\n",
    "    ax.text(i, y_bin + 0.00002, f'n={count}', ha='center', va='bottom', fontsize=12, rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(merged_df['ret_mid_30.0s']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] > 0.0004]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] < -0.0004]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] < 0]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] > 0]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] < 0 ]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[(merged_df.slippage > -0.00001) & (merged_df.Side == 'buy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/rayxu/Downloads/order.arbitrage_btc_okx_binance_09_2.csv')\n",
    "/Users/rayxu/Downloads/order.arbitrage_goat_okx_binance_99_2.csv\n",
    "df = df[df.Order2FilledPrice!=0]\n",
    "df['SR'] = df['Price']/df['Order2FilledPrice']-1\n",
    "df['slippage'] = df['SR']-df['ESR']\n",
    "df['sign'] = df['Side'].apply(lambda x: 1 if x == 'sell' else -1)\n",
    "df['slippage'] = df['slippage']*df['sign']\n",
    "df['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Side=='sell']['slippage'].hist(figsize=(12, 8))\n",
    "print(df[df.Side=='sell']['slippage'].mean())\n",
    "print(df[df.Side=='sell']['slippage'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Side=='buy']['slippage'].hist(figsize=(12, 8))\n",
    "print(df[df.Side=='buy']['slippage'].mean())\n",
    "print(df[df.Side=='buy']['slippage'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.Side=='buy')& (df.slippage>0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTC08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用示例\n",
    "file_path = '/Users/rayxu/Downloads/order.arbitrage_btc_okx_binance_08_2 (2).csv'\n",
    "stats, sell_stats, buy_stats, processed_df = analyze_slippage(file_path)\n",
    "print(\"\\nOverall slippage statistics:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选大于7/10的processed_df\n",
    "processed_df['Createtime'] = pd.to_datetime(processed_df['Createtime'])\n",
    "processed_df = processed_df[processed_df.Createtime > pd.Timestamp('2025-07-10')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 cf_depth 和 processed_df根据cf_depth的 beijing_time和processed_df的Createtime合并， 保留cf_depth的'ret_mid_1.0s', 'logret_mid_1.0s', 'ret_mid_10.0s', 'logret_mid_10.0s','ret_mid_30.0s', 'logret_mid_30.0s', 'ret_mid_60.0s','logret_mid_60.0s'\n",
    "\n",
    "# Convert Createtime to datetime if not already\n",
    "processed_df['Createtime'] = pd.to_datetime(processed_df['Createtime'])\n",
    "\n",
    "# Remove rows with null Createtime to avoid merge_asof error\n",
    "processed_df = processed_df[processed_df['Createtime'].notnull()].copy()\n",
    "\n",
    "# Select and clean cf_depth columns\n",
    "cf_depth = cf_depth[['beijing_time', 'ret_mid_1.0s', 'logret_mid_1.0s', 'ret_mid_10.0s', 'logret_mid_10.0s','ret_mid_5.0s',\n",
    "                     'ret_mid_30.0s', 'logret_mid_30.0s', 'ret_mid_60.0s', 'logret_mid_60.0s']]\n",
    "cf_depth = cf_depth.dropna()\n",
    "cf_depth = cf_depth.drop_duplicates(subset=['beijing_time'])\n",
    "\n",
    "# Ensure both are tz-aware and in the same timezone (Asia/Shanghai)\n",
    "if processed_df['Createtime'].dt.tz is None:\n",
    "    processed_df['Createtime'] = processed_df['Createtime'].dt.tz_localize('Asia/Shanghai')\n",
    "else:\n",
    "    processed_df['Createtime'] = processed_df['Createtime'].dt.tz_convert('Asia/Shanghai')\n",
    "\n",
    "if pd.to_datetime(cf_depth['beijing_time']).dt.tz is None:\n",
    "    cf_depth['beijing_time'] = pd.to_datetime(cf_depth['beijing_time']).dt.tz_localize('Asia/Shanghai')\n",
    "else:\n",
    "    cf_depth['beijing_time'] = pd.to_datetime(cf_depth['beijing_time']).dt.tz_convert('Asia/Shanghai')\n",
    "\n",
    "# Sort by time for asof merge\n",
    "cf_depth = cf_depth.sort_values('beijing_time')\n",
    "processed_df = processed_df.sort_values('Createtime')\n",
    "\n",
    "# asof merge, allow_nearest for closest match\n",
    "merged_df = pd.merge_asof(\n",
    "    processed_df.reset_index(),\n",
    "    cf_depth,\n",
    "    left_on='Createtime',\n",
    "    right_on='beijing_time',\n",
    "    direction='backward'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[(merged_df.Side == 'sell') & (merged_df['ret_mid_10.0s'] > 0)]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先定义正确的分档区间，区间必须单调递增\n",
    "# 档位：slippage > -0.0001, -0.0001 >= slippage > -0.0003, -0.0003 >= slippage > -0.0005, slippage <= -0.0005\n",
    "# 所以bins应该是 [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "# labels顺序要和bins顺序一致\n",
    "slippage_bins = [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "slippage_labels = ['<= -0.0005', '-0.0005 ~ -0.0003', '-0.0003 ~ -0.0001', '> -0.0001']\n",
    "merged_df['slippage_bin'] = pd.cut(merged_df['slippage'], bins=slippage_bins, labels=slippage_labels, right=True, include_lowest=True, ordered=True)\n",
    "\n",
    "# 新建signed方向，sell为-1，其余为1\n",
    "merged_df['signed'] = merged_df['Side'].apply(lambda x: -1 if x == 'sell' else 1)\n",
    "\n",
    "# 新建signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s\n",
    "for col in ['ret_mid_1.0s', 'ret_mid_5.0s', 'ret_mid_10.0s', 'ret_mid_30.0s', 'ret_mid_60.0s']:\n",
    "    merged_df[f'signed_{col}'] = merged_df[col] * merged_df['signed']\n",
    "\n",
    "# 统计每档的signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s的平均值\n",
    "signed_cols = [f'signed_ret_mid_{t}' for t in ['1.0s', '10.0s', '30.0s', '60.0s']]\n",
    "slippage_stats_signed = merged_df.groupby('slippage_bin')[signed_cols].mean()\n",
    "\n",
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "slippage_stats_signed.plot(kind='bar', ax=ax)\n",
    "plt.title('Mean signed returns by slippage bin')\n",
    "plt.ylabel('Mean signed value')\n",
    "plt.xlabel('Slippage bin')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先定义更细的分档区间：在0到-0.001之间分成10档，最两边为一类\n",
    "import numpy as np\n",
    "\n",
    "# 生成-0.001到0的10个等距分割点\n",
    "num_bins = 10\n",
    "inner_bins = np.linspace(-0.001, 0, num_bins + 1)\n",
    "\n",
    "# 拼接最两边\n",
    "slippage_bins = [float('-inf')] + list(inner_bins) + [float('inf')]\n",
    "\n",
    "# 构造labels\n",
    "slippage_labels = []\n",
    "slippage_labels.append(f'<= {inner_bins[0]:.6f}')\n",
    "for i in range(num_bins):\n",
    "    left = inner_bins[i]\n",
    "    right = inner_bins[i+1]\n",
    "    slippage_labels.append(f'{left:.6f} ~ {right:.6f}')\n",
    "slippage_labels.append(f'> {inner_bins[-1]:.6f}')\n",
    "\n",
    "merged_df['slippage_bin'] = pd.cut(\n",
    "    merged_df['slippage'],\n",
    "    bins=slippage_bins,\n",
    "    labels=slippage_labels,\n",
    "    right=True,\n",
    "    include_lowest=True,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# 新建signed方向，sell为-1，其余为1\n",
    "merged_df['signed'] = merged_df['Side'].apply(lambda x: -1 if x == 'sell' else 1)\n",
    "\n",
    "# 新建signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s\n",
    "for col in ['ret_mid_1.0s', 'ret_mid_5.0s', 'ret_mid_10.0s', 'ret_mid_30.0s', 'ret_mid_60.0s']:\n",
    "    merged_df[f'signed_{col}'] = merged_df[col] * merged_df['signed']\n",
    "\n",
    "# 统计每档的signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s的平均值\n",
    "signed_cols = [f'signed_ret_mid_{t}' for t in ['1.0s', '5.0s', '10.0s', '30.0s', '60.0s']]\n",
    "slippage_stats_signed = merged_df.groupby('slippage_bin')[signed_cols].mean()\n",
    "\n",
    "# 统计每个bin的数量\n",
    "bin_counts = merged_df['slippage_bin'].value_counts().sort_index()\n",
    "\n",
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "slippage_stats_signed.plot(kind='bar', ax=ax)\n",
    "plt.title('Mean signed returns by slippage bin')\n",
    "plt.ylabel('Mean signed value')\n",
    "plt.xlabel('Slippage bin')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# 在每个bar上方标注数量\n",
    "for i, count in enumerate(bin_counts):\n",
    "    # 取每个bar的最高点\n",
    "    y = slippage_stats_signed.max().max()\n",
    "    # 取当前bin的所有均值的最大值（以便标注在bar上方）\n",
    "    y_bin = slippage_stats_signed.iloc[i].max()\n",
    "    ax.text(i, y_bin + 0.00002, f'n={count}', ha='center', va='bottom', fontsize=12, rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] > 0.000]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['signed_ret_mid_30.0s'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] < -0.0003]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_30.0s'] > 0.0003]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先定义正确的分档区间，区间必须单调递增\n",
    "# 档位：slippage > -0.0001, -0.0001 >= slippage > -0.0003, -0.0003 >= slippage > -0.0005, slippage <= -0.0005\n",
    "# 所以bins应该是 [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "# labels顺序要和bins顺序一致\n",
    "slippage_bins = [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "slippage_labels = ['<= -0.0005', '-0.0005 ~ -0.0003', '-0.0003 ~ -0.0001', '> -0.0001']\n",
    "merged_df['slippage_bin'] = pd.cut(merged_df['slippage'], bins=slippage_bins, labels=slippage_labels, right=True, include_lowest=True, ordered=True)\n",
    "\n",
    "# 统计每档的ret_mid_1.0s, logret_mid_1.0s, ret_mid_10.0s, logret_mid_10.0s, ret_mid_30.0s, logret_mid_30.0s, ret_mid_60.0s, logret_mid_60.0s的平均值\n",
    "cols = ['ret_mid_1.0s', 'ret_mid_10.0s', 'ret_mid_30.0s', 'ret_mid_60.0s']\n",
    "slippage_stats = merged_df.groupby('slippage_bin')[cols].mean()\n",
    "\n",
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "slippage_stats.plot(kind='bar', ax=ax)\n",
    "plt.title('Mean returns by slippage bin')\n",
    "plt.ylabel('Mean value')\n",
    "plt.xlabel('Slippage bin')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先定义正确的分档区间，区间必须单调递增\n",
    "# 档位：slippage > -0.0001, -0.0001 >= slippage > -0.0003, -0.0003 >= slippage > -0.0005, slippage <= -0.0005\n",
    "# 所以bins应该是 [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "# labels顺序要和bins顺序一致\n",
    "slippage_bins = [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "slippage_labels = ['<= -0.0005', '-0.0005 ~ -0.0003', '-0.0003 ~ -0.0001', '> -0.0001']\n",
    "merged_df['slippage_bin'] = pd.cut(merged_df['slippage'], bins=slippage_bins, labels=slippage_labels, right=True, include_lowest=True, ordered=True)\n",
    "\n",
    "# 新建signed方向，sell为-1，其余为1\n",
    "merged_df['signed'] = merged_df['Side'].apply(lambda x: -1 if x == 'sell' else 1)\n",
    "\n",
    "# 新建signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s\n",
    "for col in ['ret_mid_1.0s', 'ret_mid_10.0s', 'ret_mid_30.0s', 'ret_mid_60.0s']:\n",
    "    merged_df[f'signed_{col}'] = merged_df[col] * merged_df['signed']\n",
    "\n",
    "# 统计每档的signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s的平均值\n",
    "signed_cols = [f'signed_ret_mid_{t}' for t in ['1.0s', '10.0s', '30.0s', '60.0s']]\n",
    "slippage_stats_signed = merged_df.groupby('slippage_bin')[signed_cols].mean()\n",
    "\n",
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "slippage_stats_signed.plot(kind='bar', ax=ax)\n",
    "plt.title('Mean signed returns by slippage bin')\n",
    "plt.ylabel('Mean signed value')\n",
    "plt.xlabel('Slippage bin')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先定义更细的分档区间：在0到-0.001之间分成10档，最两边为一类\n",
    "import numpy as np\n",
    "\n",
    "# 生成-0.001到0的10个等距分割点\n",
    "num_bins = 10\n",
    "inner_bins = np.linspace(-0.001, 0, num_bins + 1)\n",
    "\n",
    "# 拼接最两边\n",
    "slippage_bins = [float('-inf')] + list(inner_bins) + [float('inf')]\n",
    "\n",
    "# 构造labels\n",
    "slippage_labels = []\n",
    "slippage_labels.append(f'<= {inner_bins[0]:.6f}')\n",
    "for i in range(num_bins):\n",
    "    left = inner_bins[i]\n",
    "    right = inner_bins[i+1]\n",
    "    slippage_labels.append(f'{left:.6f} ~ {right:.6f}')\n",
    "slippage_labels.append(f'> {inner_bins[-1]:.6f}')\n",
    "\n",
    "merged_df['slippage_bin'] = pd.cut(\n",
    "    merged_df['slippage'],\n",
    "    bins=slippage_bins,\n",
    "    labels=slippage_labels,\n",
    "    right=True,\n",
    "    include_lowest=True,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# 新建signed方向，sell为-1，其余为1\n",
    "merged_df['signed'] = merged_df['Side'].apply(lambda x: -1 if x == 'sell' else 1)\n",
    "\n",
    "# 新建signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s\n",
    "for col in ['ret_mid_1.0s', 'ret_mid_10.0s', 'ret_mid_30.0s', 'ret_mid_60.0s']:\n",
    "    merged_df[f'signed_{col}'] = merged_df[col] * merged_df['signed']\n",
    "\n",
    "# 统计每档的signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s的平均值\n",
    "signed_cols = [f'signed_ret_mid_{t}' for t in ['1.0s', '10.0s', '30.0s', '60.0s']]\n",
    "slippage_stats_signed = merged_df.groupby('slippage_bin')[signed_cols].mean()\n",
    "\n",
    "# 统计每个bin的数量\n",
    "bin_counts = merged_df['slippage_bin'].value_counts().sort_index()\n",
    "\n",
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "slippage_stats_signed.plot(kind='bar', ax=ax)\n",
    "plt.title('Mean signed returns by slippage bin')\n",
    "plt.ylabel('Mean signed value')\n",
    "plt.xlabel('Slippage bin')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# 在每个bar上方标注数量\n",
    "for i, count in enumerate(bin_counts):\n",
    "    # 取每个bar的最高点\n",
    "    y = slippage_stats_signed.max().max()\n",
    "    # 取当前bin的所有均值的最大值（以便标注在bar上方）\n",
    "    y_bin = slippage_stats_signed.iloc[i].max()\n",
    "    ax.text(i, y_bin + 0.00002, f'n={count}', ha='center', va='bottom', fontsize=12, rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[processed_df.slippage < -0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[processed_df.OrderID == 2672731276698378240.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[processed_df.slippage > 0].tail(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.Side=='buy')& (df.slippage<-0.0008)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETH08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用示例\n",
    "file_path = '/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_08_2 (1).csv'\n",
    "stats, sell_stats, buy_stats, processed_df = analyze_slippage(file_path)\n",
    "print(\"\\nOverall slippage statistics:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 cf_depth 和 processed_df根据cf_depth的 beijing_time和processed_df的Createtime合并， 保留cf_depth的'ret_mid_1.0s', 'logret_mid_1.0s', 'ret_mid_10.0s', 'logret_mid_10.0s','ret_mid_30.0s', 'logret_mid_30.0s', 'ret_mid_60.0s','logret_mid_60.0s'\n",
    "\n",
    "# Convert Createtime to datetime if not already\n",
    "processed_df['Createtime'] = pd.to_datetime(processed_df['Createtime'])\n",
    "\n",
    "# Remove rows with null Createtime to avoid merge_asof error\n",
    "processed_df = processed_df[processed_df['Createtime'].notnull()].copy()\n",
    "\n",
    "# Select and clean cf_depth columns\n",
    "cf_depth = cf_depth[['beijing_time', 'ret_mid_1.0s', 'logret_mid_1.0s', 'ret_mid_10.0s', 'logret_mid_10.0s',\n",
    "                     'ret_mid_30.0s', 'logret_mid_30.0s', 'ret_mid_60.0s', 'logret_mid_60.0s']]\n",
    "cf_depth = cf_depth.dropna()\n",
    "cf_depth = cf_depth.drop_duplicates(subset=['beijing_time'])\n",
    "\n",
    "# Ensure both are tz-aware and in the same timezone (Asia/Shanghai)\n",
    "if processed_df['Createtime'].dt.tz is None:\n",
    "    processed_df['Createtime'] = processed_df['Createtime'].dt.tz_localize('Asia/Shanghai')\n",
    "else:\n",
    "    processed_df['Createtime'] = processed_df['Createtime'].dt.tz_convert('Asia/Shanghai')\n",
    "\n",
    "if pd.to_datetime(cf_depth['beijing_time']).dt.tz is None:\n",
    "    cf_depth['beijing_time'] = pd.to_datetime(cf_depth['beijing_time']).dt.tz_localize('Asia/Shanghai')\n",
    "else:\n",
    "    cf_depth['beijing_time'] = pd.to_datetime(cf_depth['beijing_time']).dt.tz_convert('Asia/Shanghai')\n",
    "\n",
    "# Sort by time for asof merge\n",
    "cf_depth = cf_depth.sort_values('beijing_time')\n",
    "processed_df = processed_df.sort_values('Createtime')\n",
    "\n",
    "# asof merge, allow_nearest for closest match\n",
    "merged_df = pd.merge_asof(\n",
    "    processed_df.reset_index(),\n",
    "    cf_depth,\n",
    "    left_on='Createtime',\n",
    "    right_on='beijing_time',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "# 先定义正确的分档区间，区间必须单调递增\n",
    "# 档位：slippage > -0.0001, -0.0001 >= slippage > -0.0003, -0.0003 >= slippage > -0.0005, slippage <= -0.0005\n",
    "# 所以bins应该是 [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "# labels顺序要和bins顺序一致\n",
    "slippage_bins = [float('-inf'), -0.0005, -0.0003, -0.0001, float('inf')]\n",
    "slippage_labels = ['<= -0.0005', '-0.0005 ~ -0.0003', '-0.0003 ~ -0.0001', '> -0.0001']\n",
    "merged_df['slippage_bin'] = pd.cut(merged_df['slippage'], bins=slippage_bins, labels=slippage_labels, right=True, include_lowest=True, ordered=True)\n",
    "\n",
    "# 新建signed方向，sell为-1，其余为1\n",
    "merged_df['signed'] = merged_df['Side'].apply(lambda x: -1 if x == 'sell' else 1)\n",
    "\n",
    "# 新建signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s\n",
    "for col in ['ret_mid_1.0s', 'ret_mid_10.0s', 'ret_mid_30.0s', 'ret_mid_60.0s']:\n",
    "    merged_df[f'signed_{col}'] = merged_df[col] * merged_df['signed']\n",
    "\n",
    "# 统计每档的signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s的平均值\n",
    "signed_cols = [f'signed_ret_mid_{t}' for t in ['1.0s', '10.0s', '30.0s', '60.0s']]\n",
    "slippage_stats_signed = merged_df.groupby('slippage_bin')[signed_cols].mean()\n",
    "\n",
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "slippage_stats_signed.plot(kind='bar', ax=ax)\n",
    "plt.title('Mean signed returns by slippage bin')\n",
    "plt.ylabel('Mean signed value')\n",
    "plt.xlabel('Slippage bin')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先定义更细的分档区间：在0到-0.001之间分成10档，最两边为一类\n",
    "import numpy as np\n",
    "\n",
    "# 生成-0.001到0的10个等距分割点\n",
    "num_bins = 10\n",
    "inner_bins = np.linspace(-0.001, 0, num_bins + 1)\n",
    "\n",
    "# 拼接最两边\n",
    "slippage_bins = [float('-inf')] + list(inner_bins) + [float('inf')]\n",
    "\n",
    "# 构造labels\n",
    "slippage_labels = []\n",
    "slippage_labels.append(f'<= {inner_bins[0]:.6f}')\n",
    "for i in range(num_bins):\n",
    "    left = inner_bins[i]\n",
    "    right = inner_bins[i+1]\n",
    "    slippage_labels.append(f'{left:.6f} ~ {right:.6f}')\n",
    "slippage_labels.append(f'> {inner_bins[-1]:.6f}')\n",
    "\n",
    "merged_df['slippage_bin'] = pd.cut(\n",
    "    merged_df['slippage'],\n",
    "    bins=slippage_bins,\n",
    "    labels=slippage_labels,\n",
    "    right=True,\n",
    "    include_lowest=True,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# 新建signed方向，sell为-1，其余为1\n",
    "merged_df['signed'] = merged_df['Side'].apply(lambda x: -1 if x == 'sell' else 1)\n",
    "\n",
    "# 新建signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s\n",
    "for col in ['ret_mid_1.0s', 'ret_mid_10.0s', 'ret_mid_30.0s', 'ret_mid_60.0s']:\n",
    "    merged_df[f'signed_{col}'] = merged_df[col] * merged_df['signed']\n",
    "\n",
    "# 统计每档的signed_ret_mid_1.0s, signed_ret_mid_10.0s, signed_ret_mid_30.0s, signed_ret_mid_60.0s的平均值\n",
    "signed_cols = [f'signed_ret_mid_{t}' for t in ['1.0s', '10.0s', '30.0s', '60.0s']]\n",
    "slippage_stats_signed = merged_df.groupby('slippage_bin')[signed_cols].mean()\n",
    "\n",
    "# 统计每个bin的数量\n",
    "bin_counts = merged_df['slippage_bin'].value_counts().sort_index()\n",
    "\n",
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "slippage_stats_signed.plot(kind='bar', ax=ax)\n",
    "plt.title('Mean signed returns by slippage bin')\n",
    "plt.ylabel('Mean signed value')\n",
    "plt.xlabel('Slippage bin')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# 在每个bar上方标注数量\n",
    "for i, count in enumerate(bin_counts):\n",
    "    # 取每个bar的最高点\n",
    "    y = slippage_stats_signed.max().max()\n",
    "    # 取当前bin的所有均值的最大值（以便标注在bar上方）\n",
    "    y_bin = slippage_stats_signed.iloc[i].max()\n",
    "    ax.text(i, y_bin + 0.00002, f'n={count}', ha='center', va='bottom', fontsize=12, rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[processed_df.slippage<-0.0008].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/rayxu/Downloads/order.arbitrage_kaito_okx_binance_99_2 (2).csv')\n",
    "\n",
    "df = df[df.Order2FilledPrice!=0]\n",
    "df['SR'] = df['Price']/df['Order2FilledPrice']-1\n",
    "df['slippage'] = df['SR']-df['ESR']\n",
    "df['sign'] = df['Side'].apply(lambda x: 1 if x == 'sell' else -1)\n",
    "df['slippage'] = df['slippage']*df['sign']+0.00003\n",
    "df['slippage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOL08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/Users/rayxu/Downloads/order.arbitrage_sol_okx_binance_08_2.csv')\n",
    "# df = df[df.Order2FilledPrice!=0]\n",
    "file_path = '/Users/rayxu/Downloads/order.arbitrage_sol_okx_binance_08_2.csv'\n",
    "stats, sell_stats, buy_stats, processed_df = analyze_slippage(file_path)\n",
    "print(\"\\nOverall slippage statistics:\")\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df['Createtime'] = pd.to_datetime(processed_df['Createtime'])\n",
    "processed_df = processed_df[processed_df.Createtime > pd.Timestamp('2025-07-17')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[processed_df.Side == 'buy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[processed_df['Side']=='sell']['ESR'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[processed_df['Side']=='buy']['ESR'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTC 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/rayxu/Downloads/order.btc_okx_binance_01_2.csv'\n",
    "stats, sell_stats, buy_stats, processed_df = analyze_slippage(file_path, starttime='2025-07-31 13:31:39')\n",
    "print(\"\\nOverall slippage statistics:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.sort_values(by='slippage',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/rayxu/Downloads/order.btc_okx_binance_02_2.csv'\n",
    "stats, sell_stats, buy_stats, processed_df = analyze_slippage(file_path, starttime='2025-07-31 13:31:39')\n",
    "print(\"\\nOverall slippage statistics:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.sort_values(by='Createtime',ascending=True).head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验结果对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def analyze_slippage_backtest(file_path):\n",
    "    \"\"\"\n",
    "    Analyze slippage distribution from a JSON file containing trading data.\n",
    "    Each line of the file is a separate JSON object.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSON file\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Lists to store slippage values and related data\n",
    "    slippage_values = []\n",
    "    slippage_by_type = defaultdict(list)\n",
    "    slippage_by_volume = defaultdict(list)\n",
    "    \n",
    "    # Read and process the file line by line\n",
    "    line_count = 0\n",
    "    valid_slippage_count = 0\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_count += 1\n",
    "            \n",
    "            try:\n",
    "                # Parse JSON object\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                # Check if slippage is available and not null\n",
    "                if 'slippage' in data and data['slippage'] is not None:\n",
    "                    slippage = data['slippage']\n",
    "                    trade_type = data['type']\n",
    "                    volume = data['volume']\n",
    "                    \n",
    "                    # Store slippage value\n",
    "                    slippage_values.append(slippage)\n",
    "                    slippage_by_type[trade_type].append(slippage)\n",
    "                    slippage_by_volume[volume].append(slippage)\n",
    "                    \n",
    "                    valid_slippage_count += 1\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error parsing JSON at line {line_count}\")\n",
    "                continue\n",
    "    \n",
    "    if not slippage_values:\n",
    "        print(\"No valid slippage values found.\")\n",
    "        return\n",
    "    \n",
    "    # Convert to numpy array for analysis\n",
    "    slippage_array = np.array(slippage_values)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'count': len(slippage_array),\n",
    "        'mean': np.mean(slippage_array),\n",
    "        'median': np.median(slippage_array),\n",
    "        'std': np.std(slippage_array),\n",
    "        'min': np.min(slippage_array),\n",
    "        'max': np.max(slippage_array),\n",
    "        'percentile_1': np.percentile(slippage_array, 1),\n",
    "        'percentile_5': np.percentile(slippage_array, 5),\n",
    "        'percentile_10': np.percentile(slippage_array, 10),\n",
    "        'percentile_25': np.percentile(slippage_array, 25),\n",
    "        'percentile_50': np.percentile(slippage_array, 50),\n",
    "        'percentile_55': np.percentile(slippage_array, 55),        \n",
    "        'percentile_60': np.percentile(slippage_array, 60),\n",
    "        'percentile_65': np.percentile(slippage_array, 65),\n",
    "        'percentile_75': np.percentile(slippage_array, 75),\n",
    "        'percentile_95': np.percentile(slippage_array, 95),\n",
    "        'percentile_99': np.percentile(slippage_array, 99)\n",
    "    }\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"Slippage Statistics:\")\n",
    "    print(f\"Count: {stats['count']}\")\n",
    "    print(f\"Mean: {stats['mean']:.8f}\")\n",
    "    print(f\"Median: {stats['median']:.8f}\")\n",
    "    print(f\"Standard Deviation: {stats['std']:.8f}\")\n",
    "    print(f\"Min: {stats['min']:.8f}\")\n",
    "    print(f\"Max: {stats['max']:.8f}\")\n",
    "    print(f\"1st Percentile: {stats['percentile_1']:.8f}\")\n",
    "    print(f\"5th Percentile: {stats['percentile_5']:.8f}\")\n",
    "    print(f\"10th Percentile: {stats['percentile_10']:.8f}\")\n",
    "    print(f\"25th Percentile: {stats['percentile_25']:.8f}\")\n",
    "    print(f\"50th Percentile: {stats['percentile_50']:.8f}\")\n",
    "    print(f\"55th Percentile: {stats['percentile_55']:.8f}\")\n",
    "    print(f\"60th Percentile: {stats['percentile_60']:.8f}\")\n",
    "    print(f\"65th Percentile: {stats['percentile_65']:.8f}\")\n",
    "    print(f\"75th Percentile: {stats['percentile_75']:.8f}\")\n",
    "    print(f\"95th Percentile: {stats['percentile_95']:.8f}\")\n",
    "    print(f\"99th Percentile: {stats['percentile_99']:.8f}\")\n",
    "    \n",
    "    # Statistics by trade type\n",
    "    print(\"\\nSlippage by Trade Type:\")\n",
    "    for trade_type, values in slippage_by_type.items():\n",
    "        values_array = np.array(values)\n",
    "        print(f\"{trade_type}:\")\n",
    "        print(f\"  Count: {len(values_array)}\")\n",
    "        print(f\"  Mean: {np.mean(values_array):.8f}\")\n",
    "        print(f\"  Median: {np.median(values_array):.8f}\")\n",
    "        print(f\"  Standard Deviation: {np.std(values_array):.8f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualizations(slippage_array, slippage_by_type, slippage_by_volume)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def create_visualizations(slippage_array, slippage_by_type, slippage_by_volume):\n",
    "    \"\"\"\n",
    "    Create three visualizations in one row:\n",
    "    1. Histogram of all slippage values\n",
    "    2. Histogram of slippage for trade type = Maker_ask\n",
    "    3. Histogram of slippage for trade type = Maker_bid\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    type_labels = {\n",
    "        \"all\": \"All Slippage Distribution\",\n",
    "        \"Maker_ask\": \"Type = Maker_ask (主动买单/挂卖)\",\n",
    "        \"Maker_bid\": \"Type = Maker_bid (主动卖单/挂买)\"\n",
    "    }\n",
    "    colors = {\n",
    "        \"all\": \"blue\",\n",
    "        \"Maker_ask\": \"green\",\n",
    "        \"Maker_bid\": \"red\"\n",
    "    }\n",
    "\n",
    "    # Create a single row with 3 subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # 1. All slippage\n",
    "    axes[0].hist(slippage_array, bins=50, alpha=0.7, color=colors[\"all\"])\n",
    "    axes[0].set_title(type_labels[\"all\"])\n",
    "    axes[0].set_xlabel('Slippage')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Maker_ask\n",
    "    if \"Maker_ask\" in slippage_by_type and len(slippage_by_type[\"Maker_ask\"]) > 0:\n",
    "        axes[1].hist(slippage_by_type[\"Maker_ask\"], bins=50, alpha=0.7, color=colors[\"Maker_ask\"])\n",
    "    axes[1].set_title('Ask Slippage Distribution')\n",
    "    axes[1].set_xlabel('Slippage')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Maker_bid\n",
    "    if \"Maker_bid\" in slippage_by_type and len(slippage_by_type[\"Maker_bid\"]) > 0:\n",
    "        axes[2].hist(slippage_by_type[\"Maker_bid\"], bins=50, alpha=0.7, color=colors[\"Maker_bid\"])\n",
    "    axes[2].set_title('Bid Slippage Distribution')\n",
    "    axes[2].set_xlabel('Slippage')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('slippage_hist_row.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BTC 5s 对冲 正负万1 不过滤行情\n",
    "file_path = \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-18/2025-07-01-2025-07-13_1752883486.132423.json\"\n",
    "results = analyze_slippage_backtest(file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-21/2025-07-01-2025-07-13_1753131533.778579.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-21/2025-07-01-2025-07-13_1753138977.148704.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-21/2025-07-01-2025-07-13_1753140838.4960299.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-21/2025-07-01-2025-07-13_1753143435.000588.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-21/2025-07-01-2025-07-13_1753145597.893425.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OIR + 过滤行情\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-21/2025-07-01-2025-07-13_1753150054.832012.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-21/2025-07-01-2025-07-13_1753150054.832012.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只过滤SELL, 0.0006\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-21/2025-07-01-2025-07-13_1753151673.4183512.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if self.max_pos * self.order_size > self._position_ex0 and self.sr_bid < -0.0001 and self.sr_bid_300.percentile(\n",
    "#         0.5) < -0.0001 and price_return_30s < 0.0006:\n",
    "#     return 1\n",
    "# if -self.max_pos * self.order_size < self._position_ex0 and self.sr_ask > 0.0001 and self.sr_ask_300.percentile(\n",
    "#         0.5) > 0.0001:\n",
    "#     return 2\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-21/2025-07-01-2025-07-13_1753155931.77145.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-22/2025-07-01-2025-07-13_1753216324.123404.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-22/2025-02-20-2025-02-26_1753223839.1197839.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-23/2025-06-15-2025-07-13_1753312132.4394119.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-24/2025-05-01-2025-05-31_1753398188.993484.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-24/2025-05-01-2025-05-31_1753402445.818174.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-25/2025-05-01-2025-05-31_1753477365.235345.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-25/2025-05-01-2025-05-31_1753483577.598297.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-27/2025-04-01-2025-04-30_1753661363.4618418.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-28/2025-07-01-2025-07-13_1753746252.157256.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-28/2025-07-01-2025-07-13_1753748285.254567.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-28/2025-07-01-2025-07-13_1753753382.714158.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-29/2025-07-01-2025-07-05_1753775101.038172.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Spread 版本回测验证过滤极端行情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-04/2025-07-01-2025-07-13_BTC_0.0_500.0_10_True_False_True_-0.0001_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-01/2025-07-01-2025-07-13_BTC_0.0_500.0_10_True_False_True_0.0_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-31/2025-07-01-2025-07-13_1754005460.0496042.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7/1-7/13 BTC 10s 对冲 动态阈值 不过滤行情 挂单距离0\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-29/2025-07-01-2025-07-13_1753832383.2794082.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7/1-7/13 BTC 10s 对冲 动态阈值 过滤行情30s 0.0006 挂单距离0\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-29/2025-07-01-2025-07-13_1753834612.286644.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7/1-7/13 BTC 10s 对冲 动态阈值 过滤行情30s 0.0004 挂单距离0\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-29/2025-07-01-2025-07-13_1753836274.648629.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7/1-7/13 BTC 10s 对冲 动态阈值 过滤行情30s 0.001 挂单距离0\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-29/2025-07-01-2025-07-13_1753841273.707703.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7/1-7/13 BTC 10s 对冲 动态阈值 过滤行情30s 0.001 挂单距离0\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-29/2025-07-01-2025-07-13_1753844029.8325791.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6/1-6/30 BTC 10s 对冲 动态阈值 不过滤行情  挂单距离0\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-29/2025-06-01-2025-06-30_1753845496.175724.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21：37 start\n",
    "# 6/1-6/30 BTC 10s 对冲 动态阈值 过滤行情 0.001  挂单距离0\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-29/2025-06-01-2025-06-30_1753850243.431805.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6/1-6/30 BTC 10s 对冲 动态阈值 过滤行情 0.0006  挂单距离0\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-29/2025-06-01-2025-06-30_1753853237.0063202.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6/1-6/30 BTC 10s 对冲 动态阈值 过滤行情 0.002  挂单距离0\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-29/2025-06-01-2025-06-30_1753857127.5948448.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5/1 -5/30\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-30/2025-05-01-2025-05-30_1753907846.2815092.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5/1 -5/30 0.0006\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-30/2025-05-01-2025-05-30_1753911162.587743.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4月 benchmark\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-30/2025-04-01-2025-04-30_1753915849.388318.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.0006\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-30/2025-04-01-2025-04-30_1753921646.696481.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3月 benchmark\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-30/2025-03-01-2025-03-30_1753926890.0780668.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3月 0.0006\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-30/2025-03-01-2025-03-30_1753931933.668311.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-01/2025-07-01-2025-07-13_BTC_0.0_500.0_10_True_False_True_0.0001_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-01/2025-07-01-2025-07-13_BTC_0.0_500.0_10_True_False_True_0.0_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_df = pd.read_csv('/Users/rayxu/Downloads/2025072205_scored_features_swap.csv')\n",
    "scored_df = scored_df[scored_df.Symbol.isin (['BTC-USDT','ETH-USDT','XRP-USDT','SOL-USDT','DOGE-USDT','TRUMP-USDT','PENGU-USDT'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 挂单距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读入数据\n",
    "file_path1 = '/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_08_2 (6).csv'\n",
    "file_path2 = '/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_09_2 (5).csv'\n",
    "\n",
    "stats1, sell_stats1, buy_stats1, df1 = analyze_slippage(file_path1,starttime='2025-07-31 13:31:39')\n",
    "stats2, sell_stats2, buy_stats2, df2 = analyze_slippage(file_path2,starttime='2025-07-31 13:31:39')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def make_slippage_summary_table(stats1, sell_stats1, buy_stats1, stats2, sell_stats2, buy_stats2):\n",
    "    \"\"\"\n",
    "    将5s和60s的slippage统计dict拼成一个大表格\n",
    "\n",
    "    参数:\n",
    "    - stats1, sell_stats1, buy_stats1: 5s对冲的三个统计字典\n",
    "    - stats2, sell_stats2, buy_stats2: 60s对冲的三个统计字典\n",
    "\n",
    "    返回:\n",
    "    - DataFrame，包含比较结果\n",
    "    \"\"\"\n",
    "\n",
    "    # 统一所有字段名\n",
    "    keys = [\n",
    "        \"count\", \"mean\", \"median\", \"std\", \"min\", \"max\",\n",
    "        \"percentile_1\", \"percentile_5\", \"percentile_10\", \"percentile_25\",\n",
    "        \"percentile_50\", \"percentile_55\", \"percentile_60\", \"percentile_65\",\n",
    "        \"percentile_75\", \"percentile_95\", \"percentile_99\"\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"Metric\": keys,\n",
    "        \"d=0 Sell\": [sell_stats1.get(k, None) for k in keys],\n",
    "        \"d=0 Buy\": [buy_stats1.get(k, None) for k in keys],\n",
    "        \"d=0 All\": [stats1.get(k, None) for k in keys],\n",
    "        \"d=5 Sell\": [sell_stats2.get(k, None) for k in keys],\n",
    "        \"d=5 Buy\": [buy_stats2.get(k, None) for k in keys],\n",
    "        \"d=5 All\": [stats2.get(k, None) for k in keys],\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "df = make_slippage_summary_table(stats1, sell_stats1, buy_stats1, stats2, sell_stats2, buy_stats2)\n",
    "df\n",
    "# # 确保 Createtime 是 datetime 类型（如果是字符串格式）\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1['Createtime'] = pd.to_datetime(df1['Createtime'])\n",
    "# df2['Createtime'] = pd.to_datetime(df2['Createtime'])\n",
    "\n",
    "# # 排序\n",
    "# df1 = df1.sort_values('Createtime').reset_index(drop=True)\n",
    "# df2 = df2.sort_values('Createtime').reset_index(drop=True)\n",
    "\n",
    "# # 把 df1 的时间戳转为 numpy array（加快查找）\n",
    "# df1_times = df1['Createtime'].values.astype('datetime64[ms]')\n",
    "\n",
    "# # 设置时间阈值：100ms\n",
    "# threshold = np.timedelta64(1000, 'ms')\n",
    "\n",
    "# # 创建一个掩码数组：True 表示该行在 df2 中找不到接近时间，应该保留\n",
    "# keep_mask = []\n",
    "\n",
    "# for t2 in df2['Createtime'].values.astype('datetime64[ms]'):\n",
    "#     # 查找 df1 中是否有时间在 t2 ± 100ms 内\n",
    "#     time_diffs = np.abs(df1_times - t2)\n",
    "#     if np.any(time_diffs <= threshold):\n",
    "#         keep_mask.append(False)\n",
    "#     else:\n",
    "#         keep_mask.append(True)\n",
    "\n",
    "# # 筛选出 df2 中“独有”的行\n",
    "# df2_unique = df2[np.array(keep_mask)].reset_index(drop=True)\n",
    "\n",
    "# # 查看结果\n",
    "# print(f\"原始 df2 成交数: {len(df2)}\")\n",
    "# print(f\"匹配后剩余独有成交数: {len(df2_unique)}\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_extra_times(df1, df2, time_col='Createtime', tolerance_ms=100):\n",
    "    ts1 = pd.to_datetime(df1[time_col]).sort_values().reset_index(drop=True)\n",
    "    ts2 = pd.to_datetime(df2[time_col]).sort_values().reset_index(drop=True)\n",
    "\n",
    "    extra_indices = []\n",
    "\n",
    "    i = j = 0\n",
    "    n1, n2 = len(ts1), len(ts2)\n",
    "\n",
    "    for i in range(n1):\n",
    "        t1 = ts1[i]\n",
    "        # 二分推进 j，直到 ts2[j] > t1 - tolerance\n",
    "        while j < n2 and ts2[j] < t1 - pd.Timedelta(milliseconds=tolerance_ms):\n",
    "            j += 1\n",
    "        # 检查当前 j 是否在 t1 ± tolerance 内\n",
    "        if j < n2 and abs((ts2[j] - t1).total_seconds() * 1000) <= tolerance_ms:\n",
    "            continue  # 有匹配，跳过\n",
    "        else:\n",
    "            extra_indices.append(i)\n",
    "\n",
    "    return df1.loc[ts1.index[extra_indices]].copy()\n",
    "\n",
    "df1_extra_orders = find_extra_times(df1, df2, time_col='Timestamp', tolerance_ms=7000)\n",
    "df1_extra_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出 df2 独有成交（即 df1 中没有时间靠近的）\n",
    "df2_extra_orders = find_extra_times(df2, df1, time_col='Createtime', tolerance_ms=7000)\n",
    "df2_extra_orders['slippage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_time_matched(df1, df2, time_col='Createtime', tolerance_ms=1000):\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "    df1[time_col] = pd.to_datetime(df1[time_col])\n",
    "    df2[time_col] = pd.to_datetime(df2[time_col])\n",
    "\n",
    "    df1 = df1.sort_values(time_col).reset_index(drop=True)\n",
    "    df2 = df2.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    matches_df1 = []\n",
    "    matches_df2 = []\n",
    "\n",
    "    used_df2 = set()\n",
    "\n",
    "    for i, row1 in df1.iterrows():\n",
    "        t1 = np.datetime64(row1[time_col], 'ms')\n",
    "        df2_candidates = df2[~df2.index.isin(used_df2)]\n",
    "        diffs = np.abs(df2_candidates[time_col].values.astype('datetime64[ms]') - t1)\n",
    "        diffs_ms = diffs.astype('timedelta64[ms]').astype(int)\n",
    "\n",
    "        if len(diffs_ms) == 0:\n",
    "            continue\n",
    "\n",
    "        j = diffs_ms.argmin()\n",
    "        if diffs_ms[j] <= tolerance_ms:\n",
    "            row2 = df2_candidates.iloc[j]\n",
    "            matches_df1.append(row1)\n",
    "            matches_df2.append(row2)\n",
    "            used_df2.add(df2_candidates.index[j])\n",
    "\n",
    "    df1_filtered = pd.DataFrame(matches_df1).reset_index(drop=True)\n",
    "    df2_filtered = pd.DataFrame(matches_df2).reset_index(drop=True)\n",
    "\n",
    "    return df1_filtered, df2_filtered\n",
    "df1_filtered, df2_filtered = find_common_time_matched(df1, df2, tolerance_ms=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df1_filtered))\n",
    "print(len(df2_filtered))\n",
    "print(df1_filtered['slippage'].mean())\n",
    "print(df2_filtered['slippage'].mean())\n",
    "\n",
    "df1_filtered['slippage'].mean()\n",
    "df2_filtered['slippage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_position(df, time_col='Createtime', side_col='Side', amount_col='Amount'):\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    df = df.sort_values(time_col)\n",
    "\n",
    "    # 转换 Side 为方向：买入为 +1，卖出为 -1\n",
    "    df['signed_amount'] = df[amount_col] * df[side_col].map({'buy': 1, 'sell': -1})\n",
    "    \n",
    "    # 计算累积仓位\n",
    "    df['position'] = df['signed_amount'].cumsum()\n",
    "\n",
    "    return df[[time_col, 'position']]\n",
    "df1_pos = compute_position(df1, side_col='Side', amount_col='Order2FilledAmount')\n",
    "df2_pos = compute_position(df2, side_col='Side', amount_col='Order2FilledAmount')\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df1_pos['Createtime'],\n",
    "    y=df1_pos['position'],\n",
    "    mode='lines',\n",
    "    name='df1 Position',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df2_pos['Createtime'],\n",
    "    y=df2_pos['position'],\n",
    "    mode='lines',\n",
    "    name='df2 Position',\n",
    "    line=dict(color='red')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Position Over Time\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Cumulative Position\",\n",
    "    legend=dict(x=0, y=1),\n",
    "    hovermode=\"x unified\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def check_if_really_unmatched(df1_extra_orders, df2, time_col='Createtime', tolerance_ms=100):\n",
    "    \"\"\"\n",
    "    检查 df1_extra_orders 中的时间是否真的无法在 df2 中找到近似匹配。\n",
    "\n",
    "    返回一个 DataFrame，添加 matched_in_df2 和 min_time_diff_ms 两列\n",
    "    \"\"\"\n",
    "    df2_times = pd.to_datetime(df2[time_col]).sort_values().to_numpy(dtype='datetime64[ms]')\n",
    "    check_results = []\n",
    "    min_diffs = []\n",
    "\n",
    "    for t1 in pd.to_datetime(df1_extra_orders[time_col]):\n",
    "        t1_np = np.datetime64(t1, 'ms')\n",
    "        diffs = np.abs(df2_times - t1_np).astype('timedelta64[ms]').astype(int)\n",
    "        min_diff = diffs.min() if len(diffs) > 0 else np.inf\n",
    "        check_results.append(min_diff <= tolerance_ms)\n",
    "        min_diffs.append(min_diff)\n",
    "\n",
    "    result = df1_extra_orders.copy()\n",
    "    result['matched_in_df2'] = check_results\n",
    "    result['min_time_diff_ms'] = min_diffs\n",
    "    return result\n",
    "\n",
    "# 检查 tolerance=100ms 下，df1_extra_orders 中哪些其实可以和 df2 匹配\n",
    "check_df = check_if_really_unmatched(df1_extra_orders, df2, tolerance_ms=7000)\n",
    "\n",
    "# 统计一下\n",
    "print(check_df['matched_in_df2'].value_counts())\n",
    "print(f\"其实可以匹配上的数量: {(check_df['matched_in_df2']).sum()} / {len(check_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_extra_orders['slippage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_extra_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.0001-processed_df1[processed_df1.Side=='buy']['SR'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_08_2 (5).csv'\n",
    "stats1, sell_stats1, buy_stats1, processed_df1 = analyze_slippage(file_path,starttime='2025-08-05 05:30:00')\n",
    "file_path = '/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_09_2 (4).csv'\n",
    "stats2, sell_stats2, buy_stats2, processed_df2 = analyze_slippage(file_path,starttime='2025-08-05 05:30:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-0.0001-processed_df1[processed_df1.Side=='buy']['SR'].mean())\n",
    "print(-0.0001-processed_df2[processed_df2.Side=='buy']['SR'].mean())\n",
    "\n",
    "\n",
    "print(processed_df1[processed_df1.Side=='sell']['SR'].mean()-0.0001)\n",
    "print(processed_df2[processed_df2.Side=='sell']['SR'].mean()-0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1[processed_df1.Side=='sell']['SR'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_08_2 (5).csv'\n",
    "stats1, sell_stats1, buy_stats1, processed_df1 = analyze_slippage(file_path,starttime='2025-07-31 13:31:39')\n",
    "print(\"\\nOverall slippage statistics:\")\n",
    "print(stats1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_09_2 (4).csv'\n",
    "stats2, sell_stats2, buy_stats2, processed_df2 = analyze_slippage(file_path,starttime='2025-07-31 13:31:39')\n",
    "print(\"\\nOverall slippage statistics:\")\n",
    "print(stats2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[1000:1200]\n",
    "print(len(processed_df1[processed_df1.Side == 'sell']))\n",
    "print(len(processed_df2[processed_df2.Side == 'sell']))\n",
    "print(len(processed_df1[processed_df1.Side == 'buy']))\n",
    "print(len(processed_df2[processed_df2.Side == 'buy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[10:338]['slippage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# 筛选卖单\n",
    "sell_df1 = df1[df1.Side == 'sell'].copy()\n",
    "sell_df2 = df1[df1.Side == 'sell'].copy()\n",
    "\n",
    "# 转换时间\n",
    "sell_df1['Createtime'] = pd.to_datetime(sell_df1['Createtime'])\n",
    "sell_df2['Createtime'] = pd.to_datetime(sell_df2['Createtime'])\n",
    "\n",
    "# 计算每秒卖单数量（或指定时间粒度）\n",
    "sell_df1_count = sell_df1.resample('1s', on='Createtime').size().reset_index(name='count')\n",
    "sell_df2_count = sell_df2.resample('1s', on='Createtime').size().reset_index(name='count')\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# df1 线\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sell_df1_count['Createtime'],\n",
    "    y=sell_df1_count['count'],\n",
    "    mode='lines',\n",
    "    line=dict(color='blue', width=2),\n",
    "    name='df1 sell count'\n",
    "))\n",
    "\n",
    "# df2 线\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sell_df2_count['Createtime'],\n",
    "    y=sell_df2_count['count'],\n",
    "    mode='lines',\n",
    "    line=dict(color='red', width=2, dash='dot'),\n",
    "    name='df2 sell count'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    width=1800,\n",
    "    yaxis_title='Number of Sell Orders',\n",
    "    xaxis_title='Createtime',\n",
    "    title='Sell Order Time Distribution (Line Chart)',\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1['Createtime'] = pd.to_datetime(processed_df1['Createtime'])   \n",
    "processed_df1[(processed_df1.Createtime>pd.to_datetime('2025-08-03 09:51:10')) & (processed_df1.Createtime<pd.to_datetime('2025-08-03 11:10:15'))]['slippage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# 取出两个df中Side为'sell'的行\n",
    "sell_df1 = processed_df1[processed_df1.Side == 'sell'].copy()\n",
    "sell_df2 = processed_df2[processed_df2.Side == 'sell'].copy()\n",
    "\n",
    "# 转换Createtime为pandas的datetime格式\n",
    "sell_df1['Createtime'] = pd.to_datetime(sell_df1['Createtime'])\n",
    "sell_df2['Createtime'] = pd.to_datetime(sell_df2['Createtime'])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# 在同一个坐标轴上画出两个数据集的卖单时间分布\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sell_df1['Createtime'],\n",
    "    y=[1]*len(sell_df1),\n",
    "    mode='markers',\n",
    "    marker=dict(color='blue', size=10, symbol='circle'),\n",
    "    name='df1 sell Createtime'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sell_df2['Createtime'],\n",
    "    y=[1]*len(sell_df2),\n",
    "    mode='markers',\n",
    "    marker=dict(color='red', size=10, symbol='x'),\n",
    "    name='df2 sell Createtime'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    width=1800,\n",
    "    yaxis=dict(\n",
    "        tickvals=[1],\n",
    "        ticktext=['Sell Orders'],\n",
    "        title='Sell Orders'\n",
    "    ),\n",
    "    xaxis_title='Createtime',\n",
    "    title='Sell Order Createtime for Two DataFrames (Same Axis)',\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1['Order2FilledAmount'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1.drop_duplicates(subset=['OrderID'])['Amount'].sum()/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df2.drop_duplicates(subset=['OrderID'])['Amount'].sum()/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1['AmountFilled'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1[processed_df1.OrderID == 2724685136499286016.00000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1[processed_df1.Order2FilledAmount != 0.04]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1['Amount'].sum()/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df2['Amount'].sum()/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[processed_df.Side == 'buy']['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df[processed_df.Side == 'buy'].sort_values('slippage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_columns = ['Symbol', 'Date', 'tick_size_factor', 'q_lower', 'q_upper',\n",
    "    'q_range',\n",
    "       'ZCross_ratio', 'Mean_Crossings_ratio', 'Kurtosis', 'Half_Life', 'std',\n",
    "       'bpv', 'jump_share', 'upper_long_event_count',\n",
    "       'lower_long_event_count', 'event_switch_avg_gap_ms',\n",
    "       'upper_avg_duration', 'lower_avg_duration','total_opps', 'Exchange1',\n",
    "       'Exchange2', 'ex0_24h_usdt',\n",
    "       'ex1_24h_usdt', 'total_24h_usdt', 'rank_by_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_df[show_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 晚点对冲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 晚点对冲的benchmark\n",
    "\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-31/2025-07-01-2025-07-13_1753995426.373256.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 晚对冲 + 过滤极端行情 0.0006\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-30/2025-07-01-2025-07-13_1753936668.396888.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选大于0\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-30/2025-07-01-2025-07-13_1753939636.832428.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对冲时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-07/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5s vs 60s vs 60s过滤 vs 5s过滤\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-06/2025-08-03-2025-08-06_BTC_0.0_500.0_5_True_False_False_0.001_30_test_hedging_time_August.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-06/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_False_0.001_30_test_hedging_time_August.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-06/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_True_0.0_3_test_hedging_time_August_filter_price.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-06/2025-08-03-2025-08-06_BTC_0.0_500.0_5_True_False_True_0.0_3_test_hedging_time_August_filter_price.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7月\n",
    "import pandas as pd\n",
    "\n",
    "# 定义参数和文件路径\n",
    "delays = [5, 10, 15, 20, 25, 30, 60]\n",
    "results = []\n",
    "\n",
    "for delay in delays:\n",
    "    file_path = f'/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-0{1 if delay == 5 else 2}/2025-07-01-2025-07-13_BTC_0.0_500.0_{delay}_True_False_False_0.0_30.json'\n",
    "    if delay == 10:\n",
    "        file_path = f'/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-31/2025-07-01-2025-07-13_1754005460.0496042.json'\n",
    "    # 假设 analyze_slippage_backtest 返回 dict 或 Series，包含我们关心的统计指标\n",
    "    result = analyze_slippage_backtest(file_path)\n",
    "    # 如果返回值不是 dict/Series，需要根据实际情况调整\n",
    "    if hasattr(result, 'to_dict'):\n",
    "        result = result.to_dict()\n",
    "    result['delay'] = delay\n",
    "    results.append(result)\n",
    "\n",
    "# 转为 DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "# 调整列顺序，把 delay 放前面\n",
    "cols = ['delay'] + [col for col in df.columns if col != 'delay']\n",
    "df = df[cols]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6月版本跑通\n",
    "import pandas as pd\n",
    "\n",
    "# 定义参数和文件路径\n",
    "delays = [5, 10, 15, 20, 25, 30, 60]\n",
    "results = []\n",
    "\n",
    "for delay in delays:\n",
    "    if delay == 5:\n",
    "        file_path = '/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-01/2025-06-01-2025-06-30_BTC_0.0_500.0_5_True_False_False_0.0_30.json'\n",
    "    elif delay == 10:\n",
    "        file_path = '/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-01/2025-06-01-2025-06-30_BTC_0.0_500.0_10_True_False_False_0.0_30.json'\n",
    "    else:\n",
    "        file_path = f'/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-02/2025-06-01-2025-06-30_BTC_0.0_500.0_{delay}_True_False_False_0.0_30.json'\n",
    "    # 跑通 analyze_slippage_backtest\n",
    "    result = analyze_slippage_backtest(file_path)\n",
    "    if hasattr(result, 'to_dict'):\n",
    "        result = result.to_dict()\n",
    "    if result is not None:\n",
    "        result['delay'] = delay\n",
    "        results.append(result)\n",
    "    else:\n",
    "        # 如果返回None，补充空数据\n",
    "        results.append({'delay': delay})\n",
    "\n",
    "# 转为 DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "# 调整列顺序，把 delay 放前面\n",
    "if not df.empty:\n",
    "    cols = ['delay'] + [col for col in df.columns if col != 'delay']\n",
    "    df = df[cols]\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"无数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5月\n",
    "\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-01/2025-05-01-2025-05-31_BTC_0.0_500.0_5_True_False_False_0.0_30.json')\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-01/2025-05-01-2025-05-31_BTC_0.0_500.0_10_True_False_False_0.0_30.json')\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-02/2025-05-01-2025-05-31_BTC_0.0_500.0_15_True_False_False_0.0_30.json')\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-02/2025-05-01-2025-05-31_BTC_0.0_500.0_20_True_False_False_0.0_30.json')\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-02/2025-05-01-2025-05-31_BTC_0.0_500.0_25_True_False_False_0.0_30.json')\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-02/2025-05-01-2025-05-31_BTC_0.0_500.0_30_True_False_False_0.0_30.json')\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-02/2025-05-01-2025-05-31_BTC_0.0_500.0_60_True_False_False_0.0_30.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过滤时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-03/2025-07-01-2025-07-18_BTC_0.0_500.0_10_True_False_True_0.0_3.json')\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-03/2025-07-01-2025-07-18_BTC_0.0_500.0_10_True_False_True_0.0_5.json')\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-03/2025-07-01-2025-07-18_BTC_0.0_500.0_10_True_False_True_0.0_10.json')\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-03/2025-07-01-2025-07-18_BTC_0.0_500.0_10_True_False_True_0.0_20.json')\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-03/2025-07-01-2025-07-18_BTC_0.0_500.0_10_True_False_True_0.0_30.json')\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-03/2025-07-01-2025-07-18_BTC_0.0_500.0_10_True_False_True_0.0_60.json')\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-03/2025-07-01-2025-07-18_BTC_0.0_500.0_10_True_False_True_0.0_120.json')\n",
    "\n",
    "\n",
    "\n",
    "# 7月\n",
    "import pandas as pd\n",
    "\n",
    "# 定义参数和文件路径\n",
    "delays = [3, 5, 10, 20, 30, 60, 120]\n",
    "results = []\n",
    "\n",
    "for delay in delays:\n",
    "    file_path = f'/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-03/2025-07-01-2025-07-18_BTC_0.0_500.0_10_True_False_True_0.0_{delay}.json'\n",
    "    # 假设 analyze_slippage_backtest 返回 dict 或 Series，包含我们关心的统计指标\n",
    "    result = analyze_slippage_backtest(file_path)\n",
    "    # 如果返回值不是 dict/Series，需要根据实际情况调整\n",
    "    if hasattr(result, 'to_dict'):\n",
    "        result = result.to_dict()\n",
    "    result['FilterPeriod'] = delay\n",
    "    results.append(result)\n",
    "\n",
    "# 转为 DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "# 调整列顺序，把 delay 放前面\n",
    "cols = ['FilterPeriod'] + [col for col in df.columns if col != 'FilterPeriod']\n",
    "df = df[cols]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-01/2025-07-01-2025-07-13_BTC_0.0_500.0_10_True_False_True_0.0_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-01/2025-07-01-2025-07-13_BTC_0.0_500.0_10_True_False_True_0.0001_5.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-05/2025-07-01-2025-07-13_BTC_0.0_500.0_10_True_False_True_0.001_30_trend_by_regression.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-05/2025-07-01-2025-07-13_BTC_0.0_500.0_10_True_False_True_0.001_30_trend_by_regression.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-04/2025-07-01-2025-07-13_BTC_0.0_500.0_10_True_False_True_0.0006_30.json')\n",
    "\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-04/2025-07-01-2025-07-13_BTC_0.0_500.0_10_True_False_True_0.001_30.json')\n",
    "\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-04/2025-07-01-2025-07-13_BTC_0.0_500.0_10_True_False_True_0.001_30.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对冲时间实盘结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/rayxu/Downloads/order.arbitrage_btc_okx_binance_08_2 (4).csv'\n",
    "stats1, sell_stats1, buy_stats1, processed_df1 = analyze_slippage(file_path, starttime='2025-08-06 13:49:00')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/rayxu/Downloads/order.arbitrage_btc_okx_binance_09_2 (4).csv'\n",
    "stats2, sell_stats2, buy_stats2, processed_df2 = analyze_slippage(file_path, starttime='2025-08-06 13:49:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_slippage_summary_table(stats1, sell_stats1, buy_stats1, stats2, sell_stats2, buy_stats2):\n",
    "    \"\"\"\n",
    "    将5s和60s的slippage统计dict拼成一个大表格\n",
    "\n",
    "    参数:\n",
    "    - stats1, sell_stats1, buy_stats1: 5s对冲的三个统计字典\n",
    "    - stats2, sell_stats2, buy_stats2: 60s对冲的三个统计字典\n",
    "\n",
    "    返回:\n",
    "    - DataFrame，包含比较结果\n",
    "    \"\"\"\n",
    "\n",
    "    # 统一所有字段名\n",
    "    keys = [\n",
    "        \"count\", \"mean\", \"median\", \"std\", \"min\", \"max\",\n",
    "        \"percentile_1\", \"percentile_5\", \"percentile_10\", \"percentile_25\",\n",
    "        \"percentile_50\", \"percentile_55\", \"percentile_60\", \"percentile_65\",\n",
    "        \"percentile_75\", \"percentile_95\", \"percentile_99\"\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"Metric\": keys,\n",
    "        \"5s Sell\": [sell_stats1.get(k, None) for k in keys],\n",
    "        \"5s Buy\": [buy_stats1.get(k, None) for k in keys],\n",
    "        \"5s All\": [stats1.get(k, None) for k in keys],\n",
    "        \"60s Sell\": [sell_stats2.get(k, None) for k in keys],\n",
    "        \"60s Buy\": [buy_stats2.get(k, None) for k in keys],\n",
    "        \"60s All\": [stats2.get(k, None) for k in keys],\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "df = make_slippage_summary_table(stats1, sell_stats1, buy_stats1, stats2, sell_stats2, buy_stats2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df2[(processed_df2['Side']=='sell') & (processed_df2['slippage']<-0.0005)].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(processed_df2['slippage'].sum()+0.008)/862"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RollingStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计一下 对冲时间WaitLockTime的分布\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 处理 JSONDecodeError: Extra data\n",
    "file_path = '/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type2_每天更新一次vol_long.json'\n",
    "with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 尝试每行都是一个合法的json对象\n",
    "data = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析失败: {e}，内容: {line[:100]}...\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "if 'wait2Lock_interval' in df.columns:\n",
    "    print(df['wait2Lock_interval'].describe())\n",
    "    print(df['wait2Lock_interval'].value_counts())\n",
    "else:\n",
    "    print(\"列 'wait2Lock_interval' 不存在于 DataFrame 中\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二种方法，每天更新一次vol_long, 1个小时\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_720301440_shortterm_Type2_每天更新一次vol_long.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二种方法。每天更新一次vol_long， 50s的vol5s\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_10301440_shortterm_Type2_每天更新一次vol_long.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二种方法。每天更新一次vol_long， 5分钟的vol5s\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_60301440_shortterm_Type2_每天更新一次vol_long.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二种方法。mean，每天更新一次vol_long\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type2_每天更新一次vol_long.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二种方法 \n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一种方法 30分钟 1.1\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_36030_shortterm_coeff1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一种方法 30分钟\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_36030_shortterm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTimeTwoType.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-06/2025-08-03-2025-08-06_BTC_0.0_500.0_60_True_False_False_0.0_3_rolling_std_ratio.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 读取文件\n",
    "with open(\"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/price_and_vol_ratio\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 转成 DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 把 time 转成 pandas 时间类型\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "df['vol_ratio_normalized'] = df['vol_ratio'] / np.sqrt(12)\n",
    "df['vol_ratio_normalized'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vol_ratio_normalized'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['time'],\n",
    "    y=df['vol_ratio_normalized'],\n",
    "    mode='lines',\n",
    "    name='vol_ratio_normalized'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='vol_ratio_normalized over time',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='vol_ratio_normalized',\n",
    "    width=2000,   # 放大宽度\n",
    "    height=800    # 放大高度\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vol_ratio_normalized'].median().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['expected_vol_5s'] = df['vol_5s']*np.sqrt(12)\n",
    "df['indicator'] = df['expected_vol_5s'] - df['vol_long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['indicator'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.time > pd.Timestamp('2025-08-06 00:00:00')].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_position(df, time_col='Createtime', side_col='Side', amount_col='Amount'):\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    df = df.sort_values(time_col)\n",
    "\n",
    "    # 转换 Side 为方向：买入为 +1，卖出为 -1\n",
    "    df['signed_amount'] = df[amount_col] * df[side_col].map({'buy': 1, 'sell': -1})\n",
    "    \n",
    "    # 计算累积仓位\n",
    "    df['position'] = df['signed_amount'].cumsum()\n",
    "\n",
    "    return df[[time_col, 'position']]\n",
    "df1_pos = compute_position(df1, side_col='Side', amount_col='Order2FilledAmount')\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df1_pos['Createtime'],\n",
    "    y=df1_pos['position'],\n",
    "    mode='lines',\n",
    "    name='df1 Position',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Position Over Time\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Cumulative Position\",\n",
    "    legend=dict(x=0, y=1),\n",
    "    hovermode=\"x unified\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage(file_path1, starttime='2025-08-07 16:28:00', endtime='2025-08-07 18:30:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage(file_path1,starttime='2025-08-08 03:43:00', endtime='2025-08-08 07:30:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = '/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_08_2 (7).csv'\n",
    "\n",
    "stats1, sell_stats1, buy_stats1, df1 = analyze_slippage(file_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def check_if_really_unmatched(df1_extra_orders, df2, time_col='Createtime', tolerance_ms=100):\n",
    "    \"\"\"\n",
    "    检查 df1_extra_orders 中的时间是否真的无法在 df2 中找到近似匹配。\n",
    "\n",
    "    返回一个 DataFrame，添加 matched_in_df2 和 min_time_diff_ms 两列\n",
    "    \"\"\"\n",
    "    df2_times = pd.to_datetime(df2[time_col]).sort_values().to_numpy(dtype='datetime64[ms]')\n",
    "    check_results = []\n",
    "    min_diffs = []\n",
    "\n",
    "    for t1 in pd.to_datetime(df1_extra_orders[time_col]):\n",
    "        t1_np = np.datetime64(t1, 'ms')\n",
    "        diffs = np.abs(df2_times - t1_np).astype('timedelta64[ms]').astype(int)\n",
    "        min_diff = diffs.min() if len(diffs) > 0 else np.inf\n",
    "        check_results.append(min_diff <= tolerance_ms)\n",
    "        min_diffs.append(min_diff)\n",
    "\n",
    "    result = df1_extra_orders.copy()\n",
    "    result['matched_in_df2'] = check_results\n",
    "    result['min_time_diff_ms'] = min_diffs\n",
    "    return result\n",
    "\n",
    "# 检查 tolerance=100ms 下，df1_extra_orders 中哪些其实可以和 df2 匹配\n",
    "check_df = check_if_really_unmatched(df1_extra_orders, df2, tolerance_ms=7000)\n",
    "\n",
    "# 统计一下\n",
    "print(check_df['matched_in_df2'].value_counts())\n",
    "print(f\"其实可以匹配上的数量: {(check_df['matched_in_df2']).sum()} / {len(check_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def check_if_really_unmatched(df1_extra_orders, df2, time_col='Createtime', tolerance_ms=100):\n",
    "    \"\"\"\n",
    "    检查 df1_extra_orders 中的时间是否真的无法在 df2 中找到近似匹配。\n",
    "\n",
    "    返回一个 DataFrame，添加 matched_in_df2 和 min_time_diff_ms 两列\n",
    "    \"\"\"\n",
    "    df2_times = pd.to_datetime(df2[time_col]).sort_values().to_numpy(dtype='datetime64[ms]')\n",
    "    check_results = []\n",
    "    min_diffs = []\n",
    "\n",
    "    for t1 in pd.to_datetime(df1_extra_orders[time_col]):\n",
    "        t1_np = np.datetime64(t1, 'ms')\n",
    "        diffs = np.abs(df2_times - t1_np).astype('timedelta64[ms]').astype(int)\n",
    "        min_diff = diffs.min() if len(diffs) > 0 else np.inf\n",
    "        check_results.append(min_diff <= tolerance_ms)\n",
    "        min_diffs.append(min_diff)\n",
    "\n",
    "    result = df1_extra_orders.copy()\n",
    "    result['matched_in_df2'] = check_results\n",
    "    result['min_time_diff_ms'] = min_diffs\n",
    "    return result\n",
    "\n",
    "# 检查 tolerance=100ms 下，df1_extra_orders 中哪些其实可以和 df2 匹配\n",
    "check_df = check_if_really_unmatched(df1_extra_orders, df2, tolerance_ms=7000)\n",
    "\n",
    "# 统计一下\n",
    "print(check_df['matched_in_df2'].value_counts())\n",
    "print(f\"其实可以匹配上的数量: {(check_df['matched_in_df2']).sum()} / {len(check_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def check_if_really_unmatched(df1_extra_orders, df2, time_col='Createtime', tolerance_ms=100):\n",
    "    \"\"\"\n",
    "    检查 df1_extra_orders 中的时间是否真的无法在 df2 中找到近似匹配。\n",
    "\n",
    "    返回一个 DataFrame，添加 matched_in_df2 和 min_time_diff_ms 两列\n",
    "    \"\"\"\n",
    "    df2_times = pd.to_datetime(df2[time_col]).sort_values().to_numpy(dtype='datetime64[ms]')\n",
    "    check_results = []\n",
    "    min_diffs = []\n",
    "\n",
    "    for t1 in pd.to_datetime(df1_extra_orders[time_col]):\n",
    "        t1_np = np.datetime64(t1, 'ms')\n",
    "        diffs = np.abs(df2_times - t1_np).astype('timedelta64[ms]').astype(int)\n",
    "        min_diff = diffs.min() if len(diffs) > 0 else np.inf\n",
    "        check_results.append(min_diff <= tolerance_ms)\n",
    "        min_diffs.append(min_diff)\n",
    "\n",
    "    result = df1_extra_orders.copy()\n",
    "    result['matched_in_df2'] = check_results\n",
    "    result['min_time_diff_ms'] = min_diffs\n",
    "    return result\n",
    "\n",
    "# 检查 tolerance=100ms 下，df1_extra_orders 中哪些其实可以和 df2 匹配\n",
    "check_df = check_if_really_unmatched(df1_extra_orders, df2, tolerance_ms=7000)\n",
    "\n",
    "# 统计一下\n",
    "print(check_df['matched_in_df2'].value_counts())\n",
    "print(f\"其实可以匹配上的数量: {(check_df['matched_in_df2']).sum()} / {len(check_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Side']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['Side'] == 'buy']['slippage'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-01-2025-08-07_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-09/2025-08-01-2025-08-07_BTC_0.0_500.0_5_True_False_False_0.0_3_ControlGroup.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-01-2025-08-07_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DynamicHeging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_label_from_path(\"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-01-2025-08-07_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-21/2025-08-18-2025-08-21_ETH_0.0_500.0_60_True_False_False_0.0_3_dynamic_hedging_time_percentile_0.75.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-21/2025-08-18-2025-08-21_ETH_0.0_500.0_60_True_False_False_0.0_3_dynamic_hedging_time_对照.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-21/2025-08-18-2025-08-21_ETH_0.0_500.0_5_True_False_False_0.0_3_dynamic_hedging_time.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-21/2025-08-18-2025-08-21_ETH_0.0_500.0_5_True_False_False_0.0_3_dynamic_hedging_time_对照.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-21/2025-08-18-2025-08-21_ETH_0.0_500.0_60_True_False_False_0.0_3_dynamic_hedging_time_对照.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-01-2025-08-07_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "\n",
    "# ====== 你的路径分组 ======\n",
    "DynamicHedgingTimeType1 = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-01-2025-08-07_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-07-01-2025-07-31_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-06-01-2025-06-30_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-05-01-2025-05-30_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "]\n",
    "ControlGroup = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-01-2025-08-07_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-07-01-2025-07-31_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-06-01-2025-06-30_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-09/2025-05-01-2025-05-30_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "]\n",
    "DynamicHedgingTimeType2 = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-09/2025-05-01-2025-05-30_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type2.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-09/2025-06-01-2025-06-30_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type2.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-09/2025-07-01-2025-07-31_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type2.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-09/2025-08-01-2025-08-07_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type2.json\",\n",
    "]\n",
    "DynamicHedgingTimeEWMAOneSide = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-18/2025-05-01-2025-05-30_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_vol5_EWMA60.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-18/2025-06-01-2025-06-30_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_vol5_EWMA60.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-18/2025-07-01-2025-07-31_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_vol5_EWMA60.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-18/2025-08-01-2025-08-07_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_vol5_EWMA60.json\",\n",
    "]\n",
    "GROUPS = {\n",
    "    \"DynamicHedgingTimeType1\": DynamicHedgingTimeType1,\n",
    "    \"ControlGroup\": ControlGroup,\n",
    "    \"DynamicHedgingTimeType2\": DynamicHedgingTimeType2,\n",
    "    \"DynamicHedgingTimeEWMAOneSide\": DynamicHedgingTimeEWMAOneSide\n",
    "}\n",
    "COL_ORDER = [\"DynamicHedgingTimeType1\", \"ControlGroup\", \"DynamicHedgingTimeType2\",\"DynamicHedgingTimeEWMAOneSide\"]\n",
    "\n",
    "# 从文件名提取月份标签（用起始日期的 YYYY-MM）\n",
    "def period_label_from_path(p: str) -> str:\n",
    "    # 形如 2025-08-01-2025-08-07_...\n",
    "    m = re.search(r\"(\\d{4}-\\d{2})-\\d{4}-\\d{2}\", os.path.basename(p))\n",
    "    return m.group(1) if m else \"Unknown\"\n",
    "\n",
    "# —— 核心：直接用你的函数 —— #\n",
    "# 假设函数已在当前会话中可用\n",
    "# from your_module import analyze_slippage_backtest  # 若需从模块导入，解除注释并替换模块名\n",
    "\n",
    "period_to_data = {}\n",
    "for group_name, paths in GROUPS.items():\n",
    "    for p in paths:\n",
    "        period = period_label_from_path(p)\n",
    "        stats = analyze_slippage_backtest(p)  # <—— 直接调用\n",
    "        # stats 应为 dict\n",
    "        period_to_data.setdefault(period, {})\n",
    "        period_to_data[period][group_name] = pd.Series(stats)\n",
    "\n",
    "# 组装每个月 DataFrame\n",
    "dfs = {}\n",
    "for period, data in period_to_data.items():\n",
    "    df = pd.concat(data, axis=1)\n",
    "    # 按指定列顺序对齐（若某组缺失则会 KeyError，保险起见 reindex）\n",
    "    df = df.reindex(columns=COL_ORDER)\n",
    "    dfs[period] = df\n",
    "\n",
    "# 最终四个 df（不打印不保存）\n",
    "df_2025_05 = dfs.get(\"2025-05\")\n",
    "df_2025_06 = dfs.get(\"2025-06\")\n",
    "df_2025_07 = dfs.get(\"2025-07\")\n",
    "df_2025_08 = dfs.get(\"2025-08\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.7f}'.format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025_08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_json(\"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-19/2025-08-01-2025-08-07_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_with_truncated_1side_EWMA.json\",lines=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['wait2Lock_interval'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby('wait2Lock_interval')['slippage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[(df1['wait2Lock_interval']==60)&(df1['slippage']<-0.0003)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[(df1['wait2Lock_interval']==60)&(df1['slippage']>=-0.0003)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_json(\"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-01-2025-08-07_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",lines=True)\n",
    "df2['wait2Lock_interval'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby('wait2Lock_interval')['slippage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ==== 你的路径分组 ====\n",
    "DynamicHedgingTimeType1 = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-01-2025-08-07_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-07-01-2025-07-31_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-06-01-2025-06-30_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-05-01-2025-05-30_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "]\n",
    "ControlGroup = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-08-01-2025-08-07_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-07-01-2025-07-31_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-08/2025-06-01-2025-06-30_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-09/2025-05-01-2025-05-30_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type1.json\",\n",
    "]\n",
    "DynamicHedgingTimeType2 = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-09/2025-05-01-2025-05-30_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type2.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-09/2025-06-01-2025-06-30_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type2.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-09/2025-07-01-2025-07-31_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type2.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-09/2025-08-01-2025-08-07_BTC_0.0_500.0_60_True_False_False_0.0_3_DynamicHedgingTime_360301440_shortterm_Type2.json\",\n",
    "]\n",
    "\n",
    "DynamicHedgingTimeEWMAOneSide = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-18/2025-05-01-2025-05-30_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_vol5_EWMA60.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-18/2025-06-01-2025-06-30_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_vol5_EWMA60.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-18/2025-07-01-2025-07-31_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_vol5_EWMA60.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-18/2025-08-01-2025-08-07_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_vol5_EWMA60.json\",\n",
    "]\n",
    "\n",
    "DynamicHedgingTimeEWMATwoSide = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-18/2025-05-01-2025-05-30_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_EWMA5_EWMA60.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-18/2025-06-01-2025-06-30_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_EWMA5_EWMA60.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-18/2025-07-01-2025-07-31_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_EWMA5_EWMA60.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-18/2025-08-01-2025-08-07_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_EWMA5_EWMA60.json\",\n",
    "]\n",
    "\n",
    "DynamicHedgingTimeEWMAoneSideTruncated = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-19/2025-08-01-2025-08-07_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_with_truncated_1side_EWMA.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-19/2025-07-01-2025-07-31_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_with_truncated_1side_EWMA.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-19/2025-06-01-2025-06-30_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_with_truncated_1side_EWMA.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-19/2025-05-01-2025-05-30_BTC_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_with_truncated_1side_EWMA.json\",\n",
    "]\n",
    "\n",
    "GROUPS = {\n",
    "    \"DynamicHedgingTimeType1\": DynamicHedgingTimeType1,\n",
    "    \"ControlGroup\": ControlGroup,\n",
    "    \"DynamicHedgingTimeType2\": DynamicHedgingTimeType2,\n",
    "    \"DynamicHedgingTimeEWMAOneSide\": DynamicHedgingTimeEWMAOneSide,\n",
    "    \"DynamicHedgingTimeEWMATwoSide\": DynamicHedgingTimeEWMATwoSide,\n",
    "    \"DynamicHedgingTimeEWMAoneSideTruncated\": DynamicHedgingTimeEWMAoneSideTruncated\n",
    "}\n",
    "COL_ORDER = [\"DynamicHedgingTimeType1\", \"ControlGroup\", \"DynamicHedgingTimeType2\",\"DynamicHedgingTimeEWMAOneSide\",\"DynamicHedgingTimeEWMATwoSide\",\"DynamicHedgingTimeEWMAoneSideTruncated\"]\n",
    "\n",
    "# 从文件名提取起始月份（不用 re）\n",
    "def period_label_from_path(path: str) -> str:\n",
    "    base = os.path.basename(path)  # e.g. 2025-08-01-2025-08-07_BTC_...\n",
    "    first_token = base.split(\"_\", 1)[0]  # \"2025-08-01-2025-08-07\"\n",
    "    parts = first_token.split(\"-\")       # [\"2025\",\"08\",\"01\",\"2025\",\"08\",\"07\"]\n",
    "    if len(parts) >= 2:\n",
    "        return f\"{parts[0]}-{parts[1]}\"  # \"2025-08\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# —— 核心：直接用你的函数 —— #\n",
    "# 假设 analyze_slippage_backtest 已在当前环境中可用\n",
    "period_to_data = {}\n",
    "for group_name, paths in GROUPS.items():\n",
    "    for p in paths:\n",
    "        period = period_label_from_path(p)\n",
    "        stats = analyze_slippage_backtest(p)  # 返回 dict\n",
    "        period_to_data.setdefault(period, {})\n",
    "        period_to_data[period][group_name] = pd.Series(stats)\n",
    "\n",
    "# 组装每个月 DataFrame（列按要求顺序）\n",
    "dfs_by_period = {}\n",
    "for period, data in period_to_data.items():\n",
    "    df = pd.concat(data, axis=1)\n",
    "    df = df.reindex(columns=COL_ORDER)  # 若某组缺字段则为 NaN\n",
    "    dfs_by_period[period] = df\n",
    "\n",
    "# 最终四个 df（不打印不保存）\n",
    "df_2025_05 = dfs_by_period.get(\"2025-05\")\n",
    "df_2025_06 = dfs_by_period.get(\"2025-06\")\n",
    "df_2025_07 = dfs_by_period.get(\"2025-07\")\n",
    "df_2025_08 = dfs_by_period.get(\"2025-08\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过去四个小时大趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025_08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-08-01-2025-08-06_BTC_0.0_500.0_5_True_False_True_-0.01_14400_4h_filter_price.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === 你的三组路径 ===\n",
    "ControlGroup = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-08-01-2025-08-07_BTC_0.0_500.0_5_True_False_False_-0.01_14400_ControlGroup.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-07-01-2025-07-31_BTC_0.0_500.0_5_True_False_False_-0.01_14400_ControlGroup.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-06-01-2025-06-30_BTC_0.0_500.0_5_True_False_False_-0.01_14400_ControlGroup.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-05-01-2025-05-30_BTC_0.0_500.0_5_True_False_False_-0.01_14400_ControlGroup.json\",\n",
    "]\n",
    "Exp_m0p01 = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-08-01-2025-08-07_BTC_0.0_500.0_5_True_False_True_-0.01_14400_4h_filter_price.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-07-01-2025-07-31_BTC_0.0_500.0_5_True_False_True_-0.01_14400_4h_filter_price.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-06-01-2025-06-30_BTC_0.0_500.0_5_True_False_True_-0.01_14400_4h_filter_price.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-05-01-2025-05-30_BTC_0.0_500.0_5_True_False_True_-0.01_14400_4h_filter_price.json\",\n",
    "]\n",
    "Exp_m0p005 = [\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-08-01-2025-08-07_BTC_0.0_500.0_5_True_False_True_-0.005_14400_4h_filter_price.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-07-01-2025-07-31_BTC_0.0_500.0_5_True_False_True_-0.005_14400_4h_filter_price.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-06-01-2025-06-30_BTC_0.0_500.0_5_True_False_True_-0.005_14400_4h_filter_price.json\",\n",
    "    \"/Users/rayxu/Downloads/nuts_am/log/BTC/25-08-11/2025-05-01-2025-05-30_BTC_0.0_500.0_5_True_False_True_-0.005_14400_4h_filter_price.json\",\n",
    "]\n",
    "\n",
    "GROUPS = {\n",
    "    \"Exp(-0.01)\": Exp_m0p01,\n",
    "    \"ControlGroup\": ControlGroup,\n",
    "    \"Exp(-0.005)\": Exp_m0p005,\n",
    "}\n",
    "COL_ORDER = [\"Exp(-0.01)\", \"ControlGroup\", \"Exp(-0.005)\"]\n",
    "\n",
    "# —— 不用 re：从文件名取起始月份 YYYY-MM —— #\n",
    "def period_label_from_path(path: str) -> str:\n",
    "    name = os.path.basename(path)                         # e.g. '2025-08-01-2025-08-07_BTC_...'\n",
    "    first_token = name.split(\"_\", 1)[0]                   # '2025-08-01-2025-08-07'\n",
    "    # 起始日期在最前面，直接取前 7 位 -> 'YYYY-MM'\n",
    "    # 假设文件名规范始终以起始日期开头\n",
    "    return first_token[:7] if len(first_token) >= 7 else \"Unknown\"\n",
    "\n",
    "# —— 主逻辑：直接用你的函数 —— #\n",
    "def build_three_group_monthly_tables():\n",
    "    period_to_data = {}\n",
    "    for group_name, paths in GROUPS.items():\n",
    "        for p in paths:\n",
    "            period = period_label_from_path(p)\n",
    "            stats = analyze_slippage_backtest(p)  # 你的函数，返回 dict\n",
    "            period_to_data.setdefault(period, {})\n",
    "            period_to_data[period][group_name] = pd.Series(stats)\n",
    "\n",
    "    dfs_by_period = {}\n",
    "    for period, data in period_to_data.items():\n",
    "        df = pd.concat(data, axis=1)\n",
    "        df = df.reindex(columns=COL_ORDER)  # 列顺序固定：实验(-0.01)、对照、实验(-0.005)\n",
    "        dfs_by_period[period] = df\n",
    "\n",
    "    # 返回四个常用变量 + 按期字典\n",
    "    return (\n",
    "        dfs_by_period.get(\"2025-05\"),\n",
    "        dfs_by_period.get(\"2025-06\"),\n",
    "        dfs_by_period.get(\"2025-07\"),\n",
    "        dfs_by_period.get(\"2025-08\"),\n",
    "        dfs_by_period,\n",
    "    )\n",
    "\n",
    "# 用法：\n",
    "df_2025_05, df_2025_06, df_2025_07, df_2025_08, dfs_all = build_three_group_monthly_tables()\n",
    "# 然后直接使用这四个 df 即可（不保存不打印）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,df = analyze_slippage('/Users/rayxu/Downloads/order.arbitrage_soon_okx_binance_01_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Createtime'] = pd.to_datetime(df['Createtime'])\n",
    "df[df['Createtime'] >= pd.to_datetime('2025-08-12 16:07:00')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SR'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-13/2025-08-01-2025-08-06_ETH_0.0_500.0_5_True_False_False_-0.01_14400_DynamicHedgingTime_36030_Type1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-13/2025-08-01-2025-08-06_ETH_0.0_500.0_5_True_False_False_-0.01_14400_.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats1 = analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-13/2025-08-07-2025-08-13_ETH_0.0_500.0_5_True_False_False_-0.01_14400_.json')\n",
    "stats2 = analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-13/2025-08-07-2025-08-13_ETH_0.0_500.0_60_True_False_False_-0.01_14400_.json')\n",
    "stats3 = analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-13/2025-08-07-2025-08-13_ETH_0.0_500.0_5_True_False_False_-0.01_14400_DynamicHedgingTime_36030_Type1.json')\n",
    "stats4 = analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-14/2025-08-07-2025-08-13_ETH_0.0_500.0_60_True_False_True_0.0_3_DynamicHedging_with_filter.json')\n",
    "stats5 = analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-18/2025-08-07-2025-08-13_ETH_0.0_500.0_5_True_False_False_0.0_3_Test_DynamicHedgingTime_EWMA_vol5_EWMA60.json')\n",
    "stats6 = analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-18/2025-08-07-2025-08-13_ETH_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_with_truncated_1side_EWMA.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats5 = analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-18/2025-08-07-2025-08-13_ETH_0.0_500.0_5_True_False_False_0.0_3_Test_DynamicHedgingTime_EWMA.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-18/2025-08-07-2025-08-13_ETH_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_EWMA_EWMA5_EWMA60.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncated 2 side EWMA\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-18/2025-08-07-2025-08-13_ETH_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_360301440_type1_vol5_EWMA5_EWMA60_halflife_900.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-18/2025-08-07-2025-08-13_ETH_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_with_truncated_1side_EWMA.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-18/2025-08-07-2025-08-13_ETH_0.0_500.0_5_True_False_False_0.0_3_DynamicHedgingTime_with_truncated_1side_EWMA_1.05_0.95_5s.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([stats1, stats2, stats3,stats4,stats5,stats6]).transpose()\n",
    "df.columns = ['5s', '60s', '动态切换','动态切换+filter','动态切换+EWMA一边','动态切换 + truncated EWMA']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态对冲实盘结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,df2 = analyze_slippage('/Users/rayxu/Downloads/order.eth_okx_binance_08_2 (4).csv',starttime='2025-07-22 00:00:00',endtime='2025-08-25 03:30:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Createtime'] = pd.to_datetime(df['Createtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Createtime']>=pd.to_datetime('2025-08-21 01:40:00')) & (df['Createtime']<=pd.to_datetime('2025-08-21 01:49:00'))].sort_values(by = 'Createtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,df = analyze_slippage('/Users/rayxu/Downloads/order.eth_okx_binance_09_2 (7).csv',starttime='2025-07-22 00:00:00',endtime='2025-08-25 03:30:00')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hedgeType_timeParam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把df和df2 拼起来（用merge_asof),时间容忍度设成100ms，然后筛选hedgeType_timeParam 为true_1m0s或者false_1m0s，统计一下他们的slippage的差，然后看在时序上，是不是集中在某一段时间，还是比较均匀？\n",
    "import pandas as pd\n",
    "\n",
    "# 确保 Createtime 列为 datetime 类型\n",
    "df['Createtime'] = pd.to_datetime(df['Createtime'])\n",
    "df2['Createtime'] = pd.to_datetime(df2['Createtime'])\n",
    "\n",
    "# merge_asof 只能用于排序后的 DataFrame，且 on 的列必须为 datetime 或 numeric\n",
    "# 但还要确保 Createtime 没有重复值，否则 merge_asof 会报错\n",
    "df_sorted = df.sort_values('Createtime').drop_duplicates(subset=['Createtime'])\n",
    "df2_sorted = df2.sort_values('Createtime').drop_duplicates(subset=['Createtime'])\n",
    "\n",
    "# merge_asof 拼接，时间容忍度 100ms\n",
    "df_merged = pd.merge_asof(\n",
    "    df_sorted,\n",
    "    df2_sorted,\n",
    "    on='Createtime',\n",
    "    direction='nearest',\n",
    "    tolerance=pd.Timedelta('100ms'),\n",
    "    suffixes=('_dynamic', '_fixed')\n",
    ")\n",
    "\n",
    "# 2. 筛选 hedgeType_timeParam_dynamic 为 true_1m0s 或 false_1m0s\n",
    "mask = df_merged['hedgeType_timeParam_dynamic'].isin(['true_1m0s', 'false_1m0s'])\n",
    "filtered = df_merged[mask].copy()\n",
    "\n",
    "# 3. 统计 slippage 的差\n",
    "filtered['slippage_diff'] = filtered['slippage_dynamic'] - filtered['slippage_fixed']\n",
    "\n",
    "# 4. 查看 slippage_diff 的描述性统计和分位数\n",
    "desc = filtered['slippage_diff'].describe()\n",
    "percentiles = filtered['slippage_diff'].quantile([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n",
    "print(desc)\n",
    "print(\"分位数:\")\n",
    "for p, v in percentiles.items():\n",
    "    print(f\"{int(p*100)}%: {v}\")\n",
    "\n",
    "# 5. 按时间画出 slippage_diff 的时序分布\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(filtered['Createtime'], filtered['slippage_diff'], marker='.', linestyle='none', alpha=0.5)\n",
    "plt.xlabel('Createtime')\n",
    "plt.ylabel('slippage_diff')\n",
    "plt.title('Slippage Difference Over Time (true_1m0s/false_1m0s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered[filtered['slippage_diff']>0.002][['Createtime','ESR_dynamic','hedgeType_timeParam_dynamic','slippage_dynamic','slippage_fixed','slippage_diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把df和df2 拼起来（用merge_asof),时间容忍度设成100ms，然后筛选hedgeType_timeParam 为true_1m0s或者false_1m0s，统计一下他们的slippage的差，然后看在时序上，是不是集中在某一段时间，还是比较均匀？\n",
    "import pandas as pd\n",
    "\n",
    "# 确保 Createtime 列为 datetime 类型\n",
    "df['Createtime'] = pd.to_datetime(df['Createtime'])\n",
    "df2['Createtime'] = pd.to_datetime(df2['Createtime'])\n",
    "\n",
    "# merge_asof 只能用于排序后的 DataFrame，且 on 的列必须为 datetime 或 numeric\n",
    "# 但还要确保 Createtime 没有重复值，否则 merge_asof 会报错\n",
    "df_sorted = df.sort_values('Createtime').drop_duplicates(subset=['Createtime'])\n",
    "df2_sorted = df2.sort_values('Createtime').drop_duplicates(subset=['Createtime'])\n",
    "\n",
    "# merge_asof 拼接，时间容忍度 100ms\n",
    "df_merged = pd.merge_asof(\n",
    "    df_sorted,\n",
    "    df2_sorted,\n",
    "    on='Createtime',\n",
    "    direction='nearest',\n",
    "    tolerance=pd.Timedelta('100ms'),\n",
    "    suffixes=('_dynamic', '_fixed')\n",
    ")\n",
    "\n",
    "# 2. 筛选 hedgeType_timeParam_dynamic 为 true_1m0s 或 false_1m0s\n",
    "# mask = df_merged['hedgeType_timeParam_dynamic'].isin(['true_1m0s', 'false_1m0s'])\n",
    "# mask = df_merged['hedgeType_timeParam_dynamic'].isin(['true_1m0s'])\n",
    "mask = df_merged['hedgeType_timeParam_dynamic'].isin(['false_1m0s'])\n",
    "# mask = df_merged['hedgeType_timeParam_dynamic'].isin(['true_5s','false_5s'])\n",
    "filtered = df_merged[mask].copy()\n",
    "# 3. 统计 slippage 的差\n",
    "filtered['slippage_diff'] = filtered['slippage_dynamic'] - filtered['slippage_fixed']\n",
    "# filtered = filtered[filtered['slippage_diff']<0.002]\n",
    "\n",
    "# 4. 查看 slippage_diff 的描述性统计和分位数\n",
    "desc = filtered['slippage_diff'].describe()\n",
    "percentiles = filtered['slippage_diff'].quantile([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n",
    "print(desc)\n",
    "print(\"分位数:\")\n",
    "for p, v in percentiles.items():\n",
    "    print(f\"{int(p*100)}%: {v}\")\n",
    "\n",
    "# 5. 按时间画出 slippage_diff 的时序分布\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(filtered['Createtime'], filtered['slippage_diff'], marker='.', linestyle='none', alpha=0.5)\n",
    "plt.xlabel('Createtime')\n",
    "plt.ylabel('slippage_diff')\n",
    "plt.title('Slippage Difference Over Time (true_1m0s/false_1m0s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slippage_diff 的 频率分布呢？\n",
    "filtered['slippage_diff'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1114:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hedgeType_timeParam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['hedgeType_timeParam'] == 'false_5s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hedgeType_timeParam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['TimeUsed'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TimeUsed'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,df = analyze_slippage('/Users/rayxu/Downloads/order.eth_okx_binance_09_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.slippage <-0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取文件\n",
    "with open(\"/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-14/price_and_vol_ratio\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 转成 DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 把 time 转成 pandas 时间类型，自动推断格式（mixed），以避免 ValueError\n",
    "df['time'] = pd.to_datetime(df['time'], format='mixed', errors='coerce')\n",
    "\n",
    "df['vol_ratio_normalized'] = df['vol_ratio'] / np.sqrt(12)\n",
    "df['vol_ratio_normalized'].describe()\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['time'],\n",
    "    y=df['vol_ratio_normalized'],\n",
    "    mode='lines',\n",
    "    name='vol_ratio_normalized'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='vol_ratio_normalized over time',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='vol_ratio_normalized',\n",
    "    width=2000,   # 放大宽度\n",
    "    height=800    # 放大高度\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1000:]['vol_5s'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小时调仓和每日调仓对比（+PNL图）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_pnl_from_json(json_path: str):\n",
    "    \"\"\"\n",
    "    读取逐行 JSON 文件，去掉 pnl = cash 的行和 pnl < 0 的行，并画出 pnl 随时间的变化曲线\n",
    "    \"\"\"\n",
    "    # 逐行读取 json\n",
    "    df = pd.read_json(json_path, lines=True)\n",
    "    \n",
    "    # 转换时间格式\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # 去掉 pnl == cash 的行（假设有 'cash' 列）\n",
    "    # if 'cash' in df.columns and 'pnl' in df.columns:\n",
    "    #     df = df[df['pnl'] != df['cash']]\n",
    "    # df = df[df['pos'] == 0.0]\n",
    "    # # 去掉 pnl < 0 的行\n",
    "    # if 'pnl' in df.columns:\n",
    "    #     df = df[df['pnl'] >= 0]\n",
    "    \n",
    "    # 排序（保险起见）\n",
    "    df = df.sort_values('time')\n",
    "    \n",
    "    # 画图\n",
    "    fig = px.line(\n",
    "        df,\n",
    "        x='time',\n",
    "        y='pnl',\n",
    "        title=\"PnL 随时间变化（去除 pnl = cash 且去除 pnl < 0）\",\n",
    "        labels={\"time\": \"时间\", \"pnl\": \"PnL\"}\n",
    "    )\n",
    "    fig.update_traces(mode=\"lines+markers\")\n",
    "    fig.show()\n",
    "\n",
    "# 用法示例\n",
    "# plot_pnl_from_json(\"/Users/rayxu/Downloads/example.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pnl_from_json('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-19/2025-07-01-2025-08-19_ETH_0.0_500.0_5_True_False_False_0.0_3_daily_update.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_trade_net_curve(json_path):\n",
    "    \"\"\"\n",
    "    读取逐行 JSON 交易记录，计算净值曲线和pos随时间变化，并用 matplotlib 画图（只用折线，不用大圆点）。\n",
    "    统计整体换手率 = 总成交额/1000000/total_days，放在图片标题上。\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    trade_record = pd.read_json(json_path, lines=True)\n",
    "    trade_record['time'] = pd.to_datetime(trade_record['time'], errors='coerce')\n",
    "    trade_record.set_index('time', inplace=True)\n",
    "    trade_record['flag'] = 1\n",
    "    trade_record.loc[trade_record['type'] == 'Maker_ask', 'flag'] = -1\n",
    "    trade_record['pos'] = (trade_record['volume'] * trade_record['flag']).cumsum()\n",
    "    trade_record['net'] = (\n",
    "        (-trade_record['volume'] * trade_record['price'] * trade_record['flag']).cumsum()\n",
    "        + trade_record['pos'] * trade_record['price']\n",
    "        + (trade_record['volume'] * trade_record['price'] * 0.00005).cumsum()\n",
    "    )\n",
    "    trade_record['net_diff'] = trade_record['net'].diff()\n",
    "\n",
    "    # 计算总成交额\n",
    "    trade_record['turnover'] = trade_record['volume'] * trade_record['price']\n",
    "    total_turnover = trade_record['turnover'].sum()\n",
    "\n",
    "    # 计算总天数\n",
    "    if len(trade_record) > 0:\n",
    "        min_time = trade_record.index.min()\n",
    "        max_time = trade_record.index.max()\n",
    "        total_days = (max_time - min_time).total_seconds() / 86400\n",
    "        # 至少算1天，避免极端情况\n",
    "        total_days = max(total_days, 1)\n",
    "    else:\n",
    "        total_days = 1\n",
    "\n",
    "    turnover_rate = total_turnover / 1_000_000 / total_days\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # 画净值曲线（只用折线，不加marker）\n",
    "    ax1.plot(trade_record.index, trade_record['net'], linestyle='-', label='Net Value', color='tab:blue')\n",
    "    ax1.set_xlabel(\"Time\")\n",
    "    ax1.set_ylabel(\"PnL\", color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # 画pos曲线，使用第二y轴（只用折线，不加marker）\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(trade_record.index, trade_record['pos'], linestyle='--', label='Pos', color='tab:orange')\n",
    "    ax2.set_ylabel(\"Pos\", color='tab:orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.title(f\"Pnl Curve | Turnover: {turnover_rate:.4f} \")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 用法示例\n",
    "#plot_trade_net_curve('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-19/2025-07-01-2025-08-19_ETH_0.0_500.0_5_True_False_False_0.0_3_daily_update.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pnl_from_json('/Users/rayxu/Downloads/nuts_am/log/BTC/25-07-18/2025-07-01-2025-07-13_1752883486.132423.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trade_net_curve('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-20/2025-07-01-2025-08-19_ETH_0.0_500.0_5_True_False_False_0.0_3_3_day_update.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trade_net_curve('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-19/2025-07-01-2025-08-19_ETH_0.0_500.0_5_True_False_False_0.0_3_daily_update.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trade_net_curve('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-19/2025-07-01-2025-08-19_ETH_0.0_500.0_5_True_False_False_0.0_3_hourly_update.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "trade_record = pd.read_json('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-19/2025-07-01-2025-08-19_ETH_0.0_500.0_5_True_False_False_0.0_3_hourly_update.json', lines=True)\n",
    "trade_record['time'] = pd.to_datetime(trade_record['time'], errors='coerce')\n",
    "trade_record.set_index('time', inplace=True)\n",
    "trade_record['flag'] = 1\n",
    "trade_record.loc[trade_record['type'] == 'Maker_ask', 'flag'] = -1\n",
    "trade_record['pos'] = (trade_record['volume'] * trade_record['flag']).cumsum()\n",
    "trade_record['net'] = (\n",
    "    (-trade_record['volume'] * trade_record['price'] * trade_record['flag']).cumsum()\n",
    "    + trade_record['pos'] * trade_record['price']\n",
    "    + (trade_record['volume'] * trade_record['price'] * 0.00005).cumsum()\n",
    ")\n",
    "trade_record['net_diff'] = trade_record['net'].diff()\n",
    "trade_record['net_pos'] = (trade_record['volume'] * trade_record['flag']).cumsum()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=trade_record.index,\n",
    "    y=trade_record['net_pos'],\n",
    "    mode='lines',\n",
    "    name='net_pos'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='net_pos vs Time',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='net_pos',\n",
    "    template='plotly_white',\n",
    "    width=1600,   # 放大图像宽度\n",
    "    height=800    # 放大图像高度\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record['net_pos'][95000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record['price'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record['net'].tail(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# trade_record = pd.read_json('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-19/2025-07-01-2025-08-19_ETH_0.0_500.0_5_True_False_False_0.0_3_daily_update.json', lines=True)\n",
    "# trade_record['time'] = pd.to_datetime(trade_record['time'], errors='coerce')\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(trade_record.loc[trade_record['type'] == 'Maker_ask', 'time'], trade_record.loc[trade_record['type'] == 'Maker_ask', 'pos'], label='pos')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Pos')\n",
    "# plt.title('Pos vs Time (Maker_ask)')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record['pos'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 两个json文件路径\n",
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-19/2025-07-01-2025-08-19_ETH_0.0_500.0_5_True_False_False_0.0_3_daily_update.json'\n",
    "json_path2 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-19/2025-07-01-2025-08-19_ETH_0.0_500.0_5_True_False_False_0.0_3_hourly_update.json'\n",
    "json_path3 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-20/2025-07-01-2025-08-19_ETH_0.0_500.0_5_True_False_False_0.0_3_3_day_update.json'\n",
    "# 读取数据\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "trade_record2 = pd.read_json(json_path2, lines=True)\n",
    "trade_record3 = pd.read_json(json_path3, lines=True)\n",
    "\n",
    "# 统一时间格式\n",
    "trade_record1['time'] = pd.to_datetime(trade_record1['time'], errors='coerce')\n",
    "trade_record2['time'] = pd.to_datetime(trade_record2['time'], errors='coerce')\n",
    "trade_record3['time'] = pd.to_datetime(trade_record3['time'], errors='coerce')\n",
    "\n",
    "# 只保留8/13 10:00之前的数据\n",
    "cutoff = pd.Timestamp('2025-08-13 10:00:00')\n",
    "trade_record1 = trade_record1[trade_record1['time'] < cutoff].copy()\n",
    "trade_record2 = trade_record2[trade_record2['time'] < cutoff].copy()\n",
    "trade_record3 = trade_record3[trade_record3['time'] < cutoff].copy()\n",
    "\n",
    "# 计算flag\n",
    "for df in [trade_record1, trade_record2, trade_record3]:\n",
    "    df['flag'] = 1\n",
    "    df.loc[df['type'] == 'Maker_ask', 'flag'] = -1\n",
    "\n",
    "# 计算pos\n",
    "for df in [trade_record1, trade_record2, trade_record3]:\n",
    "    df['pos'] = (df['volume'] * df['flag']).cumsum()\n",
    "\n",
    "# 计算net\n",
    "for df in [trade_record1, trade_record2, trade_record3]:\n",
    "    df['net'] = (\n",
    "        (-df['volume'] * df['price'] * df['flag']).cumsum()\n",
    "        + df['pos'] * df['price']\n",
    "        + (df['volume'] * df['price'] * 0.00005).cumsum()\n",
    "    )\n",
    "\n",
    "# 计算年化收益率和换手率\n",
    "def calc_stats(df, principal):\n",
    "    # 只考虑有成交的行\n",
    "    df = df.copy()\n",
    "    df = df[df['volume'] > 0]\n",
    "    if df.empty:\n",
    "        return 0, 0, 0\n",
    "    start_time = df['time'].iloc[0]\n",
    "    end_time = df['time'].iloc[-1]\n",
    "    days = (end_time - start_time).total_seconds() / 86400\n",
    "    if days == 0:\n",
    "        days = 1/24  # 防止除零\n",
    "    pnl = df['net'].iloc[-1]\n",
    "    ann_return = pnl / principal / days * 365\n",
    "    turnover = (df['volume'] * df['price']).sum() / principal / days\n",
    "    return ann_return, turnover, days\n",
    "\n",
    "principal = 2_000_000\n",
    "\n",
    "ann_return1, turnover1, days1 = calc_stats(trade_record1, principal)\n",
    "ann_return2, turnover2, days2 = calc_stats(trade_record2, principal)\n",
    "ann_return3, turnover3, days3 = calc_stats(trade_record3, principal)\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trade_record1['time'], trade_record1['net'], label=f'daily update\\nret: {ann_return1:.2%}\\nturnover: {turnover1:.2f}', color='tab:blue')\n",
    "plt.plot(trade_record2['time'], trade_record2['net'], label=f'hourly update\\nret: {ann_return2:.2%}\\nturnover: {turnover2:.2f}', color='tab:orange')\n",
    "plt.plot(trade_record3['time'], trade_record3['net'], label=f'3 day update\\nret: {ann_return3:.2%}\\nturnover: {turnover3:.2f}', color='tab:green')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('PnL Curve before 2025-08-13 10:00\\n(Initial Capital 2000000)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过滤OIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('/Users/rayxu/Downloads/order.btc_okx_binance_08_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage('/Users/rayxu/Downloads/order.btc_okx_binance_08_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('/Users/rayxu/Downloads/order.btc_okx_binance_09_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage('/Users/rayxu/Downloads/order.btc_okx_binance_09_2.csv',starttime=pd.to_datetime('2025-09-05 00:00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向过滤\n",
    "stats1 = analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-09-03/2025-08-01-2025-08-31_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_反向过滤OIR_0.8.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 八月整对照\n",
    "stats2 = analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-09-02/2025-08-01-2025-08-31_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_对照组.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8月整\n",
    "stats3 = analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-09-02/2025-08-01-2025-08-31_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_反向过滤OIR_0.5_只取恶劣情况.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.DataFrame([stats1]).T, pd.DataFrame([stats2]).T, pd.DataFrame([stats3]).T],axis=1)\n",
    "df.columns = ['反向过滤OIR_0.8', '八月整对照', '过滤OIR_0.5']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 两个json文件路径\n",
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-09-02/2025-08-01-2025-08-31_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_对照组.json'\n",
    "# json_path2 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-29/2025-08-07-2025-08-13_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_test_反向过滤最近一个tick的_oir_0.8.json'\n",
    "json_path2 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-09-02/2025-08-01-2025-08-31_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_反向过滤OIR_0.5_只取恶劣情况.json'\n",
    "json_path3 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-09-03/2025-08-01-2025-08-31_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_反向过滤OIR_0.8.json'\n",
    "# 读取数据\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "trade_record2 = pd.read_json(json_path2, lines=True)\n",
    "trade_record3 = pd.read_json(json_path3, lines=True)\n",
    "\n",
    "# 统一时间格式\n",
    "trade_record1['time'] = pd.to_datetime(trade_record1['time'], errors='coerce')\n",
    "trade_record2['time'] = pd.to_datetime(trade_record2['time'], errors='coerce')\n",
    "trade_record3['time'] = pd.to_datetime(trade_record3['time'], errors='coerce')\n",
    "\n",
    "# 只保留8/13 10:00之前的数据\n",
    "# cutoff = pd.Timestamp('2025-08-13 10:00:00')\n",
    "# trade_record1 = trade_record1[trade_record1['time'] < cutoff].copy()\n",
    "# trade_record2 = trade_record2[trade_record2['time'] < cutoff].copy()\n",
    "# trade_record3 = trade_record3[trade_record3['time'] < cutoff].copy()\n",
    "\n",
    "# 计算flag\n",
    "for df in [trade_record1, trade_record2, trade_record3]:\n",
    "    df['flag'] = 1\n",
    "    df.loc[df['type'] == 'Maker_ask', 'flag'] = -1\n",
    "\n",
    "# 计算pos\n",
    "for df in [trade_record1, trade_record2, trade_record3]:\n",
    "    df['pos'] = (df['volume'] * df['flag']).cumsum()\n",
    "\n",
    "# 计算net\n",
    "for df in [trade_record1, trade_record2, trade_record3]:\n",
    "    df['net'] = (\n",
    "        (-df['volume'] * df['price'] * df['flag']).cumsum()\n",
    "        + df['pos'] * df['price']\n",
    "        + (df['volume'] * df['price'] * 0.00005).cumsum()\n",
    "    )\n",
    "\n",
    "# 计算年化收益率和换手率\n",
    "def calc_stats(df, principal):\n",
    "    # 只考虑有成交的行\n",
    "    df = df.copy()\n",
    "    df = df[df['volume'] > 0]\n",
    "    if df.empty:\n",
    "        return 0, 0, 0\n",
    "    start_time = df['time'].iloc[0]\n",
    "    end_time = df['time'].iloc[-1]\n",
    "    days = (end_time - start_time).total_seconds() / 86400\n",
    "    if days == 0:\n",
    "        days = 1/24  # 防止除零\n",
    "    pnl = df['net'].iloc[-1]\n",
    "    ann_return = pnl / principal / days * 365\n",
    "    turnover = (df['volume'] * df['price']).sum() / principal / days\n",
    "    return ann_return, turnover, days\n",
    "\n",
    "principal = 16_000_000\n",
    "\n",
    "ann_return1, turnover1, days1 = calc_stats(trade_record1, principal)\n",
    "ann_return2, turnover2, days2 = calc_stats(trade_record2, principal)\n",
    "ann_return3, turnover3, days3 = calc_stats(trade_record3, principal)\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trade_record1['time'], trade_record1['net'], label=f'Without OIR\\nret: {ann_return1:.2%}\\nturnover: {turnover1:.2f}', color='tab:blue')\n",
    "plt.plot(trade_record2['time'], trade_record2['net'], label=f'With OIR Type1\\nret: {ann_return2:.2%}\\nturnover: {turnover2:.2f}', color='tab:orange')\n",
    "plt.plot(trade_record3['time'], trade_record3['net'], label=f'With OIR Type2\\nret: {ann_return3:.2%}\\nturnover: {turnover3:.2f}', color='tab:green')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('PnL Curve \\n(Initial Capital 16000000)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-09-02/2025-08-07-2025-08-12_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_反向过滤OIR_0.8_正确版.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-30/2025-08-07-2025-08-12_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_反向过滤OIR_0.5_只取恶劣情况.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-29/2025-08-07-2025-08-12_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_OIR_ETH对照.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-29/2025-08-07-2025-08-12_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_反向过滤OIR_0.5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-29/2025-08-07-2025-08-13_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_test_反向过滤最近一个tick的_oir_0.8.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 两个json文件路径\n",
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-29/2025-08-07-2025-08-12_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_OIR_ETH对照.json'\n",
    "# json_path2 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-29/2025-08-07-2025-08-13_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_test_反向过滤最近一个tick的_oir_0.8.json'\n",
    "json_path2 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-30/2025-08-07-2025-08-12_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_反向过滤OIR_0.5_只取恶劣情况.json'\n",
    "json_path3 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-09-02/2025-08-07-2025-08-12_ETH_0.0_2000.0_5_True_False_False_-0.0006_30_反向过滤OIR_0.8_正确版.json'\n",
    "# 读取数据\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "trade_record2 = pd.read_json(json_path2, lines=True)\n",
    "trade_record3 = pd.read_json(json_path3, lines=True)\n",
    "\n",
    "# 统一时间格式\n",
    "trade_record1['time'] = pd.to_datetime(trade_record1['time'], errors='coerce')\n",
    "trade_record2['time'] = pd.to_datetime(trade_record2['time'], errors='coerce')\n",
    "trade_record3['time'] = pd.to_datetime(trade_record3['time'], errors='coerce')\n",
    "\n",
    "# 只保留8/13 10:00之前的数据\n",
    "# cutoff = pd.Timestamp('2025-08-13 10:00:00')\n",
    "# trade_record1 = trade_record1[trade_record1['time'] < cutoff].copy()\n",
    "# trade_record2 = trade_record2[trade_record2['time'] < cutoff].copy()\n",
    "# trade_record3 = trade_record3[trade_record3['time'] < cutoff].copy()\n",
    "\n",
    "# 计算flag\n",
    "for df in [trade_record1, trade_record2, trade_record3]:\n",
    "    df['flag'] = 1\n",
    "    df.loc[df['type'] == 'Maker_ask', 'flag'] = -1\n",
    "\n",
    "# 计算pos\n",
    "for df in [trade_record1, trade_record2, trade_record3]:\n",
    "    df['pos'] = (df['volume'] * df['flag']).cumsum()\n",
    "\n",
    "# 计算net\n",
    "for df in [trade_record1, trade_record2, trade_record3]:\n",
    "    df['net'] = (\n",
    "        (-df['volume'] * df['price'] * df['flag']).cumsum()\n",
    "        + df['pos'] * df['price']\n",
    "        + (df['volume'] * df['price'] * 0.00005).cumsum()\n",
    "    )\n",
    "\n",
    "# 计算年化收益率和换手率\n",
    "def calc_stats(df, principal):\n",
    "    # 只考虑有成交的行\n",
    "    df = df.copy()\n",
    "    df = df[df['volume'] > 0]\n",
    "    if df.empty:\n",
    "        return 0, 0, 0\n",
    "    start_time = df['time'].iloc[0]\n",
    "    end_time = df['time'].iloc[-1]\n",
    "    days = (end_time - start_time).total_seconds() / 86400\n",
    "    if days == 0:\n",
    "        days = 1/24  # 防止除零\n",
    "    pnl = df['net'].iloc[-1]\n",
    "    ann_return = pnl / principal / days * 365\n",
    "    turnover = (df['volume'] * df['price']).sum() / principal / days\n",
    "    return ann_return, turnover, days\n",
    "\n",
    "principal = 16_000_000\n",
    "\n",
    "ann_return1, turnover1, days1 = calc_stats(trade_record1, principal)\n",
    "ann_return2, turnover2, days2 = calc_stats(trade_record2, principal)\n",
    "ann_return3, turnover3, days3 = calc_stats(trade_record3, principal)\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trade_record1['time'], trade_record1['net'], label=f'Without OIR\\nret: {ann_return1:.2%}\\nturnover: {turnover1:.2f}', color='tab:blue')\n",
    "plt.plot(trade_record2['time'], trade_record2['net'], label=f'With OIR Type1\\nret: {ann_return2:.2%}\\nturnover: {turnover2:.2f}', color='tab:orange')\n",
    "plt.plot(trade_record3['time'], trade_record3['net'], label=f'With OIR Type2\\nret: {ann_return3:.2%}\\nturnover: {turnover3:.2f}', color='tab:green')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('PnL Curve \\n(Initial Capital 16000000)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-10-22/2025-10-18-2025-10-21_ETH_0.0_2000.0_5_True_False_False_0.0_0_.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/APT/25-10-22/2025-10-18-2025-10-21_APT_0.0_2000.0_5_False_False_False_0.0_0__fix_tick4_固定阈值.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 两个json文件路径\n",
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/APT/25-10-22/2025-10-18-2025-10-21_APT_0.0_2000.0_5_False_False_False_0.0_0__fix_tick4_固定阈值.json'\n",
    "\n",
    "# 读取数据\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "\n",
    "\n",
    "# 统一时间格式\n",
    "trade_record1['time'] = pd.to_datetime(trade_record1['time'], errors='coerce')\n",
    "\n",
    "\n",
    "# 只保留8/13 10:00之前的数据\n",
    "# cutoff = pd.Timestamp('2025-08-13 10:00:00')\n",
    "# trade_record1 = trade_record1[trade_record1['time'] < cutoff].copy()\n",
    "# trade_record2 = trade_record2[trade_record2['time'] < cutoff].copy()\n",
    "# trade_record3 = trade_record3[trade_record3['time'] < cutoff].copy()\n",
    "\n",
    "# 计算flag\n",
    "for df in [trade_record1]:\n",
    "    df['flag'] = 1\n",
    "    df.loc[df['type'] == 'Maker_ask', 'flag'] = -1\n",
    "\n",
    "# 计算pos\n",
    "for df in [trade_record1]:\n",
    "    df['pos'] = (df['volume'] * df['flag']).cumsum()\n",
    "\n",
    "# 计算net\n",
    "for df in [trade_record1]:\n",
    "    df['net'] = (\n",
    "        (-df['volume'] * df['price'] * df['flag']).cumsum()\n",
    "        + df['pos'] * df['price']\n",
    "        + (df['volume'] * df['price'] * 0.00005).cumsum()\n",
    "    )\n",
    "\n",
    "# 计算年化收益率和换手率\n",
    "def calc_stats(df, principal):\n",
    "    # 只考虑有成交的行\n",
    "    df = df.copy()\n",
    "    df = df[df['volume'] > 0]\n",
    "    if df.empty:\n",
    "        return 0, 0, 0\n",
    "    start_time = df['time'].iloc[0]\n",
    "    end_time = df['time'].iloc[-1]\n",
    "    days = (end_time - start_time).total_seconds() / 86400\n",
    "    if days == 0:\n",
    "        days = 1/24  # 防止除零\n",
    "    pnl = df['net'].iloc[-1]\n",
    "    ann_return = pnl / principal / days * 365\n",
    "    turnover = (df['volume'] * df['price']).sum() / principal / days\n",
    "    return ann_return, turnover, days\n",
    "\n",
    "principal = 16_000_000\n",
    "\n",
    "ann_return1, turnover1, days1 = calc_stats(trade_record1, principal)\n",
    "\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trade_record1['time'], trade_record1['net'], label=f'Without OIR\\nret: {ann_return1:.2%}\\nturnover: {turnover1:.2f}', color='tab:blue')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('PnL Curve \\n(Initial Capital 16000000)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3831.8/3809.9-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_by_midprice: False\n",
    "interval: 100\n",
    "order_limit: 100\n",
    "leverage: 3\n",
    "order_size: 1\n",
    "order_amount: 25 #没被用到\n",
    "sr_open_limit: 0.1 #没被用到\n",
    "d: 0\n",
    "max_pos: 2000\n",
    "max_pos_ex1: 30 # 没被用到\n",
    "wait2Lock_interval: 5\n",
    "IsDynamicHedgingTime: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新下单算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest(\"/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-07/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0__测试normal模式_滑点和开仓价差比.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest(\"/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-06/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_f'_测试normal模式_{min_spread_limit_bid}_{min_spread_limit_ask}'.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-06/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0__测试normal模式_对照组.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-06/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0__测试normal模式.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用反转"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-10/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0__-0.00007_0.00008_朴素模式.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-10/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0__-0.00007_0.00008_UseReversal1110.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-10/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0__-0.00007_0.00008_UseReversal1110_增加0.00005的价差限制.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-13/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0__-0.00007_0.00008_UseReversal1110_限制premium最大为0.00001.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, sell_percentiles, buy_percentiles, df = analyze_slippage('/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_09_2 (10).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AM回测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 两个json文件路径\n",
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-21/2025-11-01-2025-11-21_ETH_0.0_1000.0_5_False_False_False_0.0_0_获取AM曲线_11月.json'\n",
    "\n",
    "# 读取数据\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "\n",
    "# 统一时间格式\n",
    "trade_record1['time'] = pd.to_datetime(trade_record1['time'], errors='coerce')\n",
    "\n",
    "\n",
    "# 只保留8/13 10:00之前的数据\n",
    "cutoff = pd.Timestamp('2026-08-13 10:00:00')\n",
    "trade_record1 = trade_record1[trade_record1['time'] < cutoff].copy()\n",
    "\n",
    "\n",
    "# 计算flag\n",
    "for df in [trade_record1]:\n",
    "    df['flag'] = 1\n",
    "    df.loc[df['type'] == 'Maker_ask', 'flag'] = -1\n",
    "\n",
    "# 计算pos\n",
    "for df in [trade_record1]:\n",
    "    df['pos'] = (df['volume'] * df['flag']).cumsum()\n",
    "\n",
    "# 计算net\n",
    "for df in [trade_record1]:\n",
    "    df['net'] = (\n",
    "        (-df['volume'] * df['price'] * df['flag']).cumsum()\n",
    "        + df['pos'] * df['price']\n",
    "        + (df['volume'] * df['price'] * 0.00005).cumsum()\n",
    "    )\n",
    "\n",
    "# 计算年化收益率和换手率\n",
    "def calc_stats(df, principal):\n",
    "    # 只考虑有成交的行\n",
    "    df = df.copy()\n",
    "    df = df[df['volume'] > 0]\n",
    "    if df.empty:\n",
    "        return 0, 0, 0\n",
    "    start_time = df['time'].iloc[0]\n",
    "    end_time = df['time'].iloc[-1]\n",
    "    days = (end_time - start_time).total_seconds() / 86400\n",
    "    if days == 0:\n",
    "        days = 1/24  # 防止除零\n",
    "    pnl = df['net'].iloc[-1]\n",
    "    ann_return = pnl / principal / days * 365\n",
    "    turnover = (df['volume'] * df['price']).sum() / principal / days\n",
    "    return ann_return, turnover, days\n",
    "\n",
    "principal = 2_000_000\n",
    "\n",
    "ann_return1, turnover1, days1 = calc_stats(trade_record1, principal)\n",
    "\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trade_record1['time'], trade_record1['net'], label=f'daily update\\nret: {ann_return1:.2%}\\nturnover: {turnover1:.2f}', color='tab:blue')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('PnL Curve before 2025-08-13 10:00\\n(Initial Capital 2000000)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更长时间稳定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-10/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0__-0.00007_0.00008_朴素模式.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-25/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_长期时间稳定性叠加（10s）_-0.00007_0.00008.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-25/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_长期时间稳定性叠加（60s）_-0.00007_0.00008.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-25/2025-11-01-2025-11-20_ETH_0.0_2000.0_5_True_False_False_0.0_0_benchmark.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-25/2025-11-01-2025-11-20_ETH_0.0_2000.0_5_True_False_False_0.0_0_1min_稳定性.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-26/2025-10-01-2025-10-31_ETH_0.0_2000.0_5_True_False_False_0.0_0_95 5 benchmark.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-26/2025-10-01-2025-10-31_ETH_0.0_2000.0_5_True_False_False_0.0_0_95 5 更长时间稳定性.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_Sep import *\n",
    "symbol = 'ETH' \n",
    "\n",
    "okx_csv     = f'/Volumes/T7/Obentech/fundingRateData/okx/{symbol}-USDT-SWAP.csv'\n",
    "binance_csv = f'/Volumes/T7/Obentech/fundingRateData/binance/{symbol}USDT.csv'\n",
    "last_time = pd.to_datetime('2025-11-07 04:15:00')\n",
    "start_time = last_time - pd.Timedelta(days=30) \n",
    "df_okx     = process_funding_time_v3(okx_csv, 'okx')\n",
    "df_binance = process_funding_time_v3(binance_csv, 'binance')\n",
    "\n",
    "df_b = df_binance[(df_binance['Time'] >= start_time) & (df_binance['Time'] < last_time)].copy()\n",
    "df_o = df_okx[(df_okx['Time'] >= start_time) & (df_okx['Time'] < last_time)].copy()\n",
    "\n",
    "\n",
    "df_o = df_o.drop_duplicates(subset='FundingTime', keep='last')\n",
    "sum_okx = df_o['FundingRate'].sum()\n",
    "df_b = df_b.drop_duplicates(subset='FundingTime', keep='last')\n",
    "sum_bnb = df_b['FundingRate'].sum()\n",
    "earn    = sum_okx - sum_bnb\n",
    "day_start = last_time - pd.Timedelta(days=1)\n",
    "\n",
    "sum_okx1 = df_o[df_o['Time'] >= day_start]['FundingRate'].sum()\n",
    "sum_bnb1 = df_b[df_b['Time'] >= day_start]['FundingRate'].sum()\n",
    "earn_1day = sum_okx1 - sum_bnb1\n",
    "\n",
    "funding_interval_bn = int((df_b.iloc[-1]['FundingTime'] - df_b.iloc[-2]['FundingTime']).total_seconds() / 3600)\n",
    "funding_interval_okx = int((df_o.iloc[-1]['FundingTime'] - df_o.iloc[-2]['FundingTime']).total_seconds() / 3600)\n",
    "\n",
    "df_o.rename(columns={'FundingRate': 'FundingRate_okx'}, inplace=True)\n",
    "df_b.rename(columns={'FundingRate': 'FundingRate_binance'}, inplace=True)\n",
    "\n",
    "if funding_interval_bn == funding_interval_okx:\n",
    "    funding_diff = df_b[['FundingTime', 'FundingRate_binance']].set_index('FundingTime').join(df_o[['FundingTime', 'FundingRate_okx']].set_index('FundingTime'), how='left')\n",
    "elif funding_interval_bn > funding_interval_okx:\n",
    "    df_o_agg = df_o.set_index('FundingTime').resample(f'{funding_interval_bn}h', label='right', closed='right')['FundingRate_okx'].sum().to_frame()\n",
    "    funding_diff = df_b[['FundingTime', 'FundingRate_binance']].set_index('FundingTime').join(df_o_agg, how='left')\n",
    "else:\n",
    "    df_b_agg = df_b.set_index('FundingTime').resample(f'{funding_interval_okx}h', label='right', closed='right')['FundingRate_binance'].sum().to_frame()\n",
    "    funding_diff = df_o[['FundingTime', 'FundingRate_okx']].set_index('FundingTime').join(df_b_agg, how='left')\n",
    "\n",
    "\n",
    "funding_diff['funding_diff']     = funding_diff['FundingRate_okx'] - funding_diff['FundingRate_binance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 两个json文件路径\n",
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-26/2025-10-01-2025-10-31_ETH_0.0_2000.0_5_True_False_False_0.0_0_95 5 benchmark.json'\n",
    "json_path2 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-26/2025-10-01-2025-10-31_ETH_0.0_2000.0_5_True_False_False_0.0_0_95 5 更长时间稳定性.json'\n",
    "# json_path3 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-08-20/2025-07-01-2025-08-19_ETH_0.0_500.0_5_True_False_False_0.0_3_3_day_update.json'\n",
    "# 读取数据\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "trade_record2 = pd.read_json(json_path2, lines=True)\n",
    "# trade_record3 = pd.read_json(json_path3, lines=True)\n",
    "\n",
    "# 统一时间格式\n",
    "trade_record1['time'] = pd.to_datetime(trade_record1['time'], errors='coerce')\n",
    "trade_record2['time'] = pd.to_datetime(trade_record2['time'], errors='coerce')\n",
    "# trade_record3['time'] = pd.to_datetime(trade_record3['time'], errors='coerce')\n",
    "\n",
    "# 只保留8/13 10:00之前的数据\n",
    "cutoff = pd.Timestamp('2026-08-13 10:00:00')\n",
    "trade_record1 = trade_record1[trade_record1['time'] < cutoff].copy()\n",
    "trade_record2 = trade_record2[trade_record2['time'] < cutoff].copy()\n",
    "# trade_record3 = trade_record3[trade_record3['time'] < cutoff].copy()\n",
    "# trade_record1.dropna(subset=['slippage'], inplace=True)\n",
    "# trade_record2.dropna(subset=['slippage'], inplace=True)\n",
    "# # 计算flag\n",
    "# for df in [trade_record1, trade_record2]:\n",
    "#     df['flag'] = 1\n",
    "#     df.loc[df['type'] == 'Maker_ask', 'flag'] = -1\n",
    "\n",
    "# # 计算pos\n",
    "# for df in [trade_record1, trade_record2]:\n",
    "#     df['pos'] = (df['volume'] * df['flag']).cumsum()\n",
    "\n",
    "# # 计算net\n",
    "# for df in [trade_record1, trade_record2]:\n",
    "#     df['net'] = ((-df['volume'] * df['price'] * df['flag']).cumsum()+ df['pos'] * df['price']+ (df['volume'] * df['price'] * 0.00005).cumsum())\n",
    "\n",
    "def process_trade_record(trade_record):\n",
    "    trade_record['flag'] = 1\n",
    "    trade_record.loc[trade_record['type'] == 'Maker_ask', 'flag'] = -1\n",
    "    df_h = trade_record[trade_record['hedge_oid'].notna()].copy()\n",
    "    df_base = trade_record[['oid', 'price', 'volume']]\n",
    "    df_merge = df_h.merge(df_base,left_on='hedge_oid',right_on='oid',how='left',suffixes=('', '_base'))\n",
    "    df_merge['pos'] = (df_merge['volume'] * df_merge['flag']).cumsum()\n",
    "    df_merge['spread'] = df_merge['price'] - df_merge['price_base']\n",
    "    df_merge['RSR'] = df_merge['spread'] / df_merge['price_base']\n",
    "    df_merge['net'] = (df_merge['flag'] * df_merge['volume'] * df_merge['spread']).cumsum() + df_merge['pos'] * df_merge['spread'] + (df_merge['volume'] * df_merge['price'] * 0.0001).cumsum()\n",
    "    return df_merge\n",
    "# 计算年化收益率和换手率\n",
    "\n",
    "def calc_stats(df, principal,target_column='net'):\n",
    "    # 只考虑有成交的行\n",
    "    df = df.copy()\n",
    "    df = df[df['volume'] > 0]\n",
    "    if df.empty:\n",
    "        return 0, 0, 0\n",
    "    start_time = df['time'].iloc[0]\n",
    "    end_time = df['time'].iloc[-1]\n",
    "    days = (end_time - start_time).total_seconds() / 86400\n",
    "    if days == 0:\n",
    "        days = 1/24  # 防止除零\n",
    "    pnl = df[target_column].iloc[-1]\n",
    "    ann_return = pnl / principal / days * 365\n",
    "    turnover = (df['volume'] * df['price']).sum() / principal / days\n",
    "    return ann_return, turnover, days\n",
    "\n",
    "principal = 45_00_000\n",
    "trade_record1 = process_trade_record(trade_record1)\n",
    "trade_record2 = process_trade_record(trade_record2)\n",
    "\n",
    "trade_record1, df1 = total_pnl(trade_record1,funding_diff)\n",
    "trade_record2, df2 = total_pnl(trade_record2,funding_diff)\n",
    "\n",
    "ann_return1, turnover1, days1 = calc_stats(trade_record1, principal)\n",
    "ann_return2, turnover2, days2 = calc_stats(trade_record2, principal)\n",
    "\n",
    "total_ann_return1, _, _ = calc_stats(trade_record1, principal,target_column='total_pnl')\n",
    "total_ann_return2, _, _ = calc_stats(trade_record2, principal,target_column='total_pnl')\n",
    "# ann_return3, turnover3, days3 = calc_stats(trade_record3, principal)\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trade_record1[trade_record1['pos'] == 0]['time'], trade_record1[trade_record1['pos'] == 0]['total_pnl'], label=f'BenchMark\\nret: {total_ann_return1:.2%} turnover: {turnover1:.2f}', color='tab:blue')\n",
    "plt.plot(trade_record2[trade_record2['pos'] == 0]['time'], trade_record2[trade_record2['pos'] == 0]['total_pnl'], label=f'Longer Stability\\nret: {total_ann_return2:.2%} turnover: {turnover2:.2f}', color='tab:orange')\n",
    "# plt.plot(trade_record3['time'], trade_record3['net'], label=f'3 day update\\nret: {ann_return3:.2%}\\nturnover: {turnover3:.2f}', color='tab:green')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('PnL Curve before 2025-08-13 10:00\\n(Initial Capital 2000000)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 画图, funding_pnl\n",
    "\n",
    "funding_ret1 = df1['cum_fr_pnl'].iloc[-1]/principal/days1*365\n",
    "funding_ret2 = df2['cum_fr_pnl'].iloc[-1]/principal/days2*365\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df1['FundingTime'], df1['cum_fr_pnl'], label=f'BenchMark\\nret: {funding_ret1:.2%}', color='tab:blue')\n",
    "plt.plot(df2['FundingTime'], df2['cum_fr_pnl'], label=f'Longer Stability\\nret: {funding_ret2:.2%}', color='tab:orange')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('Funding PnL Curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_pnl(trade_record,funding_diff):\n",
    "    funding_diff = funding_diff.sort_values('FundingTime')\n",
    "    funding_diff = funding_diff[(funding_diff.index <= trade_record['time'].iloc[-1]) & (funding_diff.index >= trade_record['time'].iloc[0])]\n",
    "    trade_record = trade_record.sort_values('time')\n",
    "    df = pd.merge_asof(\n",
    "        funding_diff.reset_index(),\n",
    "        trade_record[['time', 'pos','price']],\n",
    "        left_on='FundingTime',\n",
    "        right_on='time',\n",
    "        direction='backward'\n",
    "    )\n",
    "\n",
    "    df['fr_pnl'] = df['funding_diff'] * df['pos'] * df['price']\n",
    "    df['cum_fr_pnl'] = df['fr_pnl'].cumsum()\n",
    "\n",
    "    final_result = pd.merge_asof(trade_record,df[['FundingTime','cum_fr_pnl']],left_on='time',right_on='FundingTime', direction='backward')\n",
    "\n",
    "    final_result = final_result.dropna()\n",
    "    final_result['total_pnl'] = final_result['net'] + final_result['cum_fr_pnl']\n",
    "    return final_result, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_diff = funding_diff.sort_values('FundingTime')\n",
    "funding_diff = funding_diff[(funding_diff.index <= trade_record1['time'].iloc[-1]) & (funding_diff.index >= trade_record1['time'].iloc[0])]\n",
    "trade_record1 = trade_record1.sort_values('time')\n",
    "\n",
    "df = pd.merge_asof(\n",
    "    funding_diff.reset_index(),\n",
    "    trade_record1[['time', 'pos','price']],\n",
    "    left_on='FundingTime',\n",
    "    right_on='time',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "df['fr_pnl'] = df['funding_diff'] * df['pos'] * df['price']\n",
    "df['cum_fr_pnl'] = df['fr_pnl'].cumsum()\n",
    "\n",
    "final_result = pd.merge_asof(trade_record1,df[['FundingTime','cum_fr_pnl']],left_on='time',right_on='FundingTime', direction='backward')\n",
    "\n",
    "final_result = final_result.dropna()\n",
    "final_result['total_pnl'] = final_result['net'] + final_result['cum_fr_pnl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trade_record1['volume'] * trade_record1['price'] * 0.00005).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1['pos'].head(30).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-25/2025-11-01-2025-11-20_ETH_0.0_2000.0_5_True_False_False_0.0_0_benchmark.json'\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "trade_record1['flag'] = 1\n",
    "trade_record1.loc[trade_record1['type'] == 'Maker_ask', 'flag'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h = trade_record1[trade_record1['hedge_oid'].notna()].copy()\n",
    "df_base = trade_record1[['oid', 'price', 'volume']]\n",
    "df_merge = df_h.merge(df_base,left_on='hedge_oid',right_on='oid',how='left',suffixes=('', '_base'))\n",
    "\n",
    "\n",
    "df_merge['spread'] = df_merge['price'] - df_merge['price_base']\n",
    "df_merge['RSR'] = df_merge['spread'] / df_merge['price_base']\n",
    "# df_merge['pos'] = (-df_merge['volume'] * df_merge['flag']).cumsum()\n",
    "# if IsOkxRebate:\n",
    "#     trades_2['fee'] = trades_2['Order2FilledPrice'] * trades_2['Order2FilledAmount'] *(-0.00005-0.00005)\n",
    "# else:\n",
    "#     trades_2['fee'] = trades_2['Order2FilledPrice'] * trades_2['Order2FilledAmount'] *(-0.00005)\n",
    "# trades_2['fee'] = trades_2['fee'].astype(float)\n",
    "df_merge['net'] = (df_merge['flag'] * df_merge['volume'] * df_merge['spread']).cumsum() + df_merge['pos'] * df_merge['spread'] + (df_merge['volume'] * df_merge['price'] * 0.0001).cumsum()\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h = trade_record1[trade_record1['hedge_oid'].notna()].copy()\n",
    "\n",
    "df_base = trade_record1[['oid', 'price', 'volume', 'flag']].rename(\n",
    "    columns={'price': 'price_base', 'volume': 'volume_base', 'flag': 'flag_base'}\n",
    ")\n",
    "\n",
    "# 拼接 hedge 对应的 base 单价格和数量\n",
    "df_merge = df_h.merge(df_base, left_on='hedge_oid', right_on='oid', how='left')\n",
    "\n",
    "# direction = base flag\n",
    "df_merge['direction'] = df_merge['flag_base']\n",
    "\n",
    "# 价差\n",
    "df_merge['spread'] = df_merge['price'] - df_merge['price_base']\n",
    "\n",
    "# 单对 hedge 盈亏\n",
    "df_merge['pnl'] = df_merge['direction'] * df_merge['spread'] * df_merge['volume_base']\n",
    "\n",
    "# 手续费\n",
    "df_merge['fee_base']  = df_merge['price_base'] * df_merge['volume_base'] * -0.00005\n",
    "df_merge['fee_hedge'] = df_merge['price']      * df_merge['volume_base'] * -0.00005\n",
    "\n",
    "df_merge['pnl_net'] = df_merge['pnl'] - df_merge['fee_base'] - df_merge['fee_hedge']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge[df_merge['pos'] == 0]['net'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['net'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1.dropna(subset=['slippage'], inplace=True)\n",
    "trade_record1['pos'] = (trade_record1['volume'] * trade_record1['flag']).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1['pos'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 两种模式的资金费率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_Sep import *\n",
    "symbol = 'ETH' \n",
    "\n",
    "okx_csv     = f'/Volumes/T7/Obentech/fundingRateData/okx/{symbol}-USDT-SWAP.csv'\n",
    "binance_csv = f'/Volumes/T7/Obentech/fundingRateData/binance/{symbol}USDT.csv'\n",
    "last_time = pd.to_datetime('2025-11-26 04:15:00')\n",
    "start_time = last_time - pd.Timedelta(days=60) \n",
    "df_okx     = process_funding_time_v3(okx_csv, 'okx')\n",
    "df_binance = process_funding_time_v3(binance_csv, 'binance')\n",
    "\n",
    "df_b = df_binance[(df_binance['Time'] >= start_time) & (df_binance['Time'] < last_time)].copy()\n",
    "df_o = df_okx[(df_okx['Time'] >= start_time) & (df_okx['Time'] < last_time)].copy()\n",
    "\n",
    "\n",
    "df_o = df_o.drop_duplicates(subset='FundingTime', keep='last')\n",
    "sum_okx = df_o['FundingRate'].sum()\n",
    "df_b = df_b.drop_duplicates(subset='FundingTime', keep='last')\n",
    "sum_bnb = df_b['FundingRate'].sum()\n",
    "earn    = sum_okx - sum_bnb\n",
    "day_start = last_time - pd.Timedelta(days=1)\n",
    "\n",
    "sum_okx1 = df_o[df_o['Time'] >= day_start]['FundingRate'].sum()\n",
    "sum_bnb1 = df_b[df_b['Time'] >= day_start]['FundingRate'].sum()\n",
    "earn_1day = sum_okx1 - sum_bnb1\n",
    "\n",
    "funding_interval_bn = int((df_b.iloc[-1]['FundingTime'] - df_b.iloc[-2]['FundingTime']).total_seconds() / 3600)\n",
    "funding_interval_okx = int((df_o.iloc[-1]['FundingTime'] - df_o.iloc[-2]['FundingTime']).total_seconds() / 3600)\n",
    "\n",
    "df_o.rename(columns={'FundingRate': 'FundingRate_okx'}, inplace=True)\n",
    "df_b.rename(columns={'FundingRate': 'FundingRate_binance'}, inplace=True)\n",
    "\n",
    "if funding_interval_bn == funding_interval_okx:\n",
    "    funding_diff = df_b[['FundingTime', 'FundingRate_binance']].set_index('FundingTime').join(df_o[['FundingTime', 'FundingRate_okx']].set_index('FundingTime'), how='left')\n",
    "elif funding_interval_bn > funding_interval_okx:\n",
    "    df_o_agg = df_o.set_index('FundingTime').resample(f'{funding_interval_bn}h', label='right', closed='right')['FundingRate_okx'].sum().to_frame()\n",
    "    funding_diff = df_b[['FundingTime', 'FundingRate_binance']].set_index('FundingTime').join(df_o_agg, how='left')\n",
    "else:\n",
    "    df_b_agg = df_b.set_index('FundingTime').resample(f'{funding_interval_okx}h', label='right', closed='right')['FundingRate_binance'].sum().to_frame()\n",
    "    funding_diff = df_o[['FundingTime', 'FundingRate_okx']].set_index('FundingTime').join(df_b_agg, how='left')\n",
    "\n",
    "\n",
    "funding_diff['funding_diff']     = funding_diff['FundingRate_okx'] - funding_diff['FundingRate_binance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 两个json文件路径\n",
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-25/2025-11-01-2025-11-20_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.0001-0.00005.json'\n",
    "json_path2 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-25/2025-11-01-2025-11-20_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00008-0.00007.json'\n",
    "json_path3 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-25/2025-11-01-2025-11-20_ETH_0.0_2000.0_5_True_False_False_0.0_0_benchmark.json'\n",
    "# 读取数据\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "trade_record2 = pd.read_json(json_path2, lines=True)\n",
    "trade_record3 = pd.read_json(json_path3, lines=True)\n",
    "\n",
    "# 统一时间格式\n",
    "trade_record1['time'] = pd.to_datetime(trade_record1['time'], errors='coerce')\n",
    "trade_record2['time'] = pd.to_datetime(trade_record2['time'], errors='coerce')\n",
    "trade_record3['time'] = pd.to_datetime(trade_record3['time'], errors='coerce')\n",
    "\n",
    "# 只保留8/13 10:00之前的数据\n",
    "# cutoff = pd.Timestamp('2026-08-13 10:00:00')\n",
    "# trade_record1 = trade_record1[trade_record1['time'] < cutoff].copy()\n",
    "# trade_record2 = trade_record2[trade_record2['time'] < cutoff].copy()\n",
    "# trade_record3 = trade_record3[trade_record3['time'] < cutoff].copy()\n",
    "# trade_record1.dropna(subset=['slippage'], inplace=True)\n",
    "# trade_record2.dropna(subset=['slippage'], inplace=True)\n",
    "# # 计算flag\n",
    "# for df in [trade_record1, trade_record2]:\n",
    "#     df['flag'] = 1\n",
    "#     df.loc[df['type'] == 'Maker_ask', 'flag'] = -1\n",
    "\n",
    "# # 计算pos\n",
    "# for df in [trade_record1, trade_record2]:\n",
    "#     df['pos'] = (df['volume'] * df['flag']).cumsum()\n",
    "\n",
    "# # 计算net\n",
    "# for df in [trade_record1, trade_record2]:\n",
    "#     df['net'] = ((-df['volume'] * df['price'] * df['flag']).cumsum()+ df['pos'] * df['price']+ (df['volume'] * df['price'] * 0.00005).cumsum())\n",
    "\n",
    "def process_trade_record(trade_record):\n",
    "    trade_record['flag'] = 1\n",
    "    trade_record.loc[trade_record['type'] == 'Maker_ask', 'flag'] = -1\n",
    "    df_h = trade_record[trade_record['hedge_oid'].notna()].copy()\n",
    "    df_base = trade_record[['oid', 'price', 'volume']]\n",
    "    df_merge = df_h.merge(df_base,left_on='hedge_oid',right_on='oid',how='left',suffixes=('', '_base'))\n",
    "    df_merge['pos'] = (df_merge['volume'] * df_merge['flag']).cumsum()\n",
    "    df_merge['spread'] = df_merge['price'] - df_merge['price_base']\n",
    "    df_merge['RSR'] = df_merge['spread'] / df_merge['price_base']\n",
    "    df_merge['net'] = (df_merge['flag'] * df_merge['volume'] * df_merge['spread']).cumsum() + df_merge['pos'] * df_merge['spread'] + (df_merge['volume'] * df_merge['price'] * 0.0001).cumsum()\n",
    "    df_merge.dropna(inplace=True)\n",
    "    return df_merge\n",
    "# 计算年化收益率和换手率\n",
    "\n",
    "def calc_stats(df, principal,target_column='net'):\n",
    "    # 只考虑有成交的行\n",
    "    df = df.copy()\n",
    "    df = df[df['volume'] > 0]\n",
    "    if df.empty:\n",
    "        return 0, 0, 0\n",
    "    start_time = df['time'].iloc[0]\n",
    "    end_time = df['time'].iloc[-1]\n",
    "    days = (end_time - start_time).total_seconds() / 86400\n",
    "    if days == 0:\n",
    "        days = 1/24  # 防止除零\n",
    "    pnl = df[target_column].iloc[-1]\n",
    "    ann_return = pnl / principal / days * 365\n",
    "    turnover = (df['volume'] * df['price']).sum() / principal / days\n",
    "    return ann_return, turnover, days\n",
    "\n",
    "principal = 45_00_000\n",
    "trade_record1 = process_trade_record(trade_record1)\n",
    "trade_record2 = process_trade_record(trade_record2)\n",
    "trade_record3 = process_trade_record(trade_record3)\n",
    "\n",
    "\n",
    "\n",
    "trade_record1, df1 = total_pnl(trade_record1,funding_diff)\n",
    "trade_record2, df2 = total_pnl(trade_record2,funding_diff)\n",
    "trade_record3, df3 = total_pnl(trade_record3,funding_diff)\n",
    "\n",
    "ann_return1, turnover1, days1 = calc_stats(trade_record1, principal)\n",
    "ann_return2, turnover2, days2 = calc_stats(trade_record2, principal)\n",
    "ann_return3, turnover3, days3 = calc_stats(trade_record3, principal)\n",
    "\n",
    "total_ann_return1, _, _ = calc_stats(trade_record1, principal,target_column='total_pnl')\n",
    "total_ann_return2, _, _ = calc_stats(trade_record2, principal,target_column='total_pnl')\n",
    "total_ann_return3, _, _ = calc_stats(trade_record3, principal,target_column='total_pnl')\n",
    "# ann_return3, turnover3, days3 = calc_stats(trade_record3, principal)\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trade_record1[trade_record1['pos'] == 0]['time'], trade_record1[trade_record1['pos'] == 0]['total_pnl'], label=f'-0.0001-0.00005\\nret: {total_ann_return1:.2%} turnover: {turnover1:.2f}', color='tab:blue')\n",
    "plt.plot(trade_record2[trade_record2['pos'] == 0]['time'], trade_record2[trade_record2['pos'] == 0]['total_pnl'], label=f'-0.00008-0.00007\\nret: {total_ann_return2:.2%} turnover: {turnover2:.2f}', color='tab:orange')\n",
    "plt.plot(trade_record3[trade_record3['pos'] == 0]['time'], trade_record3[trade_record3['pos'] == 0]['total_pnl'], label=f'95% 5%\\nret: {total_ann_return3:.2%} turnover: {turnover3:.2f}', color='tab:green')\n",
    "# plt.plot(trade_record3['time'], trade_record3['net'], label=f'3 day update\\nret: {ann_return3:.2%}\\nturnover: {turnover3:.2f}', color='tab:green')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('PnL Curve before 2025-08-13 10:00\\n(Initial Capital 2000000)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 画图, funding_pnl\n",
    "\n",
    "funding_ret1 = df1['cum_fr_pnl'].iloc[-1]/principal/days1*365\n",
    "funding_ret2 = df2['cum_fr_pnl'].iloc[-1]/principal/days2*365\n",
    "funding_ret3 = df3['cum_fr_pnl'].iloc[-1]/principal/days3*365\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df1['FundingTime'], df1['cum_fr_pnl'], label=f'BenchMark\\nret: {funding_ret1:.2%}', color='tab:blue')\n",
    "plt.plot(df2['FundingTime'], df2['cum_fr_pnl'], label=f'Longer Stability\\nret: {funding_ret2:.2%}', color='tab:orange')\n",
    "plt.plot(df3['FundingTime'], df3['cum_fr_pnl'], label=f'95% 5%\\nret: {funding_ret3:.2%}', color='tab:green')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('Funding PnL Curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-26/2025-10-15-2025-11-25_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.0001-0.00005.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 两个json文件路径\n",
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-26/2025-10-15-2025-11-25_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.0001-0.00005.json'\n",
    "json_path2 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-26/2025-10-15-2025-11-25_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00008-0.00007.json'\n",
    "json_path3 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-26/2025-10-15-2025-11-25_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00015-0.00005.json'\n",
    "json_path4 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-26/2025-10-15-2025-11-25_ETH_0.0_2000.0_5_True_False_False_0.0_0_95% 5%.json'\n",
    "\n",
    "# 读取数据\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "trade_record2 = pd.read_json(json_path2, lines=True)\n",
    "trade_record3 = pd.read_json(json_path3, lines=True)\n",
    "trade_record4 = pd.read_json(json_path4, lines=True)\n",
    "\n",
    "\n",
    "# 只保留8/13 10:00之前的数据\n",
    "# cutoff = pd.Timestamp('2026-08-13 10:00:00')\n",
    "# trade_record1 = trade_record1[trade_record1['time'] < cutoff].copy()\n",
    "# trade_record2 = trade_record2[trade_record2['time'] < cutoff].copy()\n",
    "# trade_record3 = trade_record3[trade_record3['time'] < cutoff].copy()\n",
    "# trade_record1.dropna(subset=['slippage'], inplace=True)\n",
    "# trade_record2.dropna(subset=['slippage'], inplace=True)\n",
    "\n",
    "def process_trade_record(trade_record):\n",
    "    trade_record['time'] = pd.to_datetime(trade_record['time'], errors='coerce')\n",
    "    trade_record['flag'] = 1\n",
    "    trade_record.loc[trade_record['type'] == 'Maker_ask', 'flag'] = -1\n",
    "    df_h = trade_record[trade_record['hedge_oid'].notna()].copy()\n",
    "    df_base = trade_record[['oid', 'price', 'volume']]\n",
    "    df_merge = df_h.merge(df_base,left_on='hedge_oid',right_on='oid',how='left',suffixes=('', '_base'))\n",
    "    df_merge['pos'] = (df_merge['volume'] * df_merge['flag']).cumsum()\n",
    "    df_merge['spread'] = df_merge['price'] - df_merge['price_base']\n",
    "    df_merge['RSR'] = df_merge['spread'] / df_merge['price_base']\n",
    "    df_merge['net'] = (df_merge['flag'] * df_merge['volume'] * df_merge['spread']).cumsum() + df_merge['pos'] * df_merge['spread'] + (df_merge['volume'] * df_merge['price'] * 0.0001).cumsum()\n",
    "    df_merge.dropna(inplace=True)\n",
    "    return df_merge\n",
    "# 计算年化收益率和换手率\n",
    "\n",
    "def calc_stats(df, principal,target_column='net'):\n",
    "    # 只考虑有成交的行\n",
    "    df = df.copy()\n",
    "    df = df[df['volume'] > 0]\n",
    "    if df.empty:\n",
    "        return 0, 0, 0\n",
    "    start_time = df['time'].iloc[0]\n",
    "    end_time = df['time'].iloc[-1]\n",
    "    days = (end_time - start_time).total_seconds() / 86400\n",
    "    if days == 0:\n",
    "        days = 1/24  # 防止除零\n",
    "    pnl = df[target_column].iloc[-1]\n",
    "    ann_return = pnl / principal / days * 365\n",
    "    turnover = (df['volume'] * df['price']).sum() / principal / days\n",
    "    return ann_return, turnover, days\n",
    "\n",
    "def total_pnl(trade_record,funding_diff):\n",
    "    funding_diff = funding_diff.sort_values('FundingTime')\n",
    "    funding_diff = funding_diff[(funding_diff.index <= trade_record['time'].iloc[-1]) & (funding_diff.index >= trade_record['time'].iloc[0])]\n",
    "    trade_record = trade_record.sort_values('time')\n",
    "    df = pd.merge_asof(\n",
    "        funding_diff.reset_index(),\n",
    "        trade_record[['time', 'pos','price']],\n",
    "        left_on='FundingTime',\n",
    "        right_on='time',\n",
    "        direction='backward'\n",
    "    )\n",
    "\n",
    "    df['fr_pnl'] = df['funding_diff'] * df['pos'] * df['price']\n",
    "    df['cum_fr_pnl'] = df['fr_pnl'].cumsum()\n",
    "\n",
    "    final_result = pd.merge_asof(trade_record,df[['FundingTime','cum_fr_pnl']],left_on='time',right_on='FundingTime', direction='backward')\n",
    "\n",
    "    final_result = final_result.dropna()\n",
    "    final_result['total_pnl'] = final_result['net'] + final_result['cum_fr_pnl']\n",
    "    return final_result, df\n",
    "\n",
    "principal = 45_00_000\n",
    "trade_record1 = process_trade_record(trade_record1)\n",
    "trade_record2 = process_trade_record(trade_record2)\n",
    "trade_record3 = process_trade_record(trade_record3)\n",
    "trade_record4 = process_trade_record(trade_record4)\n",
    "\n",
    "\n",
    "trade_record1, df1 = total_pnl(trade_record1,funding_diff)\n",
    "trade_record2, df2 = total_pnl(trade_record2,funding_diff)\n",
    "trade_record3, df3 = total_pnl(trade_record3,funding_diff)\n",
    "trade_record4, df4 = total_pnl(trade_record4,funding_diff)\n",
    "\n",
    "ann_return1, turnover1, days1 = calc_stats(trade_record1, principal)\n",
    "ann_return2, turnover2, days2 = calc_stats(trade_record2, principal)\n",
    "ann_return3, turnover3, days3 = calc_stats(trade_record3, principal)\n",
    "ann_return4, turnover4, days4 = calc_stats(trade_record4, principal)\n",
    "\n",
    "total_ann_return1, _, _ = calc_stats(trade_record1, principal,target_column='total_pnl')\n",
    "total_ann_return2, _, _ = calc_stats(trade_record2, principal,target_column='total_pnl')\n",
    "total_ann_return3, _, _ = calc_stats(trade_record3, principal,target_column='total_pnl')\n",
    "total_ann_return4, _, _ = calc_stats(trade_record4, principal,target_column='total_pnl')\n",
    "# ann_return3, turnover3, days3 = calc_stats(trade_record3, principal)\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trade_record1[trade_record1['pos'] == 0]['time'], trade_record1[trade_record1['pos'] == 0]['total_pnl'], label=f'-0.0001-0.00005\\nret: {total_ann_return1:.2%} turnover: {turnover1:.2f}', color='tab:blue')\n",
    "plt.plot(trade_record2[trade_record2['pos'] == 0]['time'], trade_record2[trade_record2['pos'] == 0]['total_pnl'], label=f'-0.00008-0.00007\\nret: {total_ann_return2:.2%} turnover: {turnover2:.2f}', color='tab:orange')\n",
    "plt.plot(trade_record3[trade_record3['pos'] == 0]['time'], trade_record3[trade_record3['pos'] == 0]['total_pnl'], label=f'-0.00015-0.00005%\\nret: {total_ann_return3:.2%} turnover: {turnover3:.2f}', color='tab:green')\n",
    "plt.plot(trade_record4[trade_record4['pos'] == 0]['time'], trade_record4[trade_record4['pos'] == 0]['total_pnl'], label=f'95% 5%\\nret: {total_ann_return4:.2%} turnover: {turnover4:.2f}', color='tab:red')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('PnL Curve before 2025-08-13 10:00\\n(Initial Capital 2000000)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 画图, funding_pnl\n",
    "\n",
    "funding_ret1 = df1['cum_fr_pnl'].iloc[-1]/principal/days1*365\n",
    "funding_ret2 = df2['cum_fr_pnl'].iloc[-1]/principal/days2*365\n",
    "funding_ret3 = df3['cum_fr_pnl'].iloc[-1]/principal/days3*365\n",
    "funding_ret4 = df4['cum_fr_pnl'].iloc[-1]/principal/days4*365\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df1['FundingTime'], df1['cum_fr_pnl'], label=f'-0.0001-0.00005\\nret: {funding_ret1:.2%}', color='tab:blue')\n",
    "plt.plot(df2['FundingTime'], df2['cum_fr_pnl'], label=f'-0.00008-0.00007\\nret: {funding_ret2:.2%}', color='tab:orange')\n",
    "plt.plot(df3['FundingTime'], df3['cum_fr_pnl'], label=f'-0.00015-0.00005\\nret: {funding_ret3:.2%}', color='tab:green')\n",
    "plt.plot(df4['FundingTime'], df4['cum_fr_pnl'], label=f'95% 5%\\nret: {funding_ret4:.2%}', color='tab:green')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('Funding PnL Curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resilience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_json('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-04/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_resilience_0.00015)在next里改)修正了check_signal版本)增加了取消订单)修正了取消订单的bug.json', lines=True)\n",
    "\n",
    "# df1[df1['time']>='2025-11-02 02:03:23.894000'].head(20)\n",
    "\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-08/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_resilience改变对冲时间0.0005.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_json('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-04/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_朴素)在next里改)修正了check_signal版本.json', lines=True)\n",
    "\n",
    "# df1[df1['time']>='2025-11-02 02:03:23.894000'].head(20)\n",
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-04/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_朴素)在next里改)修正了check_signal版本.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-26/2025-10-15-2025-11-25_ETH_0.0_2000.0_5_True_False_False_0.0_0_95% 5%.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-05/2025-10-15-2025-11-25_ETH_0.0_2000.0_5_True_False_False_0.0_0_resilience_0.00015)在next里改)修正了check_signal版本)增加了取消订单)只用ex1的信号.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-05/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_resilience_0.00015)在next里改)修正了check_signal版本)增加了取消订单)只用ex0的信号.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-05/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_resilience_0.001)-0.00007_0.00008在next里改)修正了check_signal版本)增加了取消订单)只用ex1的信号)增加print.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-05/2025-11-01-2025-11-05_1764978282.133318.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-04/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_朴素)在next里改)修正了check_signal版本.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_json('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-04/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_朴素)在next里改）.json',lines = True)\n",
    "df2 = pd.read_json('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-04/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_resilience_0.00015)在next里改）.json',lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建唯一标识符（时间+价格+交易类型）\n",
    "df1['key'] = df1['time'].astype(str) + '_' + df1['price'].astype(str) + '_' + df1['type']\n",
    "df2['key'] = df2['time'].astype(str) + '_' + df2['price'].astype(str) + '_' + df2['type']\n",
    "\n",
    "# 找出df2中多出的记录\n",
    "extra_in_df2 = df2[~df2['key'].isin(df1['key'])]\n",
    "print(f\"\\n文件2中多出的 {len(extra_in_df2)} 条记录:\")\n",
    "print(extra_in_df2[['time', 'oid', 'price', 'volume', 'type', 'pos', 'hedge_oid']])\n",
    "\n",
    "# 找出df1中有但df2中没有的记录（如果有）\n",
    "extra_in_df1 = df1[~df1['key'].isin(df2['key'])]\n",
    "if len(extra_in_df1) > 0:\n",
    "    print(f\"\\n文件1中有但文件2中没有的 {len(extra_in_df1)} 条记录:\")\n",
    "    print(extra_in_df1[['time', 'oid', 'price', 'volume', 'type', 'pos', 'hedge_oid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['oid']==41367]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['time'] == '2025-11-02 02:03:24.194000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选中df1 hedge_oid为空的行\n",
    "df1_null_hedge = df1[df1['hedge_oid'].isna()]\n",
    "df2_null_hedge = df2[df2['hedge_oid'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_null_hedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[~df1['hedge_oid'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[~df2['hedge_oid'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "13298-11988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_null_hedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[4095:4120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[4095:4120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_in_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析对冲配对情况\n",
    "print(\"\\n对冲配对分析:\")\n",
    "print(f\"文件1: 有对冲的订单 {df1['hedge_oid'].notna().sum()} 条\")\n",
    "print(f\"文件2: 有对冲的订单 {df2['hedge_oid'].notna().sum()} 条\")\n",
    "\n",
    "# 检查是否有重复的oid\n",
    "print(f\"\\n文件1 OID重复数: {df1['oid'].duplicated().sum()}\")\n",
    "print(f\"文件2 OID重复数: {df2['oid'].duplicated().sum()}\")\n",
    "\n",
    "# 查看多出记录的对冲情况\n",
    "if len(extra_in_df2) > 0:\n",
    "    print(\"\\n多出记录的对冲情况:\")\n",
    "    print(extra_in_df2[['time', 'oid', 'type', 'hedge_oid']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['type']=='Maker_bid'].loc[4100:4150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['type']=='Maker_bid'].loc[4100:4150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['type']=='Maker_ask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['type']=='Maker_ask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 基本统计对比\n",
    "print(\"=\" * 50)\n",
    "print(\"基本信息对比:\")\n",
    "print(f\"文件1总条数: {len(df1)}\")\n",
    "print(f\"文件2总条数: {len(df2)}\")\n",
    "print(f\"差异: {len(df2) - len(df1)} 条\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 交易类型分布对比\n",
    "print(\"\\n交易类型分布:\")\n",
    "print(\"文件1:\")\n",
    "print(df1['type'].value_counts())\n",
    "print(\"\\n文件2:\")\n",
    "print(df2['type'].value_counts())\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 时间范围对比\n",
    "print(\"\\n时间范围:\")\n",
    "print(f\"文件1: {df1['time'].min()} 到 {df1['time'].max()}\")\n",
    "print(f\"文件2: {df2['time'].min()} 到 {df2['time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 两个json文件路径\n",
    "from utils_Sep import *\n",
    "symbol = 'ETH' \n",
    "\n",
    "okx_csv     = f'/Volumes/T7/Obentech/fundingRateData/okx/{symbol}-USDT-SWAP.csv'\n",
    "binance_csv = f'/Volumes/T7/Obentech/fundingRateData/binance/{symbol}USDT.csv'\n",
    "last_time = pd.to_datetime('2025-11-07 04:15:00')\n",
    "start_time = last_time - pd.Timedelta(days=30) \n",
    "df_okx     = process_funding_time_v3(okx_csv, 'okx')\n",
    "df_binance = process_funding_time_v3(binance_csv, 'binance')\n",
    "\n",
    "df_b = df_binance[(df_binance['Time'] >= start_time) & (df_binance['Time'] < last_time)].copy()\n",
    "df_o = df_okx[(df_okx['Time'] >= start_time) & (df_okx['Time'] < last_time)].copy()\n",
    "\n",
    "\n",
    "df_o = df_o.drop_duplicates(subset='FundingTime', keep='last')\n",
    "sum_okx = df_o['FundingRate'].sum()\n",
    "df_b = df_b.drop_duplicates(subset='FundingTime', keep='last')\n",
    "sum_bnb = df_b['FundingRate'].sum()\n",
    "earn    = sum_okx - sum_bnb\n",
    "day_start = last_time - pd.Timedelta(days=1)\n",
    "\n",
    "sum_okx1 = df_o[df_o['Time'] >= day_start]['FundingRate'].sum()\n",
    "sum_bnb1 = df_b[df_b['Time'] >= day_start]['FundingRate'].sum()\n",
    "earn_1day = sum_okx1 - sum_bnb1\n",
    "\n",
    "funding_interval_bn = int((df_b.iloc[-1]['FundingTime'] - df_b.iloc[-2]['FundingTime']).total_seconds() / 3600)\n",
    "funding_interval_okx = int((df_o.iloc[-1]['FundingTime'] - df_o.iloc[-2]['FundingTime']).total_seconds() / 3600)\n",
    "\n",
    "df_o.rename(columns={'FundingRate': 'FundingRate_okx'}, inplace=True)\n",
    "df_b.rename(columns={'FundingRate': 'FundingRate_binance'}, inplace=True)\n",
    "\n",
    "if funding_interval_bn == funding_interval_okx:\n",
    "    funding_diff = df_b[['FundingTime', 'FundingRate_binance']].set_index('FundingTime').join(df_o[['FundingTime', 'FundingRate_okx']].set_index('FundingTime'), how='left')\n",
    "elif funding_interval_bn > funding_interval_okx:\n",
    "    df_o_agg = df_o.set_index('FundingTime').resample(f'{funding_interval_bn}h', label='right', closed='right')['FundingRate_okx'].sum().to_frame()\n",
    "    funding_diff = df_b[['FundingTime', 'FundingRate_binance']].set_index('FundingTime').join(df_o_agg, how='left')\n",
    "else:\n",
    "    df_b_agg = df_b.set_index('FundingTime').resample(f'{funding_interval_okx}h', label='right', closed='right')['FundingRate_binance'].sum().to_frame()\n",
    "    funding_diff = df_o[['FundingTime', 'FundingRate_okx']].set_index('FundingTime').join(df_b_agg, how='left')\n",
    "\n",
    "\n",
    "funding_diff['funding_diff']     = funding_diff['FundingRate_okx'] - funding_diff['FundingRate_binance']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-04/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_朴素)在next里改)修正了check_signal版本.json'\n",
    "json_path2 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-05/2025-11-01-2025-11-05_1764978282.133318.json'\n",
    "# 读取数据\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "trade_record2 = pd.read_json(json_path2, lines=True)\n",
    "\n",
    "\n",
    "# 统一时间格式\n",
    "trade_record1['time'] = pd.to_datetime(trade_record1['time'], errors='coerce')\n",
    "trade_record2['time'] = pd.to_datetime(trade_record2['time'], errors='coerce')\n",
    "def total_pnl(trade_record,funding_diff):\n",
    "    funding_diff = funding_diff.sort_values('FundingTime')\n",
    "    funding_diff = funding_diff[(funding_diff.index <= trade_record['time'].iloc[-1]) & (funding_diff.index >= trade_record['time'].iloc[0])]\n",
    "    trade_record = trade_record.sort_values('time')\n",
    "    df = pd.merge_asof(\n",
    "        funding_diff.reset_index(),\n",
    "        trade_record[['time', 'pos','price']],\n",
    "        left_on='FundingTime',\n",
    "        right_on='time',\n",
    "        direction='backward'\n",
    "    )\n",
    "\n",
    "    df['fr_pnl'] = df['funding_diff'] * df['pos'] * df['price']\n",
    "    df['cum_fr_pnl'] = df['fr_pnl'].cumsum()\n",
    "\n",
    "    final_result = pd.merge_asof(trade_record,df[['FundingTime','cum_fr_pnl']],left_on='time',right_on='FundingTime', direction='backward')\n",
    "\n",
    "    final_result = final_result.dropna()\n",
    "    final_result['total_pnl'] = final_result['net'] + final_result['cum_fr_pnl']\n",
    "    return final_result, df\n",
    "\n",
    "def process_trade_record(trade_record):\n",
    "    trade_record['flag'] = 1\n",
    "    trade_record.loc[trade_record['type'] == 'Maker_ask', 'flag'] = -1\n",
    "    df_h = trade_record[trade_record['hedge_oid'].notna()].copy()\n",
    "    df_base = trade_record[['oid', 'price', 'volume']]\n",
    "    df_merge = df_h.merge(df_base,left_on='hedge_oid',right_on='oid',how='left',suffixes=('', '_base'))\n",
    "    df_merge['pos'] = (df_merge['volume'] * df_merge['flag']).cumsum()\n",
    "    df_merge['spread'] = df_merge['price'] - df_merge['price_base']\n",
    "    df_merge['RSR'] = df_merge['spread'] / df_merge['price_base']\n",
    "    df_merge['net'] = (df_merge['flag'] * df_merge['volume'] * df_merge['spread']).cumsum() + df_merge['pos'] * df_merge['spread'] + (df_merge['volume'] * df_merge['price'] * 0.0001).cumsum()\n",
    "    df_merge.dropna(inplace=True)\n",
    "    return df_merge\n",
    "# 计算年化收益率和换手率\n",
    "\n",
    "def calc_stats(df, principal,target_column='net'):\n",
    "    # 只考虑有成交的行\n",
    "    df = df.copy()\n",
    "    df = df[df['volume'] > 0]\n",
    "    if df.empty:\n",
    "        return 0, 0, 0\n",
    "    start_time = df['time'].iloc[0]\n",
    "    end_time = df['time'].iloc[-1]\n",
    "    days = (end_time - start_time).total_seconds() / 86400\n",
    "    if days == 0:\n",
    "        days = 1/24  # 防止除零\n",
    "    pnl = df[target_column].iloc[-1]\n",
    "    ann_return = pnl / principal / days * 365\n",
    "    turnover = (df['volume'] * df['price']).sum() / principal / days\n",
    "    return ann_return, turnover, days\n",
    "\n",
    "principal = 45_00_000\n",
    "trade_record1 = process_trade_record(trade_record1)\n",
    "trade_record2 = process_trade_record(trade_record2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trade_record1, df1 = total_pnl(trade_record1,funding_diff)\n",
    "trade_record2, df2 = total_pnl(trade_record2,funding_diff)\n",
    "\n",
    "ann_return1, turnover1, days1 = calc_stats(trade_record1, principal)\n",
    "ann_return2, turnover2, days2 = calc_stats(trade_record2, principal)\n",
    "\n",
    "\n",
    "total_ann_return1, _, _ = calc_stats(trade_record1, principal,target_column='total_pnl')\n",
    "total_ann_return2, _, _ = calc_stats(trade_record2, principal,target_column='total_pnl')\n",
    "\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trade_record1[trade_record1['pos'] == 0]['time'], trade_record1[trade_record1['pos'] == 0]['total_pnl'], label=f'BM\\nret: {total_ann_return1:.2%} turnover: {turnover1:.2f}', color='tab:blue')\n",
    "plt.plot(trade_record2[trade_record2['pos'] == 0]['time'], trade_record2[trade_record2['pos'] == 0]['total_pnl'], label=f'Improve\\nret: {total_ann_return2:.2%} turnover: {turnover2:.2f}', color='tab:orange')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('PnL Curve before 2025-08-13 10:00\\n(Initial Capital 2000000)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 画图, funding_pnl\n",
    "\n",
    "funding_ret1 = df1['cum_fr_pnl'].iloc[-1]/principal/days1*365\n",
    "funding_ret2 = df2['cum_fr_pnl'].iloc[-1]/principal/days2*365\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df1['FundingTime'], df1['cum_fr_pnl'], label=f'BenchMark\\nret: {funding_ret1:.2%}', color='tab:blue')\n",
    "plt.plot(df2['FundingTime'], df2['cum_fr_pnl'], label=f'Improve\\nret: {funding_ret2:.2%}', color='tab:orange')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('Funding PnL Curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把trade_record1和trade_record2的pos随时间变化画在一张图里，用plotly\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 如果时间不是datetime，先转换（可选）\n",
    "# trade_record1['T'] = pd.to_datetime(trade_record1['T'])\n",
    "# trade_record2['T'] = pd.to_datetime(trade_record2['T'])\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": False}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=trade_record1['time'], \n",
    "        y=trade_record1['pos'],\n",
    "        mode='lines',\n",
    "        name='pos_1'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=trade_record2['time'], \n",
    "        y=trade_record2['pos'],\n",
    "        mode='lines',\n",
    "        name='pos_2'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Position Curve Over Time\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Position\",\n",
    "    hovermode=\"x unified\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 两个json文件路径\n",
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-04/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_朴素)在next里改)修正了check_signal版本.json'\n",
    "json_path2 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-04/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_resilience_0.00015)在next里改)修正了check_signal版本.json'\n",
    "# 读取数据\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "trade_record2 = pd.read_json(json_path2, lines=True)\n",
    "\n",
    "\n",
    "# 统一时间格式\n",
    "trade_record1['time'] = pd.to_datetime(trade_record1['time'], errors='coerce')\n",
    "trade_record2['time'] = pd.to_datetime(trade_record2['time'], errors='coerce')\n",
    "\n",
    "\n",
    "trade_record1['flag'] = 1\n",
    "trade_record1.loc[trade_record1['type'] == 'Maker_ask', 'flag'] = -1\n",
    "df_h1 = trade_record1[trade_record1['hedge_oid'].notna()].copy()\n",
    "# df_base1 = trade_record1[['oid', 'price', 'volume']]\n",
    "# df_merge1 = df_h1.merge(df_base1,left_on='hedge_oid',right_on='oid',how='left',suffixes=('', '_base'))\n",
    "# df_merge1['pos'] = (df_merge1['volume'] * df_merge1['flag']).cumsum()\n",
    "# df_merge1['spread'] = df_merge1['price'] - df_merge1['price_base']\n",
    "# df_merge1['RSR'] = df_merge1['spread'] / df_merge1['price_base']\n",
    "# df_merge1['net'] = (df_merge1['flag'] * df_merge1['volume'] * df_merge1['spread']).cumsum() + df_merge1['pos'] * df_merge1['spread'] + (df_merge1['volume'] * df_merge1['price'] * 0.0001).cumsum()\n",
    "# df_merge1.dropna(inplace=True)\n",
    "\n",
    "\n",
    "trade_record2['flag'] = 1\n",
    "trade_record2.loc[trade_record2['type'] == 'Maker_ask', 'flag'] = -1\n",
    "df_h2 = trade_record2[trade_record2['hedge_oid'].notna()].copy()\n",
    "# df_base2 = trade_record2[['oid', 'price', 'volume']]\n",
    "# df_merge2 = df_h2.merge(df_base1,left_on='hedge_oid',right_on='oid',how='left',suffixes=('', '_base'))\n",
    "# df_merge2['pos'] = (df_merge2['volume'] * df_merge2['flag']).cumsum()\n",
    "# df_merge2['spread'] = df_merge2['price'] - df_merge2['price_base']\n",
    "# df_merge2['RSR'] = df_merge2['spread'] / df_merge2['price_base']\n",
    "# df_merge2['net'] = (df_merge2['flag'] * df_merge2['volume'] * df_merge2['spread']).cumsum() + df_merge2['pos'] * df_merge2['spread'] + (df_merge2['volume'] * df_merge2['price'] * 0.0001).cumsum()\n",
    "# df_merge2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record2[~trade_record2['hedge_oid'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1[~trade_record1['hedge_oid'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把trade_record1和trade_record2的pos随时间变化画在一张图里，用plotly\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 如果时间不是datetime，先转换（可选）\n",
    "# trade_record1['T'] = pd.to_datetime(trade_record1['T'])\n",
    "# trade_record2['T'] = pd.to_datetime(trade_record2['T'])\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": False}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=trade_record1['time'], \n",
    "        y=trade_record1['pos'],\n",
    "        mode='lines',\n",
    "        name='pos_1'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=trade_record2['time'], \n",
    "        y=trade_record2['pos'],\n",
    "        mode='lines',\n",
    "        name='pos_2'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Position Curve Over Time\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Position\",\n",
    "    hovermode=\"x unified\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1[trade_record1['oid'] == 41369.00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1[trade_record1['hedge_oid'] == 41369.00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record2['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-04/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_朴素)在next里改）.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-04/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_resilience_0.00015)在next里改）.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-03/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_resilience_0.00015).json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_Sep import *\n",
    "symbol = 'ETH' \n",
    "\n",
    "okx_csv     = f'/Volumes/T7/Obentech/fundingRateData/okx/{symbol}-USDT-SWAP.csv'\n",
    "binance_csv = f'/Volumes/T7/Obentech/fundingRateData/binance/{symbol}USDT.csv'\n",
    "last_time = pd.to_datetime('2025-11-26 04:15:00')\n",
    "start_time = last_time - pd.Timedelta(days=30) \n",
    "df_okx     = process_funding_time_v3(okx_csv, 'okx')\n",
    "df_binance = process_funding_time_v3(binance_csv, 'binance')\n",
    "\n",
    "df_b = df_binance[(df_binance['Time'] >= start_time) & (df_binance['Time'] < last_time)].copy()\n",
    "df_o = df_okx[(df_okx['Time'] >= start_time) & (df_okx['Time'] < last_time)].copy()\n",
    "\n",
    "\n",
    "df_o = df_o.drop_duplicates(subset='FundingTime', keep='last')\n",
    "sum_okx = df_o['FundingRate'].sum()\n",
    "df_b = df_b.drop_duplicates(subset='FundingTime', keep='last')\n",
    "sum_bnb = df_b['FundingRate'].sum()\n",
    "earn    = sum_okx - sum_bnb\n",
    "day_start = last_time - pd.Timedelta(days=1)\n",
    "\n",
    "sum_okx1 = df_o[df_o['Time'] >= day_start]['FundingRate'].sum()\n",
    "sum_bnb1 = df_b[df_b['Time'] >= day_start]['FundingRate'].sum()\n",
    "earn_1day = sum_okx1 - sum_bnb1\n",
    "\n",
    "funding_interval_bn = int((df_b.iloc[-1]['FundingTime'] - df_b.iloc[-2]['FundingTime']).total_seconds() / 3600)\n",
    "funding_interval_okx = int((df_o.iloc[-1]['FundingTime'] - df_o.iloc[-2]['FundingTime']).total_seconds() / 3600)\n",
    "\n",
    "df_o.rename(columns={'FundingRate': 'FundingRate_okx'}, inplace=True)\n",
    "df_b.rename(columns={'FundingRate': 'FundingRate_binance'}, inplace=True)\n",
    "\n",
    "if funding_interval_bn == funding_interval_okx:\n",
    "    funding_diff = df_b[['FundingTime', 'FundingRate_binance']].set_index('FundingTime').join(df_o[['FundingTime', 'FundingRate_okx']].set_index('FundingTime'), how='left')\n",
    "elif funding_interval_bn > funding_interval_okx:\n",
    "    df_o_agg = df_o.set_index('FundingTime').resample(f'{funding_interval_bn}h', label='right', closed='right')['FundingRate_okx'].sum().to_frame()\n",
    "    funding_diff = df_b[['FundingTime', 'FundingRate_binance']].set_index('FundingTime').join(df_o_agg, how='left')\n",
    "else:\n",
    "    df_b_agg = df_b.set_index('FundingTime').resample(f'{funding_interval_okx}h', label='right', closed='right')['FundingRate_binance'].sum().to_frame()\n",
    "    funding_diff = df_o[['FundingTime', 'FundingRate_okx']].set_index('FundingTime').join(df_b_agg, how='left')\n",
    "\n",
    "\n",
    "funding_diff['funding_diff']     = funding_diff['FundingRate_okx'] - funding_diff['FundingRate_binance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage_backtest('/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-10/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0__-0.00007_0.00008_朴素模式.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 两个json文件路径\n",
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-10/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0__-0.00007_0.00008_朴素模式.json'\n",
    "json_path2 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-03/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_resilience_0.00015.json'\n",
    "\n",
    "# 读取数据\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "trade_record2 = pd.read_json(json_path2, lines=True)\n",
    "\n",
    "\n",
    "# 统一时间格式\n",
    "trade_record1['time'] = pd.to_datetime(trade_record1['time'], errors='coerce')\n",
    "trade_record2['time'] = pd.to_datetime(trade_record2['time'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "def process_trade_record(trade_record):\n",
    "    trade_record['flag'] = 1\n",
    "    trade_record.loc[trade_record['type'] == 'Maker_ask', 'flag'] = -1\n",
    "    df_h = trade_record[trade_record['hedge_oid'].notna()].copy()\n",
    "    df_base = trade_record[['oid', 'price', 'volume']]\n",
    "    df_merge = df_h.merge(df_base,left_on='hedge_oid',right_on='oid',how='left',suffixes=('', '_base'))\n",
    "    df_merge['pos'] = (df_merge['volume'] * df_merge['flag']).cumsum()\n",
    "    df_merge['spread'] = df_merge['price'] - df_merge['price_base']\n",
    "    df_merge['RSR'] = df_merge['spread'] / df_merge['price_base']\n",
    "    df_merge['net'] = (df_merge['flag'] * df_merge['volume'] * df_merge['spread']).cumsum() + df_merge['pos'] * df_merge['spread'] + (df_merge['volume'] * df_merge['price'] * 0.0001).cumsum()\n",
    "    df_merge.dropna(inplace=True)\n",
    "    return df_merge\n",
    "# 计算年化收益率和换手率\n",
    "\n",
    "def calc_stats(df, principal,target_column='net'):\n",
    "    # 只考虑有成交的行\n",
    "    df = df.copy()\n",
    "    df = df[df['volume'] > 0]\n",
    "    if df.empty:\n",
    "        return 0, 0, 0\n",
    "    start_time = df['time'].iloc[0]\n",
    "    end_time = df['time'].iloc[-1]\n",
    "    days = (end_time - start_time).total_seconds() / 86400\n",
    "    if days == 0:\n",
    "        days = 1/24  # 防止除零\n",
    "    pnl = df[target_column].iloc[-1]\n",
    "    ann_return = pnl / principal / days * 365\n",
    "    turnover = (df['volume'] * df['price']).sum() / principal / days\n",
    "    return ann_return, turnover, days\n",
    "\n",
    "principal = 45_00_000\n",
    "trade_record1 = process_trade_record(trade_record1)\n",
    "trade_record2 = process_trade_record(trade_record2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trade_record1, df1 = total_pnl(trade_record1,funding_diff)\n",
    "trade_record2, df2 = total_pnl(trade_record2,funding_diff)\n",
    "\n",
    "\n",
    "ann_return1, turnover1, days1 = calc_stats(trade_record1, principal)\n",
    "ann_return2, turnover2, days2 = calc_stats(trade_record2, principal)\n",
    "\n",
    "\n",
    "total_ann_return1, _, _ = calc_stats(trade_record1, principal,target_column='total_pnl')\n",
    "total_ann_return2, _, _ = calc_stats(trade_record2, principal,target_column='total_pnl')\n",
    "\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(12, 6))\n",
    "# plt.plot(trade_record1[trade_record1['pos'] == 0]['time'], trade_record1[trade_record1['pos'] == 0]['total_pnl'], label=f'BM\\nret: {total_ann_return1:.2%} turnover: {turnover1:.2f}', color='tab:blue')\n",
    "# plt.plot(trade_record2[trade_record2['pos'] == 0]['time'], trade_record2[trade_record2['pos'] == 0]['total_pnl'], label=f'Resilience\\nret: {total_ann_return2:.2%} turnover: {turnover2:.2f}', color='tab:orange')\n",
    "plt.plot(trade_record1['time'], trade_record1['total_pnl'], label=f'BM\\nret: {total_ann_return1:.2%} turnover: {turnover1:.2f}', color='tab:blue')\n",
    "plt.plot(trade_record2['time'], trade_record2['total_pnl'], label=f'Resilience\\nret: {total_ann_return2:.2%} turnover: {turnover2:.2f}', color='tab:orange')\n",
    "\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('PnL Curve before 2025-08-13 10:00\\n(Initial Capital 2000000)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 画图, funding_pnl\n",
    "\n",
    "funding_ret1 = df1['cum_fr_pnl'].iloc[-1]/principal/days1*365\n",
    "funding_ret2 = df2['cum_fr_pnl'].iloc[-1]/principal/days2*365\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df1['FundingTime'], df1['cum_fr_pnl'], label=f'BM\\nret: {funding_ret1:.2%}', color='tab:blue')\n",
    "plt.plot(df2['FundingTime'], df2['cum_fr_pnl'], label=f'Resilience\\nret: {funding_ret2:.2%}', color='tab:orange')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PnL')\n",
    "plt.title('Funding PnL Curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 两个json文件路径\n",
    "json_path1 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-11-10/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0__-0.00007_0.00008_朴素模式.json'\n",
    "json_path2 = '/Users/rayxu/Downloads/nuts_am/log/ETH/25-12-03/2025-11-01-2025-11-05_ETH_0.0_2000.0_5_False_False_False_0.0_0_-0.00007_0.00008_resilience_0.00015.json'\n",
    "\n",
    "# 读取数据\n",
    "trade_record1 = pd.read_json(json_path1, lines=True)\n",
    "trade_record2 = pd.read_json(json_path2, lines=True)\n",
    "\n",
    "\n",
    "# 统一时间格式\n",
    "trade_record1['time'] = pd.to_datetime(trade_record1['time'], errors='coerce')\n",
    "trade_record2['time'] = pd.to_datetime(trade_record2['time'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_trade_record(trade_record1)['pos'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record2['pos'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_pnl(trade_record,funding_diff):\n",
    "    funding_diff = funding_diff.sort_values('FundingTime')\n",
    "    funding_diff = funding_diff[(funding_diff.index <= trade_record['time'].iloc[-1]) & (funding_diff.index >= trade_record['time'].iloc[0])]\n",
    "    trade_record = trade_record.sort_values('time')\n",
    "    df = pd.merge_asof(\n",
    "        funding_diff.reset_index(),\n",
    "        trade_record[['time', 'pos','price']],\n",
    "        left_on='FundingTime',\n",
    "        right_on='time',\n",
    "        direction='backward'\n",
    "    )\n",
    "\n",
    "    df['fr_pnl'] = df['funding_diff'] * df['pos'] * df['price']\n",
    "    df['cum_fr_pnl'] = df['fr_pnl'].cumsum()\n",
    "\n",
    "    final_result = pd.merge_asof(trade_record,df[['FundingTime','cum_fr_pnl']],left_on='time',right_on='FundingTime', direction='backward')\n",
    "\n",
    "    final_result = final_result.dropna()\n",
    "    final_result['total_pnl'] = final_result['net'] + final_result['cum_fr_pnl']\n",
    "    return final_result, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1 = process_trade_record(trade_record1)\n",
    "\n",
    "trade_record1, df1 = total_pnl(trade_record1,funding_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_record1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实盘滑点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,df = analyze_slippage('/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_09_2 (13).csv','/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_09_2 (14).csv','/Users/rayxu/Downloads/order.arbitrage_eth_okx_binance_09_2 (15).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "# 数据预处理\n",
    "df['Createtime'] = pd.to_datetime(df['Createtime'])\n",
    "df['date'] = df['Createtime'].dt.date\n",
    "df['slippage'] = pd.to_numeric(df['slippage'], errors='coerce')\n",
    "df['HedgingTimeUsed'] = pd.to_numeric(df['HedgingTimeUsed'], errors='coerce')\n",
    "\n",
    "# 按日期分组\n",
    "grouped = df.groupby('date')\n",
    "\n",
    "# 1. 计算每天的滑点分位数\n",
    "percentiles = [1, 5, 10, 25, 50, 55, 60, 65, 75, 95, 99]\n",
    "percentile_data = {}\n",
    "\n",
    "for p in percentiles:\n",
    "    percentile_data[f'percentile_{p}'] = grouped['slippage'].quantile(p/100)\n",
    "\n",
    "percentile_df = pd.DataFrame(percentile_data)\n",
    "\n",
    "# 2. 计算每天的滑点均值\n",
    "mean_df = grouped['slippage'].mean()\n",
    "\n",
    "# 3. 计算每天HedgingTimeUsed<=5的比例\n",
    "hedging_ratio = grouped.apply(\n",
    "    lambda x: (x['HedgingTimeUsed'] <= 5).sum() / x['HedgingTimeUsed'].notna().sum() * 100\n",
    ")\n",
    "\n",
    "\n",
    "# 3. 计算每天HedgingTimeUsed<=5的比例\n",
    "hedging_ratio_lte5 = grouped.apply(\n",
    "    lambda x: (x['HedgingTimeUsed'] <= 5).sum() / x['HedgingTimeUsed'].notna().sum()\n",
    ")\n",
    "\n",
    "# 计算每天HedgingTimeUsed>5的比例\n",
    "hedging_ratio_gt5 = grouped.apply(\n",
    "    lambda x: (x['HedgingTimeUsed'] > 5).sum() / x['HedgingTimeUsed'].notna().sum()\n",
    ")\n",
    "\n",
    "\n",
    "# 4. 计算HedgingTimeUsed<=5和>5的滑点均值\n",
    "slippage_lte5 = grouped.apply(\n",
    "    lambda x: x[x['HedgingTimeUsed'] <= 5]['slippage'].mean()\n",
    ")\n",
    "slippage_gt5 = grouped.apply(\n",
    "    lambda x: x[x['HedgingTimeUsed'] > 5]['slippage'].mean()\n",
    ")\n",
    "\n",
    "weighted_slippage_lte5 = hedging_ratio_lte5 * slippage_lte5\n",
    "weighted_slippage_gt5 = hedging_ratio_gt5 * slippage_gt5\n",
    "\n",
    "\n",
    "# 新增: 计算极端滑点(小于-0.001)的条数和比例\n",
    "extreme_slippage_count = grouped.apply(\n",
    "    lambda x: (x['slippage'] < -0.001).sum()\n",
    ")\n",
    "\n",
    "extreme_slippage_ratio = grouped.apply(\n",
    "    lambda x: (x['slippage'] < -0.001).sum() / x['slippage'].notna().sum() * 100\n",
    ")\n",
    "\n",
    "\n",
    "# 绘图\n",
    "# 图1: 分位数图\n",
    "fig1 = go.Figure()\n",
    "for col in percentile_df.columns:\n",
    "    fig1.add_trace(go.Scatter(\n",
    "        x=percentile_df.index,\n",
    "        y=percentile_df[col],\n",
    "        mode='lines+markers',\n",
    "        name=col.replace('percentile_', 'P')\n",
    "    ))\n",
    "fig1.update_layout(\n",
    "    title='每日滑点分位数分布',\n",
    "    xaxis_title='日期',\n",
    "    yaxis_title='滑点',\n",
    "    height=600\n",
    ")\n",
    "fig1.show()\n",
    "\n",
    "# 图2: 均值图\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(\n",
    "    x=mean_df.index,\n",
    "    y=mean_df.values,\n",
    "    mode='lines+markers',\n",
    "    name='滑点均值',\n",
    "    line=dict(color='rgb(55, 128, 191)')\n",
    "))\n",
    "fig2.update_layout(\n",
    "    title='每日滑点均值',\n",
    "    xaxis_title='日期',\n",
    "    yaxis_title='滑点均值',\n",
    "    height=500\n",
    ")\n",
    "fig2.show()\n",
    "\n",
    "# 图3: HedgingTimeUsed<=5的比例\n",
    "fig3 = go.Figure()\n",
    "fig3.add_trace(go.Scatter(\n",
    "    x=hedging_ratio.index,\n",
    "    y=hedging_ratio.values,\n",
    "    mode='lines+markers',\n",
    "    name='比例',\n",
    "    line=dict(color='rgb(219, 64, 82)')\n",
    "))\n",
    "fig3.update_layout(\n",
    "    title='每日HedgingTimeUsed≤5的比例',\n",
    "    xaxis_title='日期',\n",
    "    yaxis_title='比例 (%)',\n",
    "    height=500\n",
    ")\n",
    "fig3.show()\n",
    "\n",
    "# 图4: 分组滑点均值对比(线图)\n",
    "fig4 = go.Figure()\n",
    "fig4.add_trace(go.Scatter(\n",
    "    x=slippage_lte5.index,\n",
    "    y=slippage_lte5.values,\n",
    "    mode='lines+markers',\n",
    "    name='HedgingTimeUsed ≤ 5',\n",
    "    line=dict(color='rgb(50, 171, 96)')\n",
    "))\n",
    "fig4.add_trace(go.Scatter(\n",
    "    x=slippage_gt5.index,\n",
    "    y=slippage_gt5.values,\n",
    "    mode='lines+markers',\n",
    "    name='HedgingTimeUsed > 5',\n",
    "    line=dict(color='rgb(255, 127, 14)')\n",
    "))\n",
    "fig4.update_layout(\n",
    "    title='每日分组滑点均值对比',\n",
    "    xaxis_title='日期',\n",
    "    yaxis_title='滑点均值',\n",
    "    height=500\n",
    ")\n",
    "fig4.show()\n",
    "\n",
    "# 新增图6: 比例加权滑点\n",
    "fig6 = go.Figure()\n",
    "fig6.add_trace(go.Scatter(\n",
    "    x=weighted_slippage_lte5.index,\n",
    "    y=weighted_slippage_lte5.values,\n",
    "    mode='lines+markers',\n",
    "    name='HedgingTimeUsed ≤ 5 (比例×滑点)',\n",
    "    line=dict(color='rgb(50, 171, 96)', width=2)\n",
    "))\n",
    "fig6.add_trace(go.Scatter(\n",
    "    x=weighted_slippage_gt5.index,\n",
    "    y=weighted_slippage_gt5.values,\n",
    "    mode='lines+markers',\n",
    "    name='HedgingTimeUsed > 5 (比例×滑点)',\n",
    "    line=dict(color='rgb(255, 127, 14)', width=2)\n",
    "))\n",
    "fig6.update_layout(\n",
    "    title='每日比例加权滑点对比',\n",
    "    xaxis_title='日期',\n",
    "    yaxis_title='比例 × 滑点均值',\n",
    "    height=500,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "fig6.show()\n",
    "\n",
    "\n",
    "# 新增图7: 极端滑点分析 (双Y轴)\n",
    "fig7 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig7.add_trace(\n",
    "    go.Bar(\n",
    "        x=extreme_slippage_count.index,\n",
    "        y=extreme_slippage_count.values,\n",
    "        name='极端滑点条数',\n",
    "        marker_color='rgb(239, 85, 59)',\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "fig7.add_trace(\n",
    "    go.Scatter(\n",
    "        x=extreme_slippage_ratio.index,\n",
    "        y=extreme_slippage_ratio.values,\n",
    "        mode='lines+markers',\n",
    "        name='极端滑点比例',\n",
    "        line=dict(color='rgb(99, 110, 250)', width=3),\n",
    "        marker=dict(size=8)\n",
    "    ),\n",
    "    secondary_y=True\n",
    ")\n",
    "\n",
    "fig7.update_xaxes(title_text=\"日期\")\n",
    "fig7.update_yaxes(title_text=\"条数\", secondary_y=False)\n",
    "fig7.update_yaxes(title_text=\"比例 (%)\", secondary_y=True)\n",
    "\n",
    "fig7.update_layout(\n",
    "    title='每日极端滑点分析 (滑点 < -0.001)',\n",
    "    height=500,\n",
    "    hovermode='x unified',\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "fig7.show()\n",
    "\n",
    "# # 图5: 分组滑点均值对比(柱状图)\n",
    "# fig5 = go.Figure()\n",
    "# fig5.add_trace(go.Bar(\n",
    "#     x=slippage_lte5.index,\n",
    "#     y=slippage_lte5.values,\n",
    "#     name='HedgingTimeUsed ≤ 5',\n",
    "#     marker_color='rgb(50, 171, 96)'\n",
    "# ))\n",
    "# fig5.add_trace(go.Bar(\n",
    "#     x=slippage_gt5.index,\n",
    "#     y=slippage_gt5.values,\n",
    "#     name='HedgingTimeUsed > 5',\n",
    "#     marker_color='rgb(255, 127, 14)'\n",
    "# ))\n",
    "# fig5.update_layout(\n",
    "#     title='每日分组滑点均值对比(柱状图)',\n",
    "#     xaxis_title='日期',\n",
    "#     yaxis_title='滑点均值',\n",
    "#     barmode='group',\n",
    "#     height=500\n",
    "# )\n",
    "# fig5.show()\n",
    "\n",
    "# 可选: 保存图表为HTML文件\n",
    "# fig1.write_html('percentiles.html')\n",
    "# fig2.write_html('mean.html')\n",
    "# fig3.write_html('hedging_ratio.html')\n",
    "# fig4.write_html('comparison_line.html')\n",
    "# fig5.write_html('comparison_bar.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20251217 抢盘口ETH对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('/Users/rayxu/Downloads/order.eth_gate_binance_02_2.csv',starttime='2025-07-10 09:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage('/Users/rayxu/Downloads/order.eth_gate_binance_02_2.csv',starttime='2025-12-17 21:00:00.000') # 实验组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,df = analyze_slippage('/Users/rayxu/Downloads/order.arbitrage_eth_gate_binance_04_2.csv',starttime='2025-12-17 21:00:00.000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20251223 动态对冲时间对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_table, fig, df_a, df_b = analyze_slippage_compare(\n",
    "    \"/Users/rayxu/Downloads/order.btc_gate_binance_01_2.csv\",\n",
    "    \"/Users/rayxu/Downloads/order.btc_gate_binance_03_2.csv\",\n",
    "    starttime=\"2025-12-19 09:59:14\",\n",
    "    endtime=\"2025-12-24 23:59:59\",\n",
    "    label_a=\"benchmark\",\n",
    "    label_b=\"dynamichedging\",\n",
    "    bins=60,\n",
    "    xlim_quantile=(0.01, 0.99),\n",
    ")\n",
    "display(compare_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_paths=[\"/Users/rayxu/Downloads/order.btc_gate_binance_01_2.csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_03_2.csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_02_2.csv\"]\n",
    "compare_labels = [\"BenchMark\",'SwitchHedingTime','queueSniping']\n",
    "compare_table, fig, df1, df2, df = analyze_slippage_compare(\n",
    "    file_paths=compare_paths,\n",
    "    labels=compare_labels,\n",
    "    starttime=\"2025-12-19 09:59:14\",\n",
    "    endtime=\"2025-12-23 05:00:00\",\n",
    "    bins=60,\n",
    "    xlim_quantile=(0.01, 0.99),\n",
    ")\n",
    "display(compare_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同期 \"2025-12-20 09:15:14\"\n",
    "compare_paths=[\"/Users/rayxu/Downloads/order.btc_gate_binance_01_2.csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_03_2.csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_02_2.csv\"]\n",
    "compare_labels = [\"BenchMark\",'SwitchHedingTime','queueSniping']\n",
    "compare_table, fig, df1, df2, df = analyze_slippage_compare(\n",
    "    file_paths=compare_paths,\n",
    "    labels=compare_labels,\n",
    "    starttime=\"2025-12-20 09:15:14\",\n",
    "    endtime=\"2025-12-23 05:00:00\",\n",
    "    bins=60,\n",
    "    xlim_quantile=(0.01, 0.99),\n",
    ")\n",
    "display(compare_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compare_paths=[\"/Users/rayxu/Downloads/order.btc_gate_binance_01_2 (1).csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_03_2 (1).csv\"]\n",
    "compare_labels = [\"BenchMark\",'SwitchHedingTime']\n",
    "compare_table, _, _, _= analyze_slippage_compare(\n",
    "    file_paths=compare_paths,\n",
    "    labels=compare_labels,\n",
    "    starttime=\"2025-12-19 09:59:14\",\n",
    "    endtime=\"2025-12-24 05:00:00\",\n",
    "    bins=60,\n",
    "    xlim_quantile=(0.01, 0.99),\n",
    ")\n",
    "display(compare_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"/Users/rayxu/Downloads/order.btc_gate_binance_03_2 (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最新数据 + 同期 \"2025-12-20 09:15:14\"\n",
    "compare_paths=[\"/Users/rayxu/Downloads/order.btc_gate_binance_01_2 (1).csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_03_2 (1).csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_02_2.csv\"]\n",
    "compare_labels = [\"BenchMark\",'SwitchHedingTime','queueSniping']\n",
    "compare_table, fig, df1, df2, df = analyze_slippage_compare(\n",
    "    file_paths=compare_paths,\n",
    "    labels=compare_labels,\n",
    "    starttime=\"2025-12-20 09:15:14\",\n",
    "    endtime=\"2025-12-24 05:00:00\",\n",
    "    bins=60,\n",
    "    xlim_quantile=(0.01, 0.99),\n",
    ")\n",
    "display(compare_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def analyze_slippage_compare(\n",
    "    file_paths,                 # list[str]\n",
    "    starttime=None,\n",
    "    endtime=None,\n",
    "    labels=None,                # list[str]\n",
    "    bins: int = 50,\n",
    "    xlim_quantile=(0.01, 0.99),\n",
    "    show: bool = True,\n",
    "    # ---- 新增：同期匹配参数 ----\n",
    "    time_col: str = \"CreateTime\",\n",
    "    tolerance: str = \"1min\",\n",
    "    direction: str = \"nearest\",   # \"nearest\"/\"backward\"/\"forward\"\n",
    "):\n",
    "    \"\"\"\n",
    "    输入多个路径(list)，对比多个文件的滑点（仅统计同期样本）：\n",
    "    - 同期定义：按 Side 匹配，CreateTime 最近且时间差<=tolerance（默认1分钟）\n",
    "    - 输出一个对比表格 compare_table\n",
    "    - 输出一个 N*3 图（每行一个文件：Sell / Buy / Overall）\n",
    "\n",
    "    返回:\n",
    "        compare_table (pd.DataFrame)\n",
    "        fig (matplotlib.figure.Figure)\n",
    "        *dfs_matched (pd.DataFrame...): 每个文件在“同期样本集合”上的数据（_t, Side, slippage）\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- 参数检查 ----------\n",
    "    if not isinstance(file_paths, (list, tuple)) or len(file_paths) < 2:\n",
    "        raise ValueError(\"file_paths 必须是 list/tuple 且长度>=2，例如 ['a.csv','b.csv'] 或 ['a','b','c']\")\n",
    "    n = len(file_paths)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"File{i+1}\" for i in range(n)]\n",
    "    if not isinstance(labels, (list, tuple)) or len(labels) != n:\n",
    "        raise ValueError(\"labels 必须为 list/tuple 且长度与 file_paths 一致\")\n",
    "\n",
    "    def _to_ts(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "        return x if isinstance(x, pd.Timestamp) else pd.to_datetime(x)\n",
    "\n",
    "    # ---------- 读+计算slippage，只保留匹配需要的字段 ----------\n",
    "    def _read_and_calc(path: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        required_cols = [\n",
    "            \"Order2FilledPrice\", \"Price\", \"ESR\", \"Side\",\n",
    "            \"Order2Timestamp\", \"Timestamp\",\n",
    "            \"AmountFilled\", \"AveragePrice\", \"OrderID\",\n",
    "            time_col,\n",
    "        ]\n",
    "        missing = [c for c in required_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"[{path}] 缺少必要列: {missing}\")\n",
    "\n",
    "        df = df[df[\"Order2FilledPrice\"] != 0].copy()\n",
    "\n",
    "        # 计算滑点（保持你原来的定义与符号处理）\n",
    "        df[\"SR\"] = df[\"Price\"] / df[\"Order2FilledPrice\"] - 1\n",
    "        df[\"slippage\"] = df[\"SR\"] - df[\"ESR\"]\n",
    "        df[\"sign\"] = df[\"Side\"].apply(lambda x: 1 if x == \"sell\" else -1)\n",
    "        df[\"slippage\"] = df[\"slippage\"] * df[\"sign\"]\n",
    "\n",
    "        # 去重（保持原逻辑）\n",
    "        df = df.drop_duplicates(subset=[\"OrderID\"]).copy()\n",
    "\n",
    "        # 时间列：用 CreateTime 做同期匹配\n",
    "        df[\"_t\"] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
    "        df = df[df[\"_t\"].notna()].copy()\n",
    "\n",
    "        # 额外过滤区间（可选）\n",
    "        st = _to_ts(starttime)\n",
    "        et = _to_ts(endtime)\n",
    "        if st is not None:\n",
    "            df = df.loc[df[\"_t\"] > st]\n",
    "        if et is not None:\n",
    "            df = df.loc[df[\"_t\"] < et]\n",
    "\n",
    "        df = df.loc[df[\"slippage\"].notna(), [\"_t\", \"Side\", \"slippage\"]].copy()\n",
    "        df = df.sort_values(\"_t\")\n",
    "        return df\n",
    "\n",
    "    dfs_raw = [_read_and_calc(p) for p in file_paths]\n",
    "\n",
    "    # ---------- 同期匹配：基准=第一个文件，依次 merge_asof（by=Side, tolerance=1min）----------\n",
    "    tol = pd.Timedelta(tolerance)\n",
    "\n",
    "    # 给每个文件加唯一id，防止复用\n",
    "    dfs_raw = [df.copy() for df in dfs_raw]\n",
    "    for k in range(n):\n",
    "        dfs_raw[k][f\"_id_{k}\"] = np.arange(len(dfs_raw[k]))  # 或者用 OrderID 更稳：dfs_raw[k][f\"_id_{k}\"]=dfs_raw[k][\"OrderID\"]\n",
    "\n",
    "    panel = dfs_raw[0].rename(columns={\"slippage\": labels[0]}).copy()\n",
    "    panel[\"_base_id\"] = np.arange(len(panel))  # 基准行id\n",
    "\n",
    "    for i in range(1, n):\n",
    "        right = dfs_raw[i].rename(columns={\"slippage\": labels[i]}).copy()\n",
    "        # 右表时间列也带过来，便于算 dt\n",
    "        right = right.rename(columns={\"_t\": f\"_t_{i}\"})\n",
    "\n",
    "        panel = pd.merge_asof(\n",
    "            panel.sort_values(\"_t\"),\n",
    "            right.sort_values(f\"_t_{i}\"),\n",
    "            left_on=\"_t\",\n",
    "            right_on=f\"_t_{i}\",\n",
    "            by=\"Side\",\n",
    "            tolerance=tol,\n",
    "            direction=direction,\n",
    "        )\n",
    "\n",
    "        # 只对这次 merge 成功的行算 dt（没匹配到的先留着，最后统一 dropna）\n",
    "        panel[f\"dt_{i}\"] = (panel[\"_t\"] - panel[f\"_t_{i}\"]).abs()\n",
    "\n",
    "        # 关键：禁止右表记录被复用 —— 对右表 id 去重，保留 dt 最小的那条匹配\n",
    "        # （先按 dt 从小到大排序，再 drop_duplicates）\n",
    "        panel = panel.sort_values(f\"dt_{i}\").drop_duplicates(subset=[f\"_id_{i}\"], keep=\"first\")\n",
    "\n",
    "        # 同时保证基准行也不重复（一般不会，但保险）\n",
    "        panel = panel.drop_duplicates(subset=[\"_base_id\"], keep=\"first\")\n",
    "\n",
    "        # 恢复按时间排序，方便下一轮 merge_asof\n",
    "        panel = panel.sort_values(\"_t\")\n",
    "\n",
    "    # 最终只保留所有文件都匹配成功的同期样本\n",
    "    panel = panel.dropna(subset=list(labels)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 把 panel 还原成每个文件一个 df（方便复用你原来的getter逻辑）\n",
    "    dfs = []\n",
    "    for lab in labels:\n",
    "        dfi = panel[[\"_t\", \"Side\", lab]].rename(columns={lab: \"slippage\"}).copy()\n",
    "        dfs.append(dfi)\n",
    "\n",
    "    # ---------- 统计函数（不变） ----------\n",
    "    def _summarize(series: pd.Series, percentiles=(1, 5, 10, 25, 50, 75, 90, 95, 99)) -> dict:\n",
    "        arr = series.dropna().to_numpy()\n",
    "        if len(arr) == 0:\n",
    "            out = {\"count\": 0, \"mean\": np.nan, \"std\": np.nan, \"median\": np.nan, \"min\": np.nan, \"max\": np.nan}\n",
    "            for p in percentiles:\n",
    "                out[f\"p{p}\"] = np.nan\n",
    "            return out\n",
    "\n",
    "        out = {\n",
    "            \"count\": int(len(arr)),\n",
    "            \"mean\": float(np.mean(arr)),\n",
    "            \"std\": float(np.std(arr)),\n",
    "            \"median\": float(np.median(arr)),\n",
    "            \"min\": float(np.min(arr)),\n",
    "            \"max\": float(np.max(arr)),\n",
    "        }\n",
    "        for p in percentiles:\n",
    "            out[f\"p{p}\"] = float(np.percentile(arr, p))\n",
    "        return out\n",
    "\n",
    "    # ---------- 对比表格（结构基本不变：side/metric + 各label + diff相对第一个） ----------\n",
    "    sides = {\n",
    "        \"sell\": lambda d: d[d[\"Side\"] == \"sell\"][\"slippage\"],\n",
    "        \"buy\":  lambda d: d[d[\"Side\"] == \"buy\"][\"slippage\"],\n",
    "        \"overall\": lambda d: d[\"slippage\"],\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for side_name, getter in sides.items():\n",
    "        stats_list = [_summarize(getter(df)) for df in dfs]\n",
    "        keys = stats_list[0].keys()\n",
    "        for k in keys:\n",
    "            rows.append((side_name, k, *[st[k] for st in stats_list]))\n",
    "\n",
    "    summary_df = pd.DataFrame(rows, columns=[\"side\", \"metric\", *labels]).set_index([\"side\", \"metric\"])\n",
    "\n",
    "\n",
    "    compare_table = summary_df\n",
    "\n",
    "    # ---------- 作图：N*3（每行一个文件：Sell/Buy/Overall），统一 xlim ----------\n",
    "    def _q_range_many(series_list, ql, qh):\n",
    "        x = pd.concat([s.dropna() for s in series_list], ignore_index=True)\n",
    "        if len(x) == 0:\n",
    "            return None, None\n",
    "        lo = x.quantile(ql) if ql is not None else x.min()\n",
    "        hi = x.quantile(qh) if qh is not None else x.max()\n",
    "        if np.isfinite(lo) and np.isfinite(hi) and lo < hi:\n",
    "            return float(lo), float(hi)\n",
    "        return None, None\n",
    "\n",
    "    ql, qh = xlim_quantile if xlim_quantile is not None else (None, None)\n",
    "    x_lo, x_hi = _q_range_many([df[\"slippage\"] for df in dfs], ql, qh)\n",
    "\n",
    "    fig, axes = plt.subplots(n, 3, figsize=(22, 5 * n), sharex=True)\n",
    "    if n == 1:\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    col_defs = [(\"sell\", \"Sell\"), (\"buy\", \"Buy\"), (\"overall\", \"Overall\")]\n",
    "\n",
    "    for r, (dff, lab) in enumerate(zip(dfs, labels)):\n",
    "        for c, (side_key, side_title) in enumerate(col_defs):\n",
    "            ax = axes[r, c]\n",
    "            s = sides[side_key](dff).dropna()\n",
    "\n",
    "            ax.hist(s, bins=bins, alpha=0.7)\n",
    "            mu = s.mean() if len(s) else np.nan\n",
    "            sd = s.std() if len(s) else np.nan\n",
    "            if np.isfinite(mu):\n",
    "                ax.axvline(mu, linestyle=\"--\", label=f\"mean={mu:.6g}\")\n",
    "\n",
    "            ax.set_title(f\"{lab} - {side_title} (n={len(s)}, std={sd:.6g})\")\n",
    "            ax.set_xlabel(\"slippage\")\n",
    "            ax.set_ylabel(\"freq\")\n",
    "\n",
    "            # 避免 “No artists with labels...” warning\n",
    "            handles, leg_labels = ax.get_legend_handles_labels()\n",
    "            if len(leg_labels) > 0:\n",
    "                ax.legend()\n",
    "\n",
    "            if x_lo is not None and x_hi is not None:\n",
    "                ax.set_xlim(x_lo, x_hi)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return (compare_table, fig, *dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"/Users/rayxu/Downloads/order.btc_gate_binance_01_2 (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_paths=[\"/Users/rayxu/Downloads/order.btc_gate_binance_01_2 (1).csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_03_2 (1).csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_02_2.csv\"]\n",
    "compare_labels = [\"BenchMark\",'SwitchHedingTime','queueSniping']\n",
    "\n",
    "compare_table, fig, df_old, df_new, df = analyze_slippage_compare(\n",
    "    file_paths=compare_paths,\n",
    "    labels=compare_labels,\n",
    "    time_col=\"Createtime\",\n",
    "    tolerance=\"1min\",\n",
    "    direction=\"nearest\",\n",
    "    starttime=\"2025-12-20 09:15:14\",\n",
    "    endtime=\"2025-12-24 05:00:00\",\n",
    ")\n",
    "display(compare_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_paths=[\"/Users/rayxu/Downloads/order.btc_gate_binance_01_2 (1).csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_03_2 (1).csv\"]\n",
    "compare_labels = [\"BenchMark\",'SwitchHedingTime']\n",
    "\n",
    "compare_table, fig, df_old, df_new = analyze_slippage_compare(\n",
    "    file_paths=compare_paths,\n",
    "    labels=compare_labels,\n",
    "    time_col=\"Createtime\",\n",
    "    tolerance=\"2min\",\n",
    "    direction=\"nearest\",\n",
    "    starttime=\"2025-12-20 09:15:14\",\n",
    "    endtime=\"2025-12-24 05:00:00\",\n",
    ")\n",
    "display(compare_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage(\"/Users/rayxu/Downloads/order.btc_gate_binance_02_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_slippage(\"/Users/rayxu/Downloads/order.btc_gate_binance_04_2.csv\",starttime=\"2025-12-26 07:45:14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"/Users/rayxu/Downloads/order.btc_gate_binance_04_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.Order2FilledPrice!=0]\n",
    "df = df.drop_duplicates(subset=['OrderID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/rayxu/Downloads/order.btc_gate_binance_04_2.csv\")\n",
    "df = df[df.Order2FilledPrice!=0]\n",
    "# 计算滑点\n",
    "df['SR'] = df['Price']/df['Order2FilledPrice']-1\n",
    "df['slippage'] = df['SR']-df['ESR']\n",
    "df['sign'] = df['Side'].apply(lambda x: 1 if x == 'sell' else -1)\n",
    "df['slippage'] = df['slippage']*df['sign']\n",
    "df['TimeUsed'] = (pd.to_datetime(df['Order2Timestamp']) - pd.to_datetime(df['Timestamp'])).dt.total_seconds()\n",
    "df['HedgingTimeUsed'] = (pd.to_datetime(df['Order2Timestamp']) - pd.to_datetime(df['Timestamp'])).dt.total_seconds()    \n",
    "df['Amount'] = df['AmountFilled']*df['AveragePrice']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['OrderID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['slippage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compare_paths=[\"/Users/rayxu/Downloads/order.btc_gate_binance_02_2 (2).csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_03_2 (3).csv\"]\n",
    "compare_labels = [\"QueueSniping\",'BenchMark']\n",
    "compare_table, fig, df1, df2= analyze_slippage_compare(\n",
    "    file_paths=compare_paths,\n",
    "    labels=compare_labels,\n",
    "    starttime=\"2025-12-24 07:13:14\",\n",
    "    endtime=\"2025-12-27 05:00:00\",\n",
    "    bins=60,\n",
    "    xlim_quantile=(0.01, 0.99),\n",
    ")\n",
    "display(compare_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compare_paths=[\"/Users/rayxu/Downloads/order.btc_gate_binance_02_2 (2).csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_03_2 (3).csv\" ,\"/Users/rayxu/Downloads/order.btc_gate_binance_04_2.csv\"]\n",
    "compare_labels = [\"QueueSniping\",'BenchMark','trendInterval']\n",
    "compare_table, fig, df1, df2, df3= analyze_slippage_compare(\n",
    "    file_paths=compare_paths,\n",
    "    labels=compare_labels,\n",
    "    starttime=\"2025-12-26 06:52:14\",\n",
    "    endtime=\"2025-12-27 05:00:00\",\n",
    "    bins=60,\n",
    "    xlim_quantile=(0.01, 0.99),\n",
    ")\n",
    "display(compare_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compare_paths=[\"/Users/rayxu/Downloads/order.btc_gate_binance_02_2 (2).csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_03_2 (3).csv\" ,\"/Users/rayxu/Downloads/order.btc_gate_binance_04_2.csv\"]\n",
    "# compare_labels = [\"QueueSniping\",'BenchMark','trendInterval']\n",
    "# compare_table, fig, df1, df2, df3= analyze_slippage_compare(\n",
    "#     file_paths=compare_paths,\n",
    "#     labels=compare_labels,\n",
    "#     starttime=\"2025-12-26 04:59:14\",\n",
    "#     endtime=\"2025-12-27 05:00:00\",\n",
    "#     bins=60,\n",
    "#     xlim_quantile=(0.01, 0.99),\n",
    "# )\n",
    "# display(compare_table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "compare_paths=[\"/Users/rayxu/Downloads/order.btc_gate_binance_02_2 (2).csv\", \"/Users/rayxu/Downloads/order.btc_gate_binance_03_2 (3).csv\" ,\"/Users/rayxu/Downloads/order.btc_gate_binance_04_2.csv\"]\n",
    "compare_labels = [\"QueueSniping\",'BenchMark','trendInterval']\n",
    "\n",
    "compare_table, fig, df_old, df_new, df = analyze_slippage_compare(\n",
    "    file_paths=compare_paths,\n",
    "    labels=compare_labels,\n",
    "    time_col=\"Createtime\",\n",
    "    tolerance=\"1min\",\n",
    "    direction=\"nearest\",\n",
    "    starttime=\"2025-12-26 04:59:14\",\n",
    "    endtime=\"2025-12-27 05:00:00\",\n",
    ")\n",
    "display(compare_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.to_datetime(df2['Order2Timestamp'])-pd.to_datetime(df2['Order2CreateTime'])).dt.total_seconds().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.to_datetime(df1['Order2Timestamp'])-pd.to_datetime(df1['Order2CreateTime'])).dt.total_seconds().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.to_datetime(df1['Order2Timestamp'])-pd.to_datetime(df1['Order2CreateTime'])).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['HedgingTimeUsed_sec'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['HedgingTimeUsed_sec'] = (pd.to_datetime(df2['Order2Timestamp'])-pd.to_datetime(df2['Order2CreateTime'])).dt.total_seconds()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.HedgingTimeUsed_sec >= 5]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.to_datetime(df2['Order2Timestamp'])-pd.to_datetime(df2['Order2CreateTime']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20251230 抢盘口对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dcdl_orders(env=\"am_csv_s518\",ver=\"20251227-am-UpdateAccountName-fixSaveEnabled\",suffix=[\"btc_okx_binance_01_2\", \"btc_okx_binance_08_2\",'btc_okx_binance_09_2'])\n",
    "download_dcdl_orders(env=\"ps_csv_k480\",ver=\"20251009-arbitrage-checkOverallLameLog\",suffix=[\"arbitrage_btc_okx_binance_02_2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 无修饰抢盘口 vs PS基准\n",
    "\n",
    "file_paths = ['/Volumes/T7/Obentech/AMappData/am_csv_s518/20251227-am-UpdateAccountName-fixSaveEnabled/order.btc_okx_binance_01_2.csv','/Volumes/T7/Obentech/AMappData/ps_csv_k480/20251009-arbitrage-checkOverallLameLog/order.arbitrage_btc_okx_binance_02_2.csv']\n",
    "labels=[\"Snip1\",\"BM\"] \n",
    "\n",
    "# 分段\n",
    "\n",
    "compare_table, fig, df1, df2 = analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime=\"2025-12-29 17:27:00\",\n",
    "    endtime=\"2025-12-29 17:40:00\",\n",
    ")\n",
    "\n",
    "compare_table, fig, df1, df2 = analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime=\"2025-12-29 20:14:00\",\n",
    "    endtime=\"2025-12-29 21:14:00\",\n",
    ")\n",
    "\n",
    "compare_table, fig, df1, df2 = analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime=\"2025-12-30 12:10:00\",\n",
    "    endtime=\"2025-12-30 17:23:00\",\n",
    ")\n",
    "\n",
    "compare_table, fig, df1, df2 = analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime=\"2025-12-30 22:30:00\",\n",
    "    endtime=\"2025-12-31 00:00:00\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整体\n",
    "\n",
    "compare_table, fig, df1, df2 = analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime=\"2025-12-29 17:27:00\",\n",
    "    endtime=\"2025-12-31 17:40:00\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BN-OK 抢盘口优化版 vs 抢盘口朴素版 vs 抢盘口+动态对冲 vs 基准 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dcdl_orders(env=\"am_csv_s518\",ver=\"20251227-am-UpdateAccountName-fixSaveEnabled\",suffix=[\"btc_okx_binance_01_2\", \"btc_okx_binance_08_2\",'btc_okx_binance_09_2'])\n",
    "download_dcdl_orders(env=\"ps_csv_k480\",ver=\"20251009-arbitrage-checkOverallLameLog\",suffix=[\"arbitrage_btc_okx_binance_02_2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 无修饰抢盘口 vs PS基准\n",
    "\n",
    "file_paths = ['/Volumes/T7/Obentech/AMappData/am_csv_s518/20251227-am-UpdateAccountName-fixSaveEnabled/order.btc_okx_binance_01_2.csv','/Volumes/T7/Obentech/AMappData/am_csv_s518/20251227-am-UpdateAccountName-fixSaveEnabled/order.btc_okx_binance_08_2.csv','/Volumes/T7/Obentech/AMappData/am_csv_s518/20251227-am-UpdateAccountName-fixSaveEnabled/order.btc_okx_binance_09_2.csv','/Volumes/T7/Obentech/AMappData/ps_csv_k480/20251009-arbitrage-checkOverallLameLog/order.arbitrage_btc_okx_binance_02_2.csv']\n",
    "labels=[\"Snip1\",\"Snip2\",\"Snip2+DynamicHedgingTime\",\"BM\"] \n",
    "\n",
    "# 分段\n",
    "\n",
    "compare_table, fig, df1, df2, df3, df4 = analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime=\"2025-12-30 22:30:00\",\n",
    "    endtime=\"2025-12-31 17:40:00\",\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二轮\n",
    "download_dcdl_orders(env=\"am_csv_s518\",ver=\"20251231-am-fixCancel\",suffix=[\"btc_okx_binance_08_2\",'btc_okx_binance_09_2'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['/Volumes/T7/Obentech/AMappData/am_csv_s518/20251231-am-fixCancel/order.btc_okx_binance_08_2.csv','/Volumes/T7/Obentech/AMappData/am_csv_s518/20251231-am-fixCancel/order.btc_okx_binance_09_2.csv']\n",
    "labels=[\"Snip2\",\"Snip2+DynamicHedgingTime\"] \n",
    "\n",
    "\n",
    "\n",
    "compare_table, fig, df1, df2 = analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime=\"2026-01-01 03:46:00\",\n",
    "    endtime=\"2026-01-06 17:40:00\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dcdl_orders(env=\"am_csv_s518\",ver=\"20251231-am-fixCancel\",suffix=[\"eth_okx_binance_08_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dcdl_orders(env=\"ps_csv_k480\",ver=\"20251117-arbitrage-logOnOrderLag\",suffix=[\"arbitrage_eth_okx_binance_09_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dcdl_orders(env=\"ps_csv_k480\",ver=\"20260121-arbitrage-fixNegativeIsOrderBigEnough\",suffix=[\"arbitrage_eth_okx_binance_02_2\"])\n",
    "file_paths = ['/Volumes/T7/Obentech/AMappData/ps_csv_k480/20251117-arbitrage-logOnOrderLag/order.arbitrage_eth_okx_binance_09_2.csv','/Volumes/T7/Obentech/AMappData/ps_csv_k480/20260121-arbitrage-fixNegativeIsOrderBigEnough/order.arbitrage_eth_okx_binance_02_2.csv']\n",
    "labels=[\"09\",\"02\"] \n",
    "\n",
    "\n",
    "compare_table, fig, df1, df2= analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime='2026-02-04 20:00:00',\n",
    "    endtime='2026-02-05 05:00:00',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dcdl_orders(env=\"ps_csv_k480\",ver=\"20251102-arbitrage-handleOrdrNotExist-fixCheckOverallLameDueToPrecision\",suffix=[\"arbitrage_eth_okx_binance_01_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Volumes/T7/Obentech/AMappData/am_csv_s518/20251231-am-fixCancel/order.eth_okx_binance_08_2.csv')\n",
    "df = df[df[\"Order2FilledPrice\"] != 0].copy()\n",
    "df[\"SR\"] = df[\"Price\"] / df[\"Order2FilledPrice\"] - 1\n",
    "df[\"slippage\"] = df[\"SR\"] - df[\"ESR\"]\n",
    "df[\"sign\"] = df[\"Side\"].apply(lambda x: 1 if x == \"sell\" else -1)\n",
    "df[\"slippage\"] = df[\"slippage\"] * df[\"sign\"]\n",
    "\n",
    "# 1) 把 inf/-inf 变成 NaN\n",
    "df[\"slippage\"] = df[\"slippage\"].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 2) 只根据这一列 drop NaN 的行\n",
    "df = df.dropna(subset = ['slippage'])\n",
    "df[(df.Createtime > \"2026-02-02 06:21:00\")&(df.Createtime < \"2026-02-03 06:21:00\")]['slippage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2026-02-04 20:00:00'\n",
    "end_date = '2026-02-05 05:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['/Volumes/T7/Obentech/AMappData/am_csv_s518/20251231-am-fixCancel/order.eth_okx_binance_08_2.csv','/Volumes/T7/Obentech/AMappData/ps_csv_k480/20251117-arbitrage-logOnOrderLag/order.arbitrage_eth_okx_binance_09_2.csv']\n",
    "labels=[\"Snip1\",\"BM\"] \n",
    "\n",
    "\n",
    "compare_table, fig, df1, df2= analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime='2026-02-04 05:00:00',\n",
    "    endtime='2026-02-05 05:00:00',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1.sign==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1[df1.sign==1]['ESR'].mean(),df1[df1.sign==1]['SR'].mean())\n",
    "print(df1[df1.sign==-1]['ESR'].mean(),df1[df1.sign==-1]['SR'].mean())\n",
    "print(df2[df2.sign==1]['ESR'].mean(),df2[df2.sign==1]['SR'].mean())\n",
    "print(df2[df2.sign==-1]['ESR'].mean(),df2[df2.sign==-1]['SR'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 无修饰抢盘口+调整了下单量 vs PS基准\n",
    "\n",
    "file_paths = ['/Volumes/T7/Obentech/AMappData/am_csv_s518/20251231-am-fixCancel/order.eth_okx_binance_08_2.csv','/Volumes/T7/Obentech/AMappData/ps_csv_k480/20251117-arbitrage-logOnOrderLag/order.arbitrage_eth_okx_binance_09_2.csv']\n",
    "labels=[\"Snip1\",\"BM\"] \n",
    "\n",
    "# 分段\n",
    "\n",
    "compare_table, fig, df1, df2= analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime=\"2026-01-05 09:44:00\",\n",
    "    endtime=\"2026-01-06 13:30:00\",\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BN-Gate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dcdl_orders(env=\"am_csv_k480\",ver=\"20251226-am-trendInterval\",suffix=[\"btc_gate_binance_02_2\",\"btc_gate_binance_03_2\",\"btc_gate_binance_04_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dcdl_orders(env=\"am_csv_k480\",ver=\"20251217-am-randomSize\",suffix=[\"btc_gate_binance_05_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['/Volumes/T7/Obentech/AMappData/am_csv_k480/20251226-am-trendInterval/order.btc_gate_binance_02_2.csv','/Volumes/T7/Obentech/AMappData/am_csv_k480/20251226-am-trendInterval/order.btc_gate_binance_03_2.csv','/Volumes/T7/Obentech/AMappData/am_csv_k480/20251226-am-trendInterval/order.btc_gate_binance_04_2.csv','/Volumes/T7/Obentech/AMappData/am_csv_k480/20251217-am-randomSize/order.btc_gate_binance_05_2.csv']\n",
    "labels=[\"QueueSnipingV1\", \"BM+BugFix\", \"QueueSnipingV2\",\"BM+Bug\"] \n",
    "\n",
    "compare_table, fig, df1, df2, df3, df4 = analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime=\"2025-12-29 17:27:00\",\n",
    "    endtime=\"2025-12-31 08:00:00\",\n",
    ")\n",
    "display(compare_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抢盘口应用小币"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dcdl_orders(env=\"ps_csv_k480\",ver=\"20251017-arbitrage-RiskMode-fixArbitrageLame-fixOverallLame\",suffix=[\"arbitrage_aster_okx_binance_02_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dcdl_orders(env=\"am_csv_s518\",ver=\"20251231-am-fixCancel\",suffix=[\"aster_okx_binance_09_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2026-01-13 15:00:00'\n",
    "# start_date = '2025-12-11 15:30:00'\n",
    "end_date = '2026-01-15 15:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['/Volumes/T7/Obentech/AMappData/ps_csv_k480/20251017-arbitrage-RiskMode-fixArbitrageLame-fixOverallLame/order.arbitrage_aster_okx_binance_02_2.csv','/Volumes/T7/Obentech/AMappData/am_csv_s518/20251231-am-fixCancel/order.aster_okx_binance_09_2.csv']\n",
    "labels=[\"BM\",\"QueueSnipingV2\"] \n",
    "\n",
    "compare_table, fig, df1, df2 = analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime='2026-01-07 15:00:00',\n",
    "    endtime='2026-01-15 15:00:00',\n",
    ")\n",
    "display(compare_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['/Volumes/T7/Obentech/AMappData/ps_csv_k480/20251017-arbitrage-RiskMode-fixArbitrageLame-fixOverallLame/order.arbitrage_aster_okx_binance_02_2.csv','/Volumes/T7/Obentech/AMappData/am_csv_s518/20251231-am-fixCancel/order.aster_okx_binance_09_2.csv']\n",
    "labels=[\"BM\",\"QueueSnipingV2\"] \n",
    "\n",
    "compare_table, fig, df1, df2 = analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime=\"2026-01-01 06:57:00\",\n",
    "    endtime=\"2026-01-07 08:00:00\",\n",
    ")\n",
    "display(compare_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_paths = ['/Volumes/T7/Obentech/AMappData/ps_csv_k480/20251017-arbitrage-RiskMode-fixArbitrageLame-fixOverallLame/order.arbitrage_aster_okx_binance_02_2.csv','/Volumes/T7/Obentech/AMappData/am_csv_s518/20251231-am-fixCancel/order.aster_okx_binance_09_2.csv']\n",
    "labels=[\"BM\",\"QueueSnipingV2\"] \n",
    "\n",
    "compare_table, fig, df1, df2 = analyze_slippage_compare(\n",
    "    file_paths=file_paths,\n",
    "    labels=labels,\n",
    "    starttime='2026-01-07 06:20:00',\n",
    "    endtime=\"2026-01-08 12:00:00\",\n",
    ")\n",
    "display(compare_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby('sign')['SR'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['sign']==-1]['ESR'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['sign']==-1]['SR'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['sign']==1]['ESR'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['sign']==1]['SR'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0114 进一步敞口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0210 对比正负万1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dcdl_orders(env=\"am_csv_k480\",ver=\"20260124-instantOrder-maxDelay\",suffix=[\"eth_binance_okx_03_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,df = analyze_slippage('/Volumes/T7/Obentech/AMappData/am_csv_k480/20260124-instantOrder-maxDelay/order.eth_binance_okx_03_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['ESR']<0.00005)&(df['ESR']>-0.00005)]['slippage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['ESR']<0.00005)&(df['ESR']>-0.00005)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
