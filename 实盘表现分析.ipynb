{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.CONFIG import *\n",
    "from common_utils.utils_Sep import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 怎么用，把图画出来？\n",
    "# 首先导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "# 定义函数\n",
    "# def load_symbol_pnl_dict(pkl_path):\n",
    "#     \"\"\"\n",
    "#     加载保存的symbol_pnl_dict\n",
    "#     \"\"\"\n",
    "#     with open(pkl_path, 'rb') as f:\n",
    "#         return pickle.load(f)\n",
    "# def filter_series(s, start_dt, end_dt, include_prev=True):\n",
    "#     if s is None or len(s) == 0:\n",
    "#         return s\n",
    "#     # 保证索引是 DatetimeIndex，且排序\n",
    "#     idx = pd.to_datetime(s.index)\n",
    "#     s2 = s.copy()\n",
    "#     s2.index = idx\n",
    "#     s2 = s2.sort_index()\n",
    "\n",
    "#     # 时区对齐（若一边有tz一边没有，这里做个简单处理）\n",
    "#     if getattr(start_dt, 'tzinfo', None) and s2.index.tz is None:\n",
    "#         start_dt = start_dt.tz_convert(None) if hasattr(start_dt, 'tz_convert') else start_dt.tz_localize(None)\n",
    "#         end_dt = end_dt.tz_convert(None) if hasattr(end_dt, 'tz_convert') else end_dt.tz_localize(None)\n",
    "#     if s2.index.tz and getattr(start_dt, 'tzinfo', None) is None:\n",
    "#         start_dt = start_dt.tz_localize(s2.index.tz)\n",
    "#         end_dt = end_dt.tz_localize(s2.index.tz)\n",
    "\n",
    "#     mask = (s2.index >= start_dt) & (s2.index <= end_dt)\n",
    "\n",
    "#     if include_prev:\n",
    "#         i = s2.index.searchsorted(start_dt, side='left') - 1  # 严格小于 start_dt 的最后一条\n",
    "#         if i >= 0:\n",
    "#             # 把这一条也保留\n",
    "#             mask[i] = True\n",
    "\n",
    "#     return s2.loc[mask]\n",
    "\n",
    "# def normalize_series(s):\n",
    "#     if s is None or s.empty:\n",
    "#         return s\n",
    "#     s_norm = s - s.iloc[0]\n",
    "#     return s_norm.iloc[1:]\n",
    "# def plot_symbol_and_portfolio_in_period(\n",
    "#     symbol_pnl_dict, \n",
    "#     symbol, \n",
    "#     start_date, \n",
    "#     end_date, \n",
    "#     initial_capital=100000, \n",
    "#     show=True, \n",
    "#     type='pmpro',  # 新增type参数，默认pmpro\n",
    "#     show_portfolio=False\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     用法示例：\n",
    "#         plot_symbol_and_portfolio_in_period(symbol_pnl_dict, 'SOON', '2025-07-08 04:05', '2025-07-09 04:05', type='pmpro')\n",
    "#     参数说明：\n",
    "#         symbol_pnl_dict: 你的收益数据字典，通常用 load_symbol_pnl_dict 加载\n",
    "#         symbol: 你要画的币种，比如 'SOON'\n",
    "#         start_date, end_date: 字符串，格式如 '2024-06-01 12:34'，精确到分钟\n",
    "#         initial_capital: 初始资金，默认 100000（仅对portfolio有效，单symbol分母见type说明）\n",
    "#         show: 是否直接在notebook里显示图，默认True\n",
    "#         type: 'pmpro' 或 'dcpro'，影响分母和portfolio初始资金\n",
    "#     返回：\n",
    "#         (symbol_fig, portfolio_fig) 两个plotly图对象\n",
    "#     \"\"\"\n",
    "#     import pandas as pd\n",
    "#     import plotly.graph_objects as go\n",
    "#     from pandas import to_datetime\n",
    "#     from IPython.display import display\n",
    "\n",
    "#     # 解析时间，强制精确到分钟\n",
    "#     def parse_to_minute(dt_str):\n",
    "#         dt = to_datetime(dt_str)\n",
    "#         return dt.replace(second=0, microsecond=0)\n",
    "#     start_dt = parse_to_minute(start_date)\n",
    "#     end_dt = parse_to_minute(end_date)\n",
    "\n",
    "\n",
    "#     # 根据type设置分母\n",
    "#     if type == 'pmpro':\n",
    "#         symbol_denominator = 4000\n",
    "#         portfolio_initial_capital = 100000\n",
    "#     elif type == 'dcpro1':\n",
    "#         symbol_denominator = 40000\n",
    "#         portfolio_initial_capital = 1000000\n",
    "#     elif type == 'dcpro2':\n",
    "#         symbol_denominator = 164000\n",
    "#         portfolio_initial_capital = 4100000\n",
    "#     elif type == 'dcpro3':\n",
    "#         symbol_denominator = 200000\n",
    "#         portfolio_initial_capital = 5000000\n",
    "#     elif type == 'dcpro4':\n",
    "#         symbol_denominator = 164000\n",
    "#         portfolio_initial_capital = 4100000\n",
    "#     elif type == 'dcpro5':\n",
    "#         symbol_denominator = 25000\n",
    "#         portfolio_initial_capital = 1250000\n",
    "#     elif type == 'dcpro6':\n",
    "#         symbol_denominator = 80000\n",
    "#         portfolio_initial_capital = 2000000\n",
    "#     elif type == 'dcpro7':\n",
    "#         symbol_denominator = 120000\n",
    "#         portfolio_initial_capital = 3000000\n",
    "#     elif type == 'dcpro8':\n",
    "#         symbol_denominator = 80000\n",
    "#         portfolio_initial_capital = 2000000\n",
    "#     elif type == 'dcpro9':\n",
    "#         symbol_denominator = 11600\n",
    "#         portfolio_initial_capital = 290000\n",
    "#     elif type == 'dcpro10':\n",
    "#         symbol_denominator = 120000\n",
    "#         portfolio_initial_capital = 3000000\n",
    "#     elif type == 'dcpro11':\n",
    "#         symbol_denominator = 40000\n",
    "#         portfolio_initial_capital = 1000000\n",
    "#     elif type == 'dcpro12':\n",
    "#         portfolio_initial_capital = 2750000\n",
    "#         symbol_denominator = portfolio_initial_capital*0.04\n",
    "\n",
    "#     elif type == 'dcpro13':\n",
    "#         portfolio_initial_capital = 4000000\n",
    "#         symbol_denominator = portfolio_initial_capital*0.04\n",
    "        \n",
    "#     elif type == 'dcpro14':\n",
    "#         portfolio_initial_capital = 5000000\n",
    "#         symbol_denominator = portfolio_initial_capital*0.04\n",
    "        \n",
    "#     elif type == 'dcpro15':\n",
    "#         portfolio_initial_capital = 2500000\n",
    "#         symbol_denominator = portfolio_initial_capital*0.04\n",
    "        \n",
    "#     elif type == 'dcpro16':\n",
    "#         portfolio_initial_capital = 3200000\n",
    "#         symbol_denominator = portfolio_initial_capital*0.04\n",
    "        \n",
    "#     elif type == 'dcpro17':\n",
    "#         portfolio_initial_capital = 1200000\n",
    "#         symbol_denominator = portfolio_initial_capital*0.04\n",
    "        \n",
    "\n",
    "        \n",
    "#     elif type == 'pmtest2':\n",
    "#         symbol_denominator = 3000\n",
    "#         portfolio_initial_capital = 75000\n",
    "    \n",
    "#     elif type == 'pmtest4': \n",
    "#         symbol_denominator = 200\n",
    "#         portfolio_initial_capital = 5000\n",
    "        \n",
    "        \n",
    "#     else:\n",
    "#         symbol_denominator = initial_capital  # fallback\n",
    "#         portfolio_initial_capital = initial_capital\n",
    "\n",
    "#     # 处理symbol\n",
    "#     symbol_data = symbol_pnl_dict.get(symbol, None)\n",
    "#     if symbol_data is None:\n",
    "#         print(f\"Symbol {symbol} not found in symbol_pnl_dict\")\n",
    "#         symbol_fig = None\n",
    "#     else:\n",
    "#         cum_pnl_combined = filter_series(symbol_data.get('cum_pnl_combined', pd.Series(dtype=float)),start_dt,end_dt)\n",
    "#         funding_pnl_series = filter_series(symbol_data.get('funding_pnl_series', pd.Series(dtype=float)),start_dt,end_dt)\n",
    "#         trade_pnl = filter_series(symbol_data.get('trade_pnl', pd.Series(dtype=float)),start_dt,end_dt)\n",
    "\n",
    "#         # 新增：归一化\n",
    "#         cum_pnl_combined_norm = normalize_series(cum_pnl_combined)\n",
    "#         funding_pnl_series_norm = normalize_series(funding_pnl_series)\n",
    "#         trade_pnl_norm = normalize_series(trade_pnl)\n",
    "\n",
    "#         if not cum_pnl_combined_norm.empty:\n",
    "#             total_ret = cum_pnl_combined_norm.iloc[-1]\n",
    "#             total_days = (cum_pnl_combined_norm.index[-1] - cum_pnl_combined_norm.index[0]).total_seconds() / 86400\n",
    "#             total_ret_rate = 365 * total_ret / symbol_denominator / total_days if total_days > 0 else 0\n",
    "#             print(total_ret, total_days, total_ret_rate)\n",
    "#         else:\n",
    "#             total_ret_rate = 0\n",
    "\n",
    "#         if not trade_pnl_norm.empty:\n",
    "#             spread_ret = trade_pnl_norm.iloc[-1]\n",
    "#             spread_ret_rate = 365 * spread_ret / symbol_denominator / total_days if total_days > 0 else 0\n",
    "#         else:\n",
    "#             spread_ret_rate = 0\n",
    "\n",
    "#         if not funding_pnl_series_norm.empty:\n",
    "#             funding_ret = funding_pnl_series_norm.iloc[-1] \n",
    "#             funding_ret_rate = 365 * funding_ret / symbol_denominator / total_days if total_days > 0 else 0\n",
    "#         else:\n",
    "#             funding_ret_rate = 0\n",
    "\n",
    "#         turnover_rate = symbol_data.get('turnover_rate', 0)\n",
    "\n",
    "#         fig = go.Figure()\n",
    "#         if not cum_pnl_combined_norm.empty:\n",
    "#             fig.add_trace(go.Scatter(\n",
    "#                 x=cum_pnl_combined_norm.index, y=cum_pnl_combined_norm.values,\n",
    "#                 mode='lines+markers',\n",
    "#                 name='Cumulative PnL',\n",
    "#                 line=dict(color='royalblue'),\n",
    "#                 fill='tozeroy',\n",
    "#                 fillcolor='rgba(65,105,225,0.1)'\n",
    "#             ))\n",
    "#         if not funding_pnl_series_norm.empty:\n",
    "#             fig.add_trace(go.Scatter(\n",
    "#                 x=funding_pnl_series_norm.index, y=funding_pnl_series_norm.values,\n",
    "#                 mode='lines+markers',\n",
    "#                 name='Funding PnL',\n",
    "#                 line=dict(dash='dash', color='indianred')\n",
    "#             ))\n",
    "#         if not trade_pnl_norm.empty:\n",
    "#             fig.add_trace(go.Scatter(\n",
    "#                 x=trade_pnl_norm.index, y=trade_pnl_norm.values,\n",
    "#                 mode='lines+markers',\n",
    "#                 name='Trade PnL',\n",
    "#                 line=dict(color='deeppink')\n",
    "#             ))\n",
    "#         if not cum_pnl_combined_norm.empty:\n",
    "#             start_str = cum_pnl_combined_norm.index[0].strftime('%Y-%m-%d %H:%M')\n",
    "#             end_str = cum_pnl_combined_norm.index[-1].strftime('%Y-%m-%d %H:%M')\n",
    "#         else:\n",
    "#             start_str = parse_to_minute(start_date).strftime('%Y-%m-%d %H:%M')\n",
    "#             end_str = parse_to_minute(end_date).strftime('%Y-%m-%d %H:%M')\n",
    "#         fig.update_layout(\n",
    "#             title=f\"{type} {symbol} PnL from {start_str} to {end_str}, Annualized Return: {total_ret_rate:.2%} | Spread: {spread_ret_rate:.2%} | Funding: {funding_ret_rate:.2%} | Turnover: {turnover_rate:.2f}\",\n",
    "#             xaxis_title='Time (Asia/Shanghai)',\n",
    "#             yaxis_title='PnL (normalized to 0 at start)',\n",
    "#             legend=dict(font=dict(size=12)),\n",
    "#             template='plotly_white',\n",
    "#             height=700,\n",
    "#             width=1400\n",
    "#         )\n",
    "#         symbol_fig = fig\n",
    "#         if show:\n",
    "#             display(fig)\n",
    "\n",
    "#     # 处理portfolio\n",
    "#     portfolio_data = symbol_pnl_dict.get('portfolio', None)\n",
    "#     if portfolio_data is None:\n",
    "#         print(\"Portfolio data not found in symbol_pnl_dict\")\n",
    "#         portfolio_fig = None\n",
    "#     else:\n",
    "#         portfolio_cum_pnl = filter_series(portfolio_data.get('cum_pnl_combined', pd.Series(dtype=float)),start_dt,end_dt)\n",
    "#         portfolio_trade_pnl = filter_series(portfolio_data.get('trade_pnl', pd.Series(dtype=float)),start_dt,end_dt)\n",
    "#         portfolio_funding_pnl = filter_series(portfolio_data.get('funding_pnl_series', pd.Series(dtype=float)),start_dt,end_dt)\n",
    "#         portfolio_turnover_rate = portfolio_data.get('turnover_rate', 0)\n",
    "\n",
    "#         # 新增：归一化\n",
    "#         portfolio_cum_pnl_norm = normalize_series(portfolio_cum_pnl)\n",
    "#         portfolio_trade_pnl_norm = normalize_series(portfolio_trade_pnl)\n",
    "#         portfolio_funding_pnl_norm = normalize_series(portfolio_funding_pnl)\n",
    "\n",
    "#         # portfolio的初始资金由type决定\n",
    "#         portfolio_capital = portfolio_initial_capital\n",
    "\n",
    "#         if not portfolio_cum_pnl.empty:\n",
    "#             total_ret = portfolio_cum_pnl.iloc[-1] - portfolio_cum_pnl.iloc[0]\n",
    "#             total_days = (portfolio_cum_pnl.index[-1] - portfolio_cum_pnl.index[0]).total_seconds() / 86400\n",
    "#             total_ret_rate = 365 * total_ret / portfolio_capital / total_days if total_days > 0 else 0\n",
    "#         else:\n",
    "#             total_ret_rate = 0\n",
    "\n",
    "#         if not portfolio_trade_pnl.empty:\n",
    "#             spread_ret = portfolio_trade_pnl.iloc[-1] - portfolio_trade_pnl.iloc[0]\n",
    "#             spread_ret_rate = 365 * spread_ret / portfolio_capital / total_days if total_days > 0 else 0\n",
    "#         else:\n",
    "#             spread_ret_rate = 0\n",
    "\n",
    "#         if not portfolio_funding_pnl.empty:\n",
    "#             funding_ret = portfolio_funding_pnl.iloc[-1] - portfolio_funding_pnl.iloc[0]\n",
    "#             funding_ret_rate = 365 * funding_ret / portfolio_capital / total_days if total_days > 0 else 0\n",
    "#         else:\n",
    "#             funding_ret_rate = 0\n",
    "\n",
    "#         fig = go.Figure()\n",
    "#         if not portfolio_cum_pnl_norm.empty:\n",
    "#             fig.add_trace(go.Scatter(\n",
    "#                 x=portfolio_cum_pnl_norm.index, y=portfolio_cum_pnl_norm.values,\n",
    "#                 mode='lines',\n",
    "#                 name='Portfolio Total PnL',\n",
    "#                 line=dict(color='royalblue'),\n",
    "#                 fill='tozeroy',\n",
    "#                 fillcolor='rgba(65,105,225,0.1)'\n",
    "#             ))\n",
    "#         if not portfolio_trade_pnl_norm.empty:\n",
    "#             fig.add_trace(go.Scatter(\n",
    "#                 x=portfolio_trade_pnl_norm.index, y=portfolio_trade_pnl_norm.values,\n",
    "#                 mode='lines',\n",
    "#                 name='Portfolio Trade PnL',\n",
    "#                 line=dict(color='deeppink')\n",
    "#             ))\n",
    "#         if not portfolio_funding_pnl_norm.empty:\n",
    "#             fig.add_trace(go.Scatter(\n",
    "#                 x=portfolio_funding_pnl_norm.index, y=portfolio_funding_pnl_norm.values,\n",
    "#                 mode='lines',\n",
    "#                 name='Portfolio Funding PnL',\n",
    "#                 line=dict(dash='dash', color='indianred')\n",
    "#             ))\n",
    "#         if not portfolio_cum_pnl.empty:\n",
    "#             start_str = portfolio_cum_pnl.index[0].strftime('%Y-%m-%d %H:%M')\n",
    "#             end_str = portfolio_cum_pnl.index[-1].strftime('%Y-%m-%d %H:%M')\n",
    "#         else:\n",
    "#             start_str = parse_to_minute(start_date).strftime('%Y-%m-%d %H:%M')\n",
    "#             end_str = parse_to_minute(end_date).strftime('%Y-%m-%d %H:%M')\n",
    "#         fig.update_layout(\n",
    "#             title=(\n",
    "#                 f\"{type} PnL from {start_str} to {end_str}  \\n\"\n",
    "#                 f\"Annualized Return: {total_ret_rate:.2%} | Spread: {spread_ret_rate:.2%} | Funding: {funding_ret_rate:.2%} | Turnover: {portfolio_turnover_rate:.2f}\"\n",
    "#             ),\n",
    "#             xaxis_title='Time',\n",
    "#             yaxis_title='PnL (normalized to 0 at start)',\n",
    "#             template='plotly_white',\n",
    "#             legend=dict(font=dict(size=12)),\n",
    "#             height=700,\n",
    "#             width=1400\n",
    "#         )\n",
    "#         portfolio_fig = fig\n",
    "#         if show_portfolio:\n",
    "#             display(fig)\n",
    "\n",
    "#     return symbol_fig, portfolio_fig\n",
    "\n",
    "# # 用法举例（在notebook里直接运行下面这行即可画图）：\n",
    "# # plot_symbol_and_portfolio_in_period(symbol_pnl_dict, 'SOON', '2025-07-08 04:05', '2025-07-09 04:05', type='pmpro')\n",
    "\n",
    "\n",
    "# # ========== 新增功能：区间内收益率最高和最低的n个symbol分别单独画出来 ==========\n",
    "\n",
    "# def plot_top_bottom_symbols_in_period(\n",
    "#     symbol_pnl_dict, \n",
    "#     start_date, \n",
    "#     end_date, \n",
    "#     n=5, \n",
    "#     initial_capital=100000, \n",
    "#     show=True, \n",
    "#     type='pmpro'\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     画出给定区间内收益率最高和最低的n个symbol的表现（每个symbol单独画一张归一化收益曲线图）\n",
    "#     参数说明：\n",
    "#         symbol_pnl_dict: 你的收益数据字典\n",
    "#         start_date, end_date: 字符串，格式如 '2024-06-01 12:34'\n",
    "#         n: top/bottom的数量\n",
    "#         initial_capital, show, type: 同plot_symbol_and_portfolio_in_period\n",
    "#     返回：\n",
    "#         (top_figs, bottom_figs) 两个list，分别为top n和bottom n symbol的plotly图对象\n",
    "#     \"\"\"\n",
    "#     import pandas as pd\n",
    "#     import plotly.graph_objects as go\n",
    "#     from pandas import to_datetime\n",
    "#     from IPython.display import display\n",
    "\n",
    "#     def parse_to_minute(dt_str):\n",
    "#         dt = to_datetime(dt_str)\n",
    "#         return dt.replace(second=0, microsecond=0)\n",
    "#     start_dt = parse_to_minute(start_date)\n",
    "#     end_dt = parse_to_minute(end_date)\n",
    "\n",
    "\n",
    "#     # 根据type设置分母\n",
    "#     if type == 'pmpro':\n",
    "#         symbol_denominator = 4000\n",
    "#     elif type == 'dcpro':\n",
    "#         symbol_denominator = 40000\n",
    "#     elif type == 'dcpro5':\n",
    "#         symbol_denominator = 21000\n",
    "#     elif type == 'dcpro3':\n",
    "#         symbol_denominator = 200000\n",
    "#     elif type == 'dcpro6':\n",
    "#         symbol_denominator = 80000\n",
    "#     elif type == 'pmtest4':\n",
    "#         symbol_denominator = 200\n",
    "#     else:\n",
    "#         symbol_denominator = initial_capital  # fallback\n",
    "\n",
    "#     # 计算所有symbol的收益率\n",
    "#     symbol_returns = []\n",
    "#     for symbol, data in symbol_pnl_dict.items():\n",
    "#         if symbol == 'portfolio':\n",
    "#             continue\n",
    "#         cum_pnl = filter_series(data.get('cum_pnl_combined', pd.Series(dtype=float)),start_dt,end_dt)\n",
    "#         if cum_pnl is None or cum_pnl.empty:\n",
    "#             continue\n",
    "#         # 只考虑区间内有数据的symbol\n",
    "#         if len(cum_pnl) < 2:\n",
    "#             continue\n",
    "#         ret = (cum_pnl.iloc[-1] - cum_pnl.iloc[0]) / symbol_denominator\n",
    "#         symbol_returns.append((symbol, ret, cum_pnl))\n",
    "\n",
    "#     if not symbol_returns:\n",
    "#         print(\"No symbol has valid PnL data in the given period.\")\n",
    "#         return None, None\n",
    "\n",
    "#     # 排序\n",
    "#     symbol_returns_sorted = sorted(symbol_returns, key=lambda x: x[1], reverse=True)\n",
    "#     top_symbols = symbol_returns_sorted[:n]\n",
    "#     bottom_symbols = symbol_returns_sorted[-n:]\n",
    "\n",
    "#     # 画top n，每个symbol单独画\n",
    "#     top_figs = []\n",
    "#     for symbol, ret, cum_pnl in top_symbols:\n",
    "#         norm_curve = normalize_series(cum_pnl)\n",
    "#         fig = go.Figure()\n",
    "#         fig.add_trace(go.Scatter(\n",
    "#             x=norm_curve.index, y=norm_curve.values,\n",
    "#             mode='lines',\n",
    "#             name=f\"{symbol} ({ret:.2%})\"\n",
    "#         ))\n",
    "#         fig.update_layout(\n",
    "#             title=f\"Top Symbol: {symbol} ({ret:.2%})\\nfrom {start_dt} to {end_dt}\",\n",
    "#             xaxis_title='Time',\n",
    "#             yaxis_title='PnL (normalized to 0 at start)',\n",
    "#             template='plotly_white',\n",
    "#             legend=dict(font=dict(size=12)),\n",
    "#             height=700,\n",
    "#             width=1400\n",
    "#         )\n",
    "#         if show:\n",
    "#             display(fig)\n",
    "#         top_figs.append(fig)\n",
    "\n",
    "#     # 画bottom n，每个symbol单独画\n",
    "#     bottom_figs = []\n",
    "#     for symbol, ret, cum_pnl in bottom_symbols:\n",
    "#         norm_curve = normalize_series(cum_pnl)\n",
    "#         fig = go.Figure()\n",
    "#         fig.add_trace(go.Scatter(\n",
    "#             x=norm_curve.index, y=norm_curve.values,\n",
    "#             mode='lines',\n",
    "#             name=f\"{symbol} ({ret:.2%})\"\n",
    "#         ))\n",
    "#         fig.update_layout(\n",
    "#             title=f\"Bottom Symbol: {symbol} ({ret:.2%})\\nfrom {start_dt} to {end_dt}\",\n",
    "#             xaxis_title='Time',\n",
    "#             yaxis_title='PnL (normalized to 0 at start)',\n",
    "#             template='plotly_white',\n",
    "#             legend=dict(font=dict(size=12)),\n",
    "#             height=700,\n",
    "#             width=1400\n",
    "#         )\n",
    "#         if show:\n",
    "#             display(fig)\n",
    "#         bottom_figs.append(fig)\n",
    "\n",
    "\n",
    "# 用法举例（在notebook里直接运行下面这行即可画图）：\n",
    "# plot_top_bottom_symbols_in_period(symbol_pnl_dict, '2025-07-08 04:05', '2025-07-09 04:05', n=5, type='pmpro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = get_base_dir()\n",
    "pkl_path0 = f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_10WU.pkl'\n",
    "symbol_pnl_dict0 = load_symbol_pnl_dict(pkl_path0)\n",
    "pkl_path1 = f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_100WU.pkl'\n",
    "symbol_pnl_dict1 = load_symbol_pnl_dict(pkl_path1)\n",
    "pkl_path2 = f'{base_dir}//Obentech/pnl_analysis/symbol_pnl_dict_dcpro2.pkl'\n",
    "symbol_pnl_dict2 = load_symbol_pnl_dict(pkl_path2)\n",
    "pkl_path3 = f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro3.pkl'\n",
    "symbol_pnl_dict3 = load_symbol_pnl_dict(pkl_path3)\n",
    "pkl_path4 = f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro4.pkl'\n",
    "symbol_pnl_dict4 = load_symbol_pnl_dict(pkl_path4)\n",
    "pkl_path5 = f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro5.pkl'\n",
    "symbol_pnl_dict5 = load_symbol_pnl_dict(pkl_path5)\n",
    "pkl_path6 = f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro6.pkl'\n",
    "symbol_pnl_dict6 = load_symbol_pnl_dict(pkl_path6)\n",
    "pkl_path7 = f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro7.pkl'\n",
    "symbol_pnl_dict7 = load_symbol_pnl_dict(pkl_path7)\n",
    "pkl_path8 = f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro8.pkl'\n",
    "symbol_pnl_dict8 = load_symbol_pnl_dict(pkl_path8)\n",
    "pkl_path9 = f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro9.pkl'\n",
    "symbol_pnl_dict9 = load_symbol_pnl_dict(pkl_path9)\n",
    "pkl_path10 =f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro10.pkl'\n",
    "symbol_pnl_dict10 = load_symbol_pnl_dict(pkl_path10)\n",
    "pkl_path11 =f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro11.pkl'\n",
    "symbol_pnl_dict11 = load_symbol_pnl_dict(pkl_path11)\n",
    "pkl_path12 =f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro12.pkl'\n",
    "symbol_pnl_dict12 = load_symbol_pnl_dict(pkl_path12)\n",
    "pkl_path13 =f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro13.pkl'\n",
    "symbol_pnl_dict13 = load_symbol_pnl_dict(pkl_path13)\n",
    "pkl_path14 =f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro14.pkl'\n",
    "symbol_pnl_dict14 = load_symbol_pnl_dict(pkl_path14)\n",
    "pkl_path15 =f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro15.pkl'\n",
    "symbol_pnl_dict15 = load_symbol_pnl_dict(pkl_path15)\n",
    "pkl_path16 =f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro16.pkl'\n",
    "symbol_pnl_dict16 = load_symbol_pnl_dict(pkl_path16)\n",
    "pkl_path17 =f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_dcpro17.pkl'\n",
    "symbol_pnl_dict17 = load_symbol_pnl_dict(pkl_path17)\n",
    "\n",
    "\n",
    "\n",
    "pkl_path_pmtest2 = f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_pmtest2.pkl'\n",
    "symbol_pnl_dict_pmtest2 = load_symbol_pnl_dict(pkl_path_pmtest2)\n",
    "\n",
    "pkl_path_pmtest4 = f'{base_dir}/Obentech/pnl_analysis/symbol_pnl_dict_pmtest4.pkl'\n",
    "symbol_pnl_dict_pmtest4 = load_symbol_pnl_dict(pkl_path_pmtest4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_series = 1+symbol_pnl_dict1['portfolio']['cum_pnl_combined']/1000000\n",
    "# return_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 把return_series的index转化为天级别的\n",
    "# return_series.index = pd.to_datetime(return_series.index)\n",
    "# return_series = return_series.resample('D').last().dropna()\n",
    "# return_series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Intraday NAV -> metrics, 并返回月度收益率\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from math import sqrt\n",
    "# from pathlib import Path\n",
    "\n",
    "# def compute_metrics(nav_ts: pd.Series):\n",
    "#     s = nav_ts.sort_index().astype(float)\n",
    "#     s = s[~s.index.duplicated(keep=\"last\")]\n",
    "#     # per-day normalized returns from irregular seconds\n",
    "#     log_nav = np.log(s)\n",
    "#     delta_sec = s.index.to_series().diff().dt.total_seconds()\n",
    "#     delta_days = delta_sec / 86400.0\n",
    "#     log_ret_step = log_nav.diff()\n",
    "#     per_day_log_ret = log_ret_step / delta_days\n",
    "#     per_day_ret = np.expm1(per_day_log_ret)\n",
    "#     daily_like_ret = per_day_ret.dropna()\n",
    "\n",
    "#     DAYS_PER_YEAR = 365.0\n",
    "#     ret_mean = daily_like_ret.mean()\n",
    "#     ret_std = daily_like_ret.std(ddof=1) if len(daily_like_ret) > 1 else daily_like_ret.std(ddof=0)\n",
    "#     ann_vol = ret_std * np.sqrt(DAYS_PER_YEAR) if ret_std > 0 else np.nan\n",
    "#     sharpe = (ret_mean / ret_std) * np.sqrt(DAYS_PER_YEAR) if ret_std > 0 else np.nan\n",
    "\n",
    "#     downside = np.minimum(daily_like_ret, 0.0)\n",
    "#     downside_dev = np.sqrt((downside**2).mean())\n",
    "#     sortino = (ret_mean / downside_dev) * np.sqrt(DAYS_PER_YEAR) if downside_dev > 0 else np.nan\n",
    "\n",
    "#     roll_max = s.cummax()\n",
    "#     drawdown = s / roll_max - 1.0\n",
    "#     max_dd = drawdown.min()\n",
    "#     dd_end = drawdown.idxmin()\n",
    "#     dd_start = s.loc[:dd_end].idxmax() if pd.notnull(dd_end) else pd.NaT\n",
    "#     dd_recovery_date = s.loc[dd_end:][s.loc[dd_end:] >= s.loc[dd_start]].index.min() if pd.notnull(dd_start) else pd.NaT\n",
    "\n",
    "#     start_date, end_date = s.index[0], s.index[-1]\n",
    "#     total_days = (end_date - start_date).total_seconds() / 86400.0\n",
    "#     total_return = s.iloc[-1] / s.iloc[0] - 1.0\n",
    "#     cagr = (s.iloc[-1] / s.iloc[0]) ** (DAYS_PER_YEAR / total_days) - 1.0 if total_days > 0 else np.nan\n",
    "#     calmar = (cagr / abs(max_dd)) if (max_dd < 0) else np.nan\n",
    "\n",
    "#     summary = pd.DataFrame({\n",
    "#         \"Metric\": [\n",
    "#             \"Start\",\n",
    "#             \"End\",\n",
    "#             \"Observations\",\n",
    "#             \"Total Return\",\n",
    "#             \"CAGR (calendar)\",\n",
    "#             \"Ann. Vol (calendar)\",\n",
    "#             \"Sharpe (rf=0)\",\n",
    "#             \"Sortino (MAR=0)\",\n",
    "#             \"Max Drawdown\",\n",
    "#             \"MDD Start\",\n",
    "#             \"MDD Trough\",\n",
    "#             \"MDD Recovery (if any)\",\n",
    "#             \"Calmar (CAGR/|MDD|)\",\n",
    "#         ],\n",
    "#         \"Value\": [\n",
    "#             start_date,\n",
    "#             end_date,\n",
    "#             len(s),\n",
    "#             f\"{total_return:.6%}\",\n",
    "#             f\"{cagr:.6%}\" if pd.notnull(cagr) else \"NA\",\n",
    "#             f\"{ann_vol:.6%}\" if pd.notnull(ann_vol) else \"NA\",\n",
    "#             f\"{sharpe:.4f}\" if pd.notnull(sharpe) else \"NA\",\n",
    "#             f\"{sortino:.4f}\" if pd.notnull(sortino) else \"NA\",\n",
    "#             f\"{max_dd:.6%}\" if pd.notnull(max_dd) else \"NA\",\n",
    "#             dd_start if pd.notnull(dd_start) else \"NA\",\n",
    "#             dd_end if pd.notnull(dd_end) else \"NA\",\n",
    "#             (dd_recovery_date if pd.notnull(dd_recovery_date) else \"Not recovered\"),\n",
    "#             f\"{calmar:.4f}\" if pd.notnull(calmar) else \"NA\",\n",
    "#         ]\n",
    "#     })\n",
    "\n",
    "#     # 计算月度收益率\n",
    "#     # 先将index转为datetime（如果还没转）\n",
    "#     s_month = s.copy()\n",
    "#     if not np.issubdtype(s_month.index.dtype, np.datetime64):\n",
    "#         s_month.index = pd.to_datetime(s_month.index)\n",
    "#     # 取每月最后一个NAV\n",
    "#     monthly_nav = s_month.resample('M').last()\n",
    "#     # 计算月度收益率\n",
    "#     monthly_return = monthly_nav.pct_change().dropna()\n",
    "#     monthly_return.name = \"Monthly Return\"\n",
    "\n",
    "#     return summary, pd.DataFrame({\"NAV\": s, \"drawdown\": drawdown}), daily_like_ret, monthly_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_close = pd.read_csv('/Users/rayxu/Downloads/AERO_USDT.close.csv')\n",
    "# df_close = df_close[df_close['start'].astype(str).str.contains(r'\\d{4}-\\d{2}-\\d{2}')]\n",
    "# df_close['sr_close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary,drawdown,daily_like_ret, monthly_return = compute_metrics(return_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 增加标题 Cross Exchange Nav Curve\n",
    "# # 增加X轴 Date, Y轴 NAV\n",
    "# # 变得好看一点\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(drawdown.index, drawdown['NAV'], color='royalblue', linewidth=2)\n",
    "# plt.title('Cross Exchange NAV Curve', fontsize=18, fontweight='bold')\n",
    "# plt.xlabel('Date', fontsize=14)\n",
    "# plt.ylabel('NAV', fontsize=14)\n",
    "# plt.grid(True, linestyle='--', alpha=0.5)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_pnl_dict_pmtest2['portfolio']['cum_pnl_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s = symbol_pnl_dict_pmtest2['portfolio']['cum_pnl_combined']\n",
    "# 1) 保证索引是 datetime 且按时间排序\n",
    "s = s.copy()\n",
    "s.index = pd.to_datetime(s.index)\n",
    "s = s.sort_index()\n",
    "\n",
    "# 2) 选择你要按哪个时区“切日”\n",
    "#    - 如果 s.index 已经带 tz，就用 tz_convert\n",
    "#    - 如果不带 tz（你这个看起来是不带 tz 的 naive），需要先 tz_localize 再 tz_convert\n",
    "TARGET_TZ = \"UTC\"                 # 改成 \"Asia/Shanghai\" 或 \"America/Los_Angeles\" 等\n",
    "SRC_TZ = \"UTC\"                    # 如果你的 naive 时间其实是北京时间，就改成 \"Asia/Shanghai\"\n",
    "\n",
    "if s.index.tz is None:\n",
    "    s = s.tz_localize(SRC_TZ)\n",
    "s = s.tz_convert(TARGET_TZ)\n",
    "\n",
    "# 3) 日频取每一天“最后一个值”（你想要 daily close 的 cum_pnl 就用这个）\n",
    "daily_cum = s.resample(\"1D\").last()\n",
    "\n",
    "# 可选：如果你想补齐缺失天，用前值填充（比如中间某天没数据）\n",
    "# daily_cum = daily_cum.ffill()\n",
    "\n",
    "# 4) 输出 CSV（包含 date, cum_pnl）\n",
    "out_path = \"cum_pnl_daily.csv\"\n",
    "daily_cum.rename(\"cum_pnl_combined\").to_csv(out_path, header=True)\n",
    "\n",
    "print(\"saved:\", out_path, \"| rows:\", len(daily_cum))\n",
    "daily_cum.to_csv(\"/Volumes/T7/Obentech/nav/daily_pnl_bn_bybit.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s = symbol_pnl_dict1['portfolio']['cum_pnl_combined']  # pd.Series, DatetimeIndex\n",
    "\n",
    "# 1) 保证索引是 datetime 且按时间排序\n",
    "s = s.copy()\n",
    "s.index = pd.to_datetime(s.index)\n",
    "s = s.sort_index()\n",
    "\n",
    "# 2) 选择你要按哪个时区“切日”\n",
    "#    - 如果 s.index 已经带 tz，就用 tz_convert\n",
    "#    - 如果不带 tz（你这个看起来是不带 tz 的 naive），需要先 tz_localize 再 tz_convert\n",
    "TARGET_TZ = \"UTC\"                 # 改成 \"Asia/Shanghai\" 或 \"America/Los_Angeles\" 等\n",
    "SRC_TZ = \"UTC\"                    # 如果你的 naive 时间其实是北京时间，就改成 \"Asia/Shanghai\"\n",
    "\n",
    "if s.index.tz is None:\n",
    "    s = s.tz_localize(SRC_TZ)\n",
    "s = s.tz_convert(TARGET_TZ)\n",
    "\n",
    "# 3) 日频取每一天“最后一个值”（你想要 daily close 的 cum_pnl 就用这个）\n",
    "daily_cum = s.resample(\"1D\").last()\n",
    "\n",
    "# 可选：如果你想补齐缺失天，用前值填充（比如中间某天没数据）\n",
    "# daily_cum = daily_cum.ffill()\n",
    "\n",
    "# 4) 输出 CSV（包含 date, cum_pnl）\n",
    "out_path = \"cum_pnl_daily.csv\"\n",
    "daily_cum.rename(\"cum_pnl_combined\").to_csv(out_path, header=True)\n",
    "\n",
    "print(\"saved:\", out_path, \"| rows:\", len(daily_cum))\n",
    "print(daily_cum.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_cum.to_csv(\"/Volumes/T7/Obentech/nav/daily_pnl.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pnl = daily_cum.diff().rename(\"daily_pnl\")\n",
    "daily_pnl.to_csv(\"daily_pnl.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_pnl_dict1['portfolio']['cum_pnl_combined'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取前一天的仓位数据\n",
    "start_date = '2025-12-22 04:00:00'\n",
    "end_date = '2025-12-23 09:30:00'\n",
    "dcdl_date = '2025122304'\n",
    "df_pos_limit = pd.read_csv(f'/Users/rayxu/Downloads/{dcdl_date}_cfdc_dcpro1_limit_pos.csv')\n",
    "show_symbol_list = df_pos_limit[df_pos_limit['PositionLimit']>0].sort_values(by='PositionLimit', ascending=False)['Symbol'].tolist()\n",
    "show_symbol_list_without_USDT = [x.split('-')[0] for x in show_symbol_list]+['POL','H','ALLO','CELO']\n",
    "print(show_symbol_list_without_USDT)\n",
    "df_pos_limit[df_pos_limit['PositionLimit']>200].sort_values(by='PositionLimit', ascending=False)\n",
    "for ticker in show_symbol_list_without_USDT:\n",
    "    plot_symbol_and_portfolio_in_period(symbol_pnl_dict1, ticker, start_date, end_date, type='dcpro1', show_portfolio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2025-01-01 04:00:00'\n",
    "end_date = '2026-01-31 04:00:00'\n",
    "ticker = 'ETH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict_pmtest2,ticker, start_date,end_date,type='pmtest2',show_portfolio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict8,ticker, start_date,end_date,type='dcpro1',show_portfolio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((4689/2000000)/27)*365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict8,ticker, start_date,end_date,type='dcpro8')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict8,ticker, start_date,end_date,type='dcpro9')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict8,ticker, start_date,end_date,type='dcpro1',initial_capital=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_symbol_and_portfolio_in_period(symbol_pnl_dict0,ticker, start_date,end_date,type='pmpro')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict1,ticker, start_date,end_date,type='dcpro1')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict2,ticker, start_date,end_date,type='dcpro2')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict3,ticker, start_date,end_date,type='dcpro3')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict4,ticker, start_date,end_date,type='dcpro4')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict5,ticker, start_date,end_date,type='dcpro5')\n",
    "# _ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict6,ticker, start_date,end_date,type='dcpro6')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict7,ticker, start_date,end_date,type='dcpro7')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict8,ticker, start_date,end_date,type='dcpro8')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict9,ticker, start_date,end_date,type='dcpro9')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict10,ticker, start_date,end_date,type='dcpro10')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict11,ticker, start_date,end_date,type='dcpro11')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict12,ticker, start_date,end_date,type='dcpro12')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict13,ticker, start_date,end_date,type='dcpro13')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict14,ticker, start_date,end_date,type='dcpro14')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict15,ticker, start_date,end_date,type='dcpro15')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict16,ticker, start_date,end_date,type='dcpro16')\n",
    "_ = plot_symbol_and_portfolio_in_period(symbol_pnl_dict17,ticker, start_date,end_date,type='dcpro17')\n",
    "# plot_symbol_and_portfolio_in_period(symbol_pnl_dict_pmtest2,ticker, start_date,end_date,type='pmtest2',figure_type='matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in ['ETH']:\n",
    "    plot_symbol_and_portfolio_in_period(symbol_pnl_dict1, ticker, start_date, end_date, type='dcpro', show_portfolio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2025-12-01 00:00:00'\n",
    "end_date = '2025-12-04 09:30:00'\n",
    "ticker = 'GIGGLE'\n",
    "# plot_symbol_and_portfolio_in_period(symbol_pnl_dict_pmtest4,ticker, start_date,end_date,type='pmtest4', show_portfolio=True)\n",
    "plot_top_bottom_symbols_in_period(symbol_pnl_dict_pmtest4, start_date, end_date, n=5, initial_capital=5000, show=True, type='pmtest4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_bottom_symbols_in_period(\n",
    "    symbol_pnl_dict4, \n",
    "    start_date, \n",
    "    end_date, \n",
    "    n=5, \n",
    "    initial_capital=100000, \n",
    "    show=True, \n",
    "    type='pmpro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_portfolio_pro1 = symbol_pnl_dict0['portfolio']['cum_pnl_combined']\n",
    "df_portfolio_pro1.index = pd.to_datetime(df_portfolio_pro1.index)\n",
    "df_portfolio_pro1_daily = df_portfolio_pro1.resample('D').last()\n",
    "df_portfolio_pro1_daily = df_portfolio_pro1_daily.to_frame()\n",
    "df_portfolio_pro1_daily.columns = ['PnL']\n",
    "df_portfolio_pro1_daily['NaV'] = (df_portfolio_pro1_daily['PnL'] + 100000)/100000\n",
    "df_portfolio_pro1_daily['daily_ret'] = (df_portfolio_pro1_daily['NaV'].pct_change())\n",
    "df_portfolio_pro1_daily['daily_ret_annualized'] = (df_portfolio_pro1_daily['NaV'].pct_change())*365\n",
    "df_portfolio_pro1_daily['daily_ret'].plot()\n",
    "df_portfolio_pro1_daily.to_csv('pmpro每日收益统计.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_symbol_and_portfolio_in_period(symbol_pnl_dict,'SWARMS', start_date='2025-08-10 04:05',end_date='2025-08-13 04:05',type='dcpro')\n",
    "plot_symbol_and_portfolio_in_period(symbol_pnl_dict2,'BERA', start_date='2025-08-16 00:05',end_date='2025-08-17 00:05',type='pmpro')\n",
    "# plot_symbol_and_portfolio_in_period(symbol_pnl_dict5,'SWARMS', start_date='2025-08-10 04:05',end_date='2025-08-13 04:05',type='dcpro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_symbol_and_portfolio_in_period(symbol_pnl_dict2,'ETH', start_date='2025-07-13 04:05',end_date='2025-07-23 04:05',type='pmpro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['signed_ret_mid_10.0s'] < 0]['slippage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_symbol_and_portfolio_in_period(symbol_pnl_dict,'HYPE', start_date='2025-07-01 04:05',end_date='2025-07-18 04:05',type='dcpro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_symbol_and_portfolio_in_period(symbol_pnl_dict2,'SHELL', start_date='2025-07-16 04:05',end_date='2025-07-17 04:05',type='pmpro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_symbol_curves(symbol, symbol_data):\n",
    "    \"\"\"\n",
    "    使用Plotly绘制单个symbol的PnL曲线，适用于notebook环境\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 获取数据\n",
    "        cum_pnl_combined = symbol_data.get('cum_pnl_combined', pd.Series(dtype=float))\n",
    "        funding_pnl_series = symbol_data.get('funding_pnl_series', pd.Series(dtype=float))\n",
    "        trade_pnl = symbol_data.get('trade_pnl', pd.Series(dtype=float))\n",
    "\n",
    "        total_ret_rate = symbol_data.get('Total_Ret_Rate', 0)\n",
    "        spread_ret_rate = symbol_data.get('Spread_Ret_Rate', 0)\n",
    "        funding_ret_rate = symbol_data.get('Funding_Ret_Rate', 0)\n",
    "        turnover_rate = symbol_data.get('turnover_rate', 0)\n",
    "\n",
    "        # 创建Plotly图表\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # 1. 总盈亏（含 funding + real_earn - 手动费用）\n",
    "        if not cum_pnl_combined.empty:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=cum_pnl_combined.index, y=cum_pnl_combined.values,\n",
    "                mode='lines+markers',\n",
    "                name='Cumulative PnL',\n",
    "                line=dict(color='royalblue'),\n",
    "                fill='tozeroy',\n",
    "                fillcolor='rgba(65,105,225,0.1)'\n",
    "            ))\n",
    "        \n",
    "        # 2. funding 累计盈亏\n",
    "        if not funding_pnl_series.empty:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=funding_pnl_series.index, y=funding_pnl_series.values,\n",
    "                mode='lines+markers',\n",
    "                name='Funding PnL',\n",
    "                line=dict(dash='dash', color='indianred')\n",
    "            ))\n",
    "        \n",
    "        # 3. trade PnL（仅 real_earn）\n",
    "        if not trade_pnl.empty:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=trade_pnl.index, y=trade_pnl.values,\n",
    "                mode='lines+markers',\n",
    "                name='Trade PnL',\n",
    "                line=dict(color='deeppink')\n",
    "            ))\n",
    "        \n",
    "        # 获取时间范围\n",
    "        start_time = None\n",
    "        end_time = None\n",
    "        if not cum_pnl_combined.empty:\n",
    "            start_time = cum_pnl_combined.index.min()\n",
    "            end_time = cum_pnl_combined.index.max()\n",
    "        start_str = start_time.strftime('%Y-%m-%d %H:%M')\n",
    "        end_str = end_time.strftime('%Y-%m-%d %H:%M')\n",
    "        # 设置图表布局\n",
    "        fig.update_layout(\n",
    "            title=f\"{symbol} PnL from {start_str} to {end_str}, Return Rate: {total_ret_rate:.1%} | Spread : {spread_ret_rate:.1%} | Funding : {funding_ret_rate:.1%} | Turnover: {turnover_rate:.1f}\",\n",
    "            xaxis_title='Time (Asia/Shanghai)',\n",
    "            yaxis_title='PnL',\n",
    "            legend=dict(font=dict(size=12)),\n",
    "            template='plotly_white',\n",
    "            height=700,\n",
    "            width=1400\n",
    "        )\n",
    "        \n",
    "        # 在notebook中显示图表\n",
    "        return fig\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting {symbol}: {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_portfolio_pnl(symbol_pnl_dict, initial_capital=100000):\n",
    "    \"\"\"\n",
    "    使用Plotly绘制portfolio的PnL曲线，适用于notebook环境\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'portfolio' not in symbol_pnl_dict:\n",
    "            print(\"Portfolio data not found in symbol_pnl_dict\")\n",
    "            return None\n",
    "            \n",
    "        portfolio_data = symbol_pnl_dict['portfolio']\n",
    "        portfolio_cum_pnl = portfolio_data.get('cum_pnl_combined', pd.Series(dtype=float))\n",
    "        portfolio_trade_pnl = portfolio_data.get('trade_pnl', pd.Series(dtype=float))\n",
    "        portfolio_funding_pnl = portfolio_data.get('funding_pnl_series', pd.Series(dtype=float))\n",
    "        portfolio_turnover_rate = portfolio_data.get('turnover_rate', 0)\n",
    "        print(f'portfolio_turnover_rate: {portfolio_turnover_rate}')\n",
    "        # 计算时间和收益率\n",
    "        start_time = portfolio_cum_pnl.index.min() if not portfolio_cum_pnl.empty else None\n",
    "        end_time = portfolio_cum_pnl.index.max() if not portfolio_cum_pnl.empty else None\n",
    "        \n",
    "        if start_time and end_time:\n",
    "            total_days = (end_time - start_time).total_seconds() / 86400\n",
    "            total_ret = portfolio_cum_pnl.iloc[-1] if not portfolio_cum_pnl.empty else 0\n",
    "            spread_ret = portfolio_trade_pnl.iloc[-1] if not portfolio_trade_pnl.empty else 0\n",
    "            funding_ret = portfolio_funding_pnl.iloc[-1] if not portfolio_funding_pnl.empty else 0\n",
    "            \n",
    "            total_ret_rate = 365 * total_ret / initial_capital / total_days\n",
    "            spread_ret_rate = 365 * spread_ret / initial_capital / total_days\n",
    "            funding_ret_rate = 365 * funding_ret / initial_capital / total_days\n",
    "            \n",
    "            start_str = start_time.strftime('%Y-%m-%d %H:%M')\n",
    "            end_str = end_time.strftime('%Y-%m-%d %H:%M')\n",
    "            \n",
    "            # 创建Plotly图表\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # 添加总PnL曲线\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=portfolio_cum_pnl.index, y=portfolio_cum_pnl.values,\n",
    "                mode='lines',\n",
    "                name='Portfolio Total PnL',\n",
    "                line=dict(color='royalblue'),\n",
    "                fill='tozeroy',\n",
    "                fillcolor='rgba(65,105,225,0.1)'\n",
    "            ))\n",
    "            \n",
    "            # 添加Trade PnL曲线\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=portfolio_trade_pnl.index, y=portfolio_trade_pnl.values,\n",
    "                mode='lines',\n",
    "                name='Portfolio Trade PnL',\n",
    "                line=dict(color='deeppink')\n",
    "            ))\n",
    "            \n",
    "            # 添加Funding PnL曲线\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=portfolio_funding_pnl.index, y=portfolio_funding_pnl.values,\n",
    "                mode='lines',\n",
    "                name='Portfolio Funding PnL',\n",
    "                line=dict(dash='dash', color='indianred')\n",
    "            ))\n",
    "            \n",
    "            # 设置图表布局\n",
    "            fig.update_layout(\n",
    "                title=(\n",
    "                    f\"Portfolio PnL from {start_str} to {end_str} ({total_days:.1f} days)  \\n\"\n",
    "                    f\"Annualized Return: {total_ret_rate:.2%} | Spread: {spread_ret_rate:.2%} | Funding: {funding_ret_rate:.2%} | Turnover: {portfolio_turnover_rate:.2f}\"\n",
    "                ),\n",
    "                xaxis_title='Time',\n",
    "                yaxis_title='PnL',\n",
    "                template='plotly_white',\n",
    "                legend=dict(font=dict(size=12)),\n",
    "                height=700,\n",
    "                width=1400\n",
    "            )\n",
    "            \n",
    "            # 打印统计信息\n",
    "            print(f\"Portfolio Total Return: {total_ret:.2f} USDT\")\n",
    "            print(f\"Portfolio Holding Days: {total_days:.2f} days\")\n",
    "            print(f\"Annualized Total Return Rate:   {total_ret_rate:.2%}\")\n",
    "            print(f\"Annualized Spread Return Rate:  {spread_ret_rate:.2%}\")\n",
    "            print(f\"Annualized Funding Return Rate: {funding_ret_rate:.2%}\")\n",
    "            \n",
    "            return fig\n",
    "        else:\n",
    "            print(\"Cannot calculate portfolio statistics: missing time data\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting portfolio PnL: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_top_symbols(symbol_pnl_dict, top_n=10, exclude_portfolio=True):\n",
    "    \"\"\"\n",
    "    获取收益最高的前N个symbol\n",
    "    \"\"\"\n",
    "    symbol_returns = {}\n",
    "    for symbol, data in symbol_pnl_dict.items():\n",
    "        if symbol == 'portfolio' and exclude_portfolio:\n",
    "            continue\n",
    "        if 'cum_pnl_combined' in data and not data['cum_pnl_combined'].empty:\n",
    "            symbol_returns[symbol] = data['cum_pnl_combined'].iloc[-1]\n",
    "    \n",
    "    # 按收益排序并选择前top_n个\n",
    "    sorted_symbols = sorted(symbol_returns.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [symbol for symbol, _ in sorted_symbols[:top_n]]\n",
    "\n",
    "\n",
    "def get_bottom_symbols(symbol_pnl_dict, bottom_n=10, exclude_portfolio=True):\n",
    "    \"\"\"\n",
    "    获取收益最低的前N个symbol\n",
    "    \"\"\"\n",
    "    symbol_returns = {}\n",
    "    for symbol, data in symbol_pnl_dict.items():\n",
    "        if symbol == 'portfolio' and exclude_portfolio:\n",
    "            continue\n",
    "        if 'cum_pnl_combined' in data and not data['cum_pnl_combined'].empty:\n",
    "            symbol_returns[symbol] = data['cum_pnl_combined'].iloc[-1]  \n",
    "    sorted_symbols = sorted(symbol_returns.items(), key=lambda x: x[1], reverse=False)\n",
    "    return [symbol for symbol, _ in sorted_symbols[:bottom_n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "# pkl_path = '/Users/rayxu/Downloads/symbol_pnl_dict (17).pkl'  # 修改为你的文件路径\n",
    "# pkl_path = '/Users/rayxu/Downloads/symbol_pnl_dict_100WU (14).pkl'\n",
    "# pkl_path = '/Users/rayxu/Downloads/symbol_pnl_dict_100WU_20250708_0405_20250709_0405.pkl'\n",
    "# pkl_path = '/Users/rayxu/Downloads/symbol_pnl_dict_100WU (29).pkl'\n",
    "# pkl_path = '/Users/rayxu/Downloads/symbol_pnl_dict_dcob1 (3).pkl'\n",
    "# pkl_path = '/Users/rayxu/Downloads/symbol_pnl_dict_dcpro5.pkl'\n",
    "pkl_path = '/Users/rayxu/Downloads/symbol_pnl_dict_500WU.pkl'\n",
    "symbol_pnl_dict = load_symbol_pnl_dict(pkl_path)\n",
    "print(f\"Loaded data for {len(symbol_pnl_dict)} symbols\")\n",
    "\n",
    "# 绘制portfolio图表\n",
    "# fig_portfolio = plot_portfolio_pnl(symbol_pnl_dict,initial_capital=1000000)\n",
    "# fig_portfolio.show()  # 在notebook中显示\n",
    "\n",
    "#绘制特定symbol的图表\n",
    "symbol = 'NMR'  # 修改为你想查看的symbol\n",
    "if symbol in symbol_pnl_dict:\n",
    "    fig_symbol = plot_symbol_curves(symbol, symbol_pnl_dict[symbol])\n",
    "    fig_symbol.show()  # 在notebook中显示\n",
    "else:\n",
    "    print(f\"Symbol {symbol} not found in data\")\n",
    "\n",
    "#绘制收益最高的前5个symbol\n",
    "# top_symbols = get_top_symbols(symbol_pnl_dict, top_n=5)\n",
    "# print(f\"Top 5 symbols by return: {top_symbols}\")\n",
    "\n",
    "# for symbol in top_symbols:\n",
    "#     fig = plot_symbol_curves(symbol, symbol_pnl_dict[symbol])\n",
    "#     fig.show()  # 在notebook中显示\n",
    "\n",
    "# #绘制收益最低的前5个symbol\n",
    "# bottom_symbols = get_bottom_symbols(symbol_pnl_dict, bottom_n=5)\n",
    "# print(f\"Bottom 5 symbols by return: {bottom_symbols}\")\n",
    "\n",
    "# for symbol in bottom_symbols:\n",
    "#     fig = plot_symbol_curves(symbol, symbol_pnl_dict[symbol])\n",
    "#     fig.show()  # 在notebook中显示\n",
    "\n",
    "\n",
    "# 绘制全部symbol图标\n",
    "# for symbol,_ in symbol_pnl_dict.items():\n",
    "#     fig = plot_symbol_curves(symbol, symbol_pnl_dict[symbol])\n",
    "#     fig.show()  # 在notebook中显示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "pkl_path = '/Users/rayxu/Downloads/symbol_pnl_dict (17).pkl'  # 修改为你的文件路径\n",
    "pkl_path = '/Users/rayxu/Downloads/symbol_pnl_dict_10WU (16).pkl'\n",
    "# pkl_path = '/Users/rayxu/Downloads/symbol_pnl_dict_10WU_20250708_0405_20250709_0405.pkl'\n",
    "symbol_pnl_dict = load_symbol_pnl_dict(pkl_path)\n",
    "print(f\"Loaded data for {len(symbol_pnl_dict)} symbols\")\n",
    "\n",
    "# # 绘制portfolio图表\n",
    "fig_portfolio = plot_portfolio_pnl(symbol_pnl_dict,initial_capital=100000)\n",
    "fig_portfolio.show()  # 在notebook中显示\n",
    "\n",
    "#绘制特定symbol的图表\n",
    "symbol = 'LSK'  # 修改为你想查看的symbol\n",
    "if symbol in symbol_pnl_dict:\n",
    "    fig_symbol = plot_symbol_curves(symbol, symbol_pnl_dict[symbol])\n",
    "    fig_symbol.show()  # 在notebook中显示\n",
    "else:\n",
    "    print(f\"Symbol {symbol} not found in data\")\n",
    "\n",
    "# # 绘制收益最高的前5个symbol\n",
    "# top_symbols = get_top_symbols(symbol_pnl_dict, top_n=5)\n",
    "# print(f\"Top 5 symbols by return: {top_symbols}\")\n",
    "\n",
    "# for symbol in top_symbols:\n",
    "#     fig = plot_symbol_curves(symbol, symbol_pnl_dict[symbol])\n",
    "#     fig.show()  # 在notebook中显示\n",
    "\n",
    "# 绘制收益最低的前5个symbol\n",
    "# bottom_symbols = get_bottom_symbols(symbol_pnl_dict, bottom_n=5)\n",
    "# print(f\"Bottom 5 symbols by return: {bottom_symbols}\")\n",
    "\n",
    "# for symbol in bottom_symbols:\n",
    "#     fig = plot_symbol_curves(symbol, symbol_pnl_dict[symbol])\n",
    "#     fig.show()  # 在notebook中显示\n",
    "\n",
    "\n",
    "# 绘制全部symbol图标\n",
    "# for symbol,_ in symbol_pnl_dict.items():\n",
    "#     fig = plot_symbol_curves(symbol, symbol_pnl_dict[symbol])\n",
    "#     fig.show()  # 在notebook中显示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('/Volumes/T7/Obentech/dcdlData/binance/books/ALCH/swap/alchusdt_2025-06-19_depth5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['received_time'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(temp2['local time'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = pd.read_csv('/Users/rayxu/Downloads/depth_gateio_swap_btc-usdt_5_100.copy.csv')\n",
    "temp2['local time'] = '2025-' + temp2['local time'].astype(str)  # 补上年份\n",
    "temp2['exchange time'] = pd.to_datetime(temp2['exchange time'],unit='ms')\n",
    "temp2['local time'] = pd.to_datetime(temp2['local time']) - pd.Timedelta(hours=8)\n",
    "temp2['time_diff'] = (temp2['local time'] - temp2['exchange time']).dt.total_seconds() * 1000\n",
    "print(temp['time_diff'].mean())\n",
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['time_diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['time_diff'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('/Volumes/T7/Obentech/dcdlData/binance/books/ETH/swap/ethusdt_2025-06-18_depth5.csv')\n",
    "temp = pd.read_csv('/Users/rayxu/Desktop/Obentech/dcdlData/binance/books/ETH/swap/ethusdt_2025-01-01_depth5.csv')\n",
    "temp['T'] = pd.to_datetime(temp['T'],unit='ms')\n",
    "temp['E'] = pd.to_datetime(temp['E'],unit='ms')\n",
    "temp['received_time'] = pd.to_datetime(temp['received_time']) - pd.Timedelta(hours=8)\n",
    "temp['time_diff'] = (temp['received_time'] - temp['T']).dt.total_seconds() * 1000\n",
    "temp['time_diff_E'] = (temp['E'] - temp['T']).dt.total_seconds() * 1000\n",
    "temp = temp[1:]\n",
    "print(temp['time_diff'].mean())\n",
    "print(temp['time_diff_E'].mean())\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['time_diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[temp['time_diff']>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['time_diff'].nlargest(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['time_diff'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['time_diff_E'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_diff['funding_diff'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_diff['funding_diff'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_diff['funding_diff'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_diff['funding_diff'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卸载numpy\n",
    "pip uninstall numpy -y\n",
    "\n",
    "# 重新安装numpy\n",
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pandas.errors import EmptyDataError\n",
    "import pytz\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "tickers =  ['BTC','ETH','XRP','SOL','DOGE','TON','ONDO','AAVE','ICP','TRX','XLM','IMX','TRUMP','BNB','ADA','SUI','LINK','AVAX','HBAR','BCH','DOT','LTC','APT','UNI','ETC', 'RENDER', 'POL', 'ATOM', 'ALGO', 'FIL', 'TIA', 'TAO','SONIC','VIRTUAL','MOVE', 'ETHFI' ,'KAITO' ,'MOODENG' ,'VINE' ,'GOAT','PARTI','FARTCOIN','GRASS','JELLYJELLY','EOS', 'NEIRO', 'OP', 'PENGU', 'PEOPLE', 'PNUT', 'WIF', 'WLD','ME','ALCH','WAL','ICX','VANA','NEIROETH','T','BABY','AGLD','SSV','PLUME','NMR','JST','ANIME','IOST','UMA','MASK','RVN','BR','COMP','CATI','INIT','BRETT','AVAAI','AIXBT','EIGEN','WCT']\n",
    "# tickers = ['PENGU' ,'SONIC','MOVE','IMX']\n",
    "data_dir = '/Users/rayxu/Downloads'\n",
    "records = []\n",
    "\n",
    "\n",
    "# start_date = pd.Timestamp('2025-06-06 06:20:00') \n",
    "\n",
    "start_date = pd.Timestamp('2025-06-13 06:20:00') \n",
    "# start_date = pd.Timestamp('2025-05-22 06:20:00') \n",
    "# end_date = pd.Timestamp('2025-05-31 06:20:00') \n",
    "end_date = pd.Timestamp('2025-06-17 04:40:00')  # 只保留这之前的数据\n",
    "\n",
    "\n",
    "# 为了对应回测曲线\n",
    "# start_date_real = pd.Timestamp('2025-05-21 00:00:00') \n",
    "# end_date = pd.Timestamp('2025-05-21 23:59:59')  \n",
    "\n",
    "\n",
    "def load_and_concat(data_dir, ticker, kind, dropna_subset=None):\n",
    "    \"\"\"\n",
    "    读取并合并：\n",
    "      '{ticker}_USDT.{kind}.csv' 和 '{ticker}_USDT.{kind} (1).csv'。\n",
    "    逻辑：\n",
    "      1. 对每个路径，存在则尝试 pd.read_csv：\n",
    "         - 如果抛 EmptyDataError，认为是空文件，跳过；\n",
    "         - 否则读出 DF，再做 dropna（如果指定），再判断 df.empty。\n",
    "      2. 收集所有非空 df：\n",
    "         - 若列表为空，返回 None；\n",
    "         - 若只有 1 个，reset_index 后返回它；\n",
    "         - 若有 2 个，pd.concat 后返回。\n",
    "    \"\"\"\n",
    "    paths = [\n",
    "        os.path.join(data_dir, f\"{ticker}_USDT.{kind}.csv\"),\n",
    "        os.path.join(data_dir, f\"{ticker}_USDT.{kind} (1).csv\"),\n",
    "        os.path.join(data_dir, f\"{ticker}_USDT.{kind} (2).csv\")\n",
    "    ]\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "        except EmptyDataError:\n",
    "            # 文件存在但无数据，跳过\n",
    "            continue\n",
    "        if dropna_subset:\n",
    "            df = df.dropna(subset=dropna_subset)\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "    if not dfs:\n",
    "        print(f'No {kind} data found for {ticker}')\n",
    "        return None\n",
    "    if len(dfs) == 1:\n",
    "        return dfs[0].reset_index(drop=True)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "def remove_outliers_zscore(\n",
    "        s: pd.Series,\n",
    "        z_thresh: float = 3.0,\n",
    "        winsorize: bool = False\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Remove (or optionally winsor‑ize) extreme values from a Series\n",
    "    using the z‑score rule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : pd.Series\n",
    "        Input Series (long_slippage or short_slippage).\n",
    "    z_thresh : float, default 3.0\n",
    "        Absolute z‑score above which a point is considered an outlier.\n",
    "    winsorize : bool, default False\n",
    "        • False  → drop the outliers  \n",
    "        • True   → clamp outliers to ±z_thresh·σ instead of dropping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Outlier‑filtered (or winsor‑ized) Series, index preserved.\n",
    "    \"\"\"\n",
    "    if s.empty:\n",
    "        return s.copy()\n",
    "\n",
    "    mean, std = s.mean(), s.std(ddof=0)\n",
    "    if std == 0:            # avoid division‑by‑zero on degenerate data\n",
    "        return s.copy()\n",
    "\n",
    "    z_scores = (s - mean) / std\n",
    "    mask = z_scores.abs() <= z_thresh\n",
    "\n",
    "    if winsorize:\n",
    "        lower_bound = mean - z_thresh * std\n",
    "        upper_bound = mean + z_thresh * std\n",
    "        return s.clip(lower_bound, upper_bound)\n",
    "    else:\n",
    "        return s[mask]\n",
    "\n",
    "    \n",
    "def parse_start_time(start_str):\n",
    "    match = re.match(r'^([0-9\\-:.\\s]+)\\s\\+0800', str(start_str))\n",
    "    if match:\n",
    "        time_str = match.group(1)\n",
    "        try:\n",
    "            return pd.Timestamp(time_str)\n",
    "        except Exception:\n",
    "            return pd.NaT\n",
    "    else:\n",
    "        return pd.NaT\n",
    "    \n",
    "\n",
    "portfolio_cum_pnl = pd.Series(dtype=float)\n",
    "portfolio_trade_pnl = pd.Series(dtype=float)\n",
    "portfolio_funding_pnl = pd.Series(dtype=float)\n",
    "\n",
    "\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "\n",
    "        denominator = (\n",
    "            600 if ticker in ['ehhe']\n",
    "            else 1000 if ticker in ['XLM','ETH']\n",
    "            else 2000 if ticker in ['ME','ALCH','ICX','VANA','NEIROETH','T','BABY','AGLD','SSV','NMR','IOST','JST']\n",
    "            else 10000 if ticker in ['KAITO','GOAT','ETHFI','SONIC','VINE','JELLYJELLY','MOODENG','VIRTUAL','MOVE','TRUMP','WCT','SOL','NEIRO']\n",
    "            else 5000 if ticker in ['PARTI','IMX','ATOM','WAL']\n",
    "            else 200\n",
    "        )\n",
    "\n",
    "\n",
    "        df_open = load_and_concat(data_dir, ticker, \"open\",   dropna_subset=['pos_id'])\n",
    "        df_close = load_and_concat(data_dir, ticker, \"close\",  dropna_subset=['pos_ids'])\n",
    "        df_funding = load_and_concat(data_dir, ticker, \"funding\")\n",
    "        \n",
    "        if df_open is None or df_open.empty:\n",
    "            print(f'Missing open data for {ticker}, skipping...')\n",
    "            continue\n",
    "\n",
    "        if ticker == 'WCT':  \n",
    "            end_date = pd.Timestamp('2025-06-04 04:40:00')\n",
    "\n",
    "\n",
    "        if df_funding is not None:\n",
    "            df_funding['ts'] = pd.to_datetime(df_funding['ts'])\n",
    "            df_funding = df_funding[(df_funding['ts'] >= start_date)&(df_funding['ts']  <= end_date)]\n",
    "            funding_pnl = df_funding['funding'].sum()\n",
    "            df_funding['ts_rounded'] = df_funding['ts'].dt.floor('1s')\n",
    "            funding_net = df_funding.groupby('ts_rounded')['funding'].sum().reset_index()\n",
    "            funding_pnl_series = funding_net.set_index('ts_rounded')['funding'].cumsum()\n",
    "\n",
    "        else:\n",
    "            funding_pnl = 0\n",
    "\n",
    "\n",
    "        df_open['time'] = df_open['start'].apply(parse_start_time)\n",
    "        df_open = df_open[(df_open['time'] >= start_date) & (df_open['time'] <= end_date)]\n",
    "\n",
    "        s = df_funding['ts'].iloc[0]\n",
    "        if df_open.empty:\n",
    "            s2 = pd.NaT\n",
    "        else:\n",
    "            s2 = df_open['time'].iloc[0]\n",
    "        \n",
    "\n",
    "        if df_close is not None and not df_close.empty:\n",
    "\n",
    "            df_close['time'] = df_close['start'].apply(parse_start_time)\n",
    "            df_close = df_close[(df_close['time'] >= start_date) & (df_close['time'] <= end_date)]\n",
    "            e = df_funding['ts'].iloc[-1]\n",
    "            e2 = df_close['time'].iloc[-1]\n",
    "            e3 = end_date\n",
    "            s = min(pd.to_datetime(s), s2)   \n",
    "            e = max(pd.to_datetime(e), e2,e3)\n",
    "            total_days = (e-s).total_seconds() / 86400\n",
    "\n",
    "            df_open['sign_short'] = np.where(df_open['swap1_side'] == 'buy', 1, 0)\n",
    "            df_close['sign_short'] = np.where(df_close['swap1_side'] == 'closeShort', 1, 0)\n",
    "            short_slippage = pd.concat([(df_open['sr_open_real']  - df_open['sr_open'])[df_open['sign_short'] == 1], (df_close['sr_close_real'] - df_close['sr_close'])[df_close['sign_short'] == 1]], ignore_index=True)\n",
    "\n",
    "\n",
    "            df_open['sign_long'] = np.where(df_open['swap1_side'] == 'sell', 1, 0)\n",
    "            df_close['sign_long'] = np.where(df_close['swap1_side'] == 'closeLong', 1, 0)\n",
    "            long_slippage = pd.concat([(df_open['sr_open']  - df_open['sr_open_real'])[df_open['sign_long'] == 1], (df_close['sr_close'] - df_close['sr_close_real'])[df_close['sign_long'] == 1]], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "            total_earn_series = df_close['real_earn']\n",
    "            total_earn = total_earn_series.sum()\n",
    "\n",
    "            spread_pnl = total_earn - df_close['funding1_real'].sum() - df_close['funding2_real'].sum()\n",
    "\n",
    "            spread_series = df_close['real_earn'] - df_close['funding1_real'] - df_close['funding2_real']\n",
    "\n",
    "            spread_winning_rate = spread_series[spread_series > 0].count() / len(spread_series) if len(spread_series) > 0 else np.nan\n",
    "        \n",
    "\n",
    "\n",
    "            trade_pnl = spread_series.cumsum()\n",
    "\n",
    "\n",
    "            df_close = df_close.set_index('time')\n",
    "            trade_pnl.index = df_close.index\n",
    "            # cum_pnl = df_close['real_earn'].cumsum()-df_close['funding1_real'].cumsum() - df_close['funding2_real'].cumsum()\n",
    "\n",
    "            # 先获取两个索引的并集（按时间排序）\n",
    "            combined_index = trade_pnl.index.union(funding_pnl_series.index)\n",
    "\n",
    "            # 将两个Series都reindex到合并后的索引（统一填充方式）\n",
    "            cum_pnl_aligned = trade_pnl.reindex(combined_index, method='ffill')\n",
    "            cum_pnl_aligned.fillna(0, inplace=True)\n",
    "            funding_pnl_aligned = funding_pnl_series.reindex(combined_index, method='ffill')\n",
    "            funding_pnl_aligned.fillna(0, inplace=True)\n",
    "            cum_pnl_combined = cum_pnl_aligned + funding_pnl_aligned\n",
    "            clean_long  = remove_outliers_zscore(long_slippage,  z_thresh=3.0)\n",
    "            clean_short = remove_outliers_zscore(short_slippage, z_thresh=3.0)\n",
    "            stats = {\n",
    "                'ticker': ticker,\n",
    "                'short_slippage_mean': clean_short.mean(),\n",
    "                'short_slippage_std': short_slippage.std(),\n",
    "                'short_slippage_max': short_slippage.min(),\n",
    "                'short_slippage_min': short_slippage.max(),\n",
    "                'short_count': len(clean_short),\n",
    "\n",
    "                'long_slippage_mean': clean_long.mean(),\n",
    "                'long_slippage_std': long_slippage.std(),\n",
    "                'long_slippage_max': long_slippage.min(),\n",
    "                'long_slippage_min': long_slippage.max(),\n",
    "                'long_count': len(clean_long),\n",
    "\n",
    "                'spread_winning_rate':spread_winning_rate,\n",
    "                'spread_pnl': spread_pnl,\n",
    "                'funding_pnl': funding_pnl,\n",
    "                'total_pnl': spread_pnl + funding_pnl,\n",
    "                'Total Ret Rate': 365* (spread_pnl + funding_pnl)/(denominator*total_days),\n",
    "                'Spread Ret Rate': 365*spread_pnl/(denominator*total_days),\n",
    "                'Funding Ret Rate': 365*funding_pnl/(denominator*total_days),\n",
    "                'close_count': len(df_close),\n",
    "                'daily_turnover':(len(df_close)/(0.5*denominator/(df_close['deposit1_real'].mean()+df_close['deposit2_real'].mean())))/total_days,\n",
    "                'total_days':total_days\n",
    "            }\n",
    "\n",
    "        else: # 没有close数据 ,但是有funding的数据\n",
    "            print(f'{ticker} has no close data, using only funding curve.')\n",
    "            e = df_funding['ts'].iloc[-1]\n",
    "            e3 = end_date\n",
    "            s = min(pd.to_datetime(s), s2)   \n",
    "            e = max(pd.to_datetime(e), e3)\n",
    "            total_days = (e-s).total_seconds() / 86400\n",
    "            df_open['sign_short'] = np.where(df_open['swap1_side'] == 'buy', 1, 0)\n",
    "\n",
    "            short_slippage = (df_open['sr_open_real']  - df_open['sr_open'])[df_open['sign_short'] == 1]\n",
    "\n",
    "\n",
    "            df_open['sign_long'] = np.where(df_open['swap1_side'] == 'sell', 1, 0)\n",
    "            long_slippage = (df_open['sr_open']  - df_open['sr_open_real'])[df_open['sign_long'] == 1]\n",
    "\n",
    "            spread_pnl = 0\n",
    "            spread_winning_rate = np.nan\n",
    "        \n",
    "            trade_pnl = pd.Series(dtype=float)\n",
    "            # trade_pnl.index = df_close.index\n",
    "            cum_pnl = pd.Series(dtype=float)\n",
    "\n",
    "            # 先获取两个索引的并集（按时间排序）\n",
    "            combined_index = cum_pnl.index.union(funding_pnl_series.index)\n",
    "\n",
    "            # 将两个Series都reindex到合并后的索引（统一填充方式）\n",
    "            cum_pnl_aligned = cum_pnl.reindex(combined_index, method='ffill')\n",
    "            cum_pnl_aligned.fillna(0, inplace=True)\n",
    "            funding_pnl_aligned = funding_pnl_series.reindex(combined_index, method='ffill')\n",
    "            funding_pnl_aligned.fillna(0, inplace=True)\n",
    "            cum_pnl_combined = funding_pnl_aligned\n",
    "\n",
    "            clean_long  = remove_outliers_zscore(long_slippage,  z_thresh=3.0)\n",
    "            clean_short = remove_outliers_zscore(short_slippage, z_thresh=3.0)\n",
    "            stats = {\n",
    "                'ticker': ticker,\n",
    "                'short_slippage_mean': clean_short.mean(),\n",
    "                'short_slippage_std': short_slippage.std(),\n",
    "                'short_slippage_max': short_slippage.min(),\n",
    "                'short_slippage_min': short_slippage.max(),\n",
    "                'short_count': len(clean_short),\n",
    "\n",
    "                'long_slippage_mean': clean_long.mean(),\n",
    "                'long_slippage_std': long_slippage.std(),\n",
    "                'long_slippage_max': long_slippage.min(),\n",
    "                'long_slippage_min': long_slippage.max(),\n",
    "                'long_count': len(clean_long),\n",
    "\n",
    "                'spread_winning_rate':spread_winning_rate,\n",
    "                'spread_pnl': spread_pnl,\n",
    "                'funding_pnl': funding_pnl,\n",
    "                'total_pnl': spread_pnl + funding_pnl,\n",
    "                'Total Ret Rate': 365* (spread_pnl + funding_pnl)/(denominator*total_days),\n",
    "                'Spread Ret Rate': 365*spread_pnl/(denominator*total_days),\n",
    "                'Funding Ret Rate': 365*funding_pnl/(denominator*total_days),\n",
    "                'open_count': len(df_open),\n",
    "                'daily_turnover':(len(df_open)/(0.5*denominator/(df_open['deposit1_real'].mean()+df_open['deposit2_real'].mean())))/total_days,\n",
    "                'total_days':total_days\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        records.append(stats)\n",
    "\n",
    "\n",
    "        # union index + forward fill + fillna(0)\n",
    "        combined_index = cum_pnl_combined.index.union(portfolio_cum_pnl.index)\n",
    "        portfolio_cum_pnl = (\n",
    "            portfolio_cum_pnl.reindex(combined_index, method='ffill').fillna(0)\n",
    "            + cum_pnl_combined.reindex(combined_index, method='ffill').fillna(0)\n",
    "        )\n",
    "\n",
    "        portfolio_trade_pnl = (\n",
    "            portfolio_trade_pnl.reindex(combined_index, method='ffill').fillna(0)\n",
    "            + trade_pnl.reindex(combined_index, method='ffill').fillna(0)\n",
    "        )\n",
    "\n",
    "        portfolio_funding_pnl = (\n",
    "            portfolio_funding_pnl.reindex(combined_index, method='ffill').fillna(0)\n",
    "            + funding_pnl_series.reindex(combined_index, method='ffill').fillna(0)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import plotly.graph_objects as go\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # 1. 总盈亏（含 funding + real_earn - 手动费用）\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=cum_pnl_combined.index, y=cum_pnl_combined.values,\n",
    "            mode='lines+markers',\n",
    "            name='Cumulative PnL',\n",
    "            line=dict(color='royalblue'),\n",
    "            fill='tozeroy',\n",
    "            fillcolor='rgba(65,105,225,0.1)'\n",
    "        ))\n",
    "\n",
    "        # 2. funding 累计盈亏\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=funding_pnl_series.index, y=funding_pnl_series.values,\n",
    "            mode='lines+markers',\n",
    "            name='Funding PnL',\n",
    "            line=dict(dash='dash', color='indianred')\n",
    "        ))\n",
    "\n",
    "        # 3. trade PnL（仅 real_earn）\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=trade_pnl.index, y=trade_pnl.values,\n",
    "            mode='lines+markers',\n",
    "            name='Trade PnL',\n",
    "            line=dict(color='deeppink')\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{ticker} Cumulative PnL from {s} to {e}\",\n",
    "            xaxis_title='Time (Asia/Shanghai)',\n",
    "            yaxis_title='PnL',\n",
    "            legend=dict(font=dict(size=12)),\n",
    "            template='plotly_white',\n",
    "            height=700,\n",
    "            width=1400\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] Error: {e}\")\n",
    "# Portfolio 总体收益图\n",
    "        \n",
    "\n",
    "\n",
    "initial_capital = 100000\n",
    "start_time = portfolio_cum_pnl.index.min()\n",
    "end_time   = portfolio_cum_pnl.index.max()\n",
    "\n",
    "total_days = (end_time - start_time).total_seconds() / 86400\n",
    "total_ret  = portfolio_cum_pnl.iloc[-1]\n",
    "spread_ret = portfolio_trade_pnl.iloc[-1]\n",
    "funding_ret = portfolio_funding_pnl.iloc[-1]\n",
    "\n",
    "total_ret_rate = 365 * total_ret / initial_capital / total_days\n",
    "spread_ret_rate = 365 * spread_ret / initial_capital / total_days\n",
    "funding_ret_rate = 365 * funding_ret / initial_capital / total_days\n",
    "\n",
    "start_str = start_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "end_str = end_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "fig_portfolio = go.Figure()\n",
    "\n",
    "fig_portfolio.add_trace(go.Scatter(\n",
    "    x=portfolio_cum_pnl.index, y=portfolio_cum_pnl.values,\n",
    "    mode='lines',\n",
    "    name='Portfolio Total PnL',\n",
    "    line=dict(color='royalblue'),\n",
    "    fill='tozeroy',\n",
    "    fillcolor='rgba(65,105,225,0.1)'\n",
    "))\n",
    "\n",
    "fig_portfolio.add_trace(go.Scatter(\n",
    "    x=portfolio_trade_pnl.index, y=portfolio_trade_pnl.values,\n",
    "    mode='lines',\n",
    "    name='Portfolio Trade PnL',\n",
    "    line=dict(color='deeppink')\n",
    "))\n",
    "\n",
    "fig_portfolio.add_trace(go.Scatter(\n",
    "    x=portfolio_funding_pnl.index, y=portfolio_funding_pnl.values,\n",
    "    mode='lines',\n",
    "    name='Portfolio Funding PnL',\n",
    "    line=dict(dash='dash', color='indianred')\n",
    "))\n",
    "\n",
    "fig_portfolio.update_layout(\n",
    "    # title=f'Total Portfolio Cumulative PnL from {start_str} to {end_str}, total days: {total_days}. Total annualized Return Rates: {total_ret_rate:.2%} , {spread_ret_rate:.2%} (Spread), {funding_ret_rate:.2%}(Funding)',\n",
    "    title=(\n",
    "        f\"Portfolio PnL from {start_str} to {end_str} ({total_days:.2f} days)  \\n\"\n",
    "        f\"Annualized Return: {total_ret_rate:.2%} | Spread: {spread_ret_rate:.2%} | Funding: {funding_ret_rate:.2%}\"\n",
    "    ),\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='PnL',\n",
    "    template='plotly_white',\n",
    "    legend=dict(font=dict(size=12)),\n",
    "    height=700,\n",
    "    width=1400\n",
    ")\n",
    "\n",
    "fig_portfolio.show()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Portfolio Total Return: {total_ret:.2f} USDT\")\n",
    "print(f\"Portfolio Holding Days: {total_days:.2f} days\")\n",
    "print(f\"Annualized Total Return Rate:   {total_ret_rate:.2%}\")\n",
    "print(f\"Annualized Spread Return Rate:  {spread_ret_rate:.2%}\")\n",
    "print(f\"Annualized Funding Return Rate: {funding_ret_rate:.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(records).sort_values(by = 'Total Ret Rate', ascending=False)\n",
    "result_df\n",
    "# result_df.to_csv('slippage_summary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_diff['funding_diff'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['total_pnl'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/Users/rayxu/Downloads/pnl_analysis_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['total_pnl'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df['ticker']=='VINE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_funding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers =  ['BTC','ETH','XRP','SOL','DOGE','TON','ONDO','AAVE','ICP','TRX','XLM','IMX','WCT','TRUMP','BNB','ADA','SUI','LINK','AVAX','HBAR','BCH','DOT','LTC','APT','UNI','ETC', 'RENDER', 'POL', 'ATOM', 'ALGO', 'FIL', 'TIA', 'TAO','SONIC','VIRTUAL','MOVE', 'ETHFI' ,'KAITO' ,'MOODENG' ,'VINE' ,'GOAT','PARTI','FARTCOIN','GRASS','JELLYJELLY']\n",
    "symbols = ['BTC','ETH','XRP','SOL','DOGE','TON','ONDO','AAVE','ICP','TRX','XLM','IMX','WCT','TRUMP','BNB','ADA','SUI','LINK','AVAX','HBAR','BCH','DOT','LTC','APT','UNI','ETC', 'RENDER', 'POL', 'ATOM', 'ALGO', 'FIL', 'TIA', 'TAO','SONIC','VIRTUAL','MOVE','MOODENG','PNUT','WIF','GOAT','FARTCOIN','PARTI','KAITO','VINE','NEIRO','ETHFI','WLD','PEOPLE','JELLYJELLY','PENGU','EOS','GRASS','OP']\n",
    "set(symbols) - set(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( ['BTC','ETH','XRP','SOL','DOGE','TON','ONDO','AAVE','ICP','TRX','XLM','IMX','TRUMP','BNB','ADA','SUI','LINK','AVAX','HBAR','BCH','DOT','LTC','APT','UNI','ETC', 'RENDER', 'POL', 'ATOM', 'ALGO', 'FIL', 'TIA', 'TAO','SONIC','VIRTUAL','MOVE', 'ETHFI' ,'KAITO' ,'MOODENG' ,'VINE' ,'GOAT','PARTI','FARTCOIN','GRASS','JELLYJELLY','EOS', 'NEIRO', 'OP', 'PENGU', 'PEOPLE', 'PNUT', 'WIF', 'WLD','ME','ALCH','WAL','ICX','VANA','NEIROETH','T','BABY','AGLD','SSV','PLUME','NMR','JST','ANIME','IOST','UMA','MASK','RVN','BR','COMP','WCT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最新数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pandas.errors import EmptyDataError\n",
    "import pytz\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "tickers =  ['BTC','ETH','XRP','SOL','DOGE','TON','ONDO','AAVE','ICP','TRX','XLM','IMX','TRUMP','BNB','ADA','SUI','LINK','AVAX','HBAR','BCH','DOT','LTC','APT','UNI','ETC', 'RENDER', 'POL', 'ATOM', 'ALGO', 'FIL', 'TIA', 'TAO','SONIC','VIRTUAL','MOVE', 'ETHFI' ,'KAITO' ,'MOODENG' ,'VINE' ,'GOAT','PARTI','FARTCOIN','GRASS','JELLYJELLY','EOS', 'NEIRO', 'OP', 'PENGU', 'PEOPLE', 'PNUT', 'WIF', 'WLD','ME','ALCH','WAL','ICX','VANA','NEIROETH','T','BABY','AGLD','SSV','PLUME','NMR','JST','ANIME','IOST','UMA','MASK','RVN','BR','COMP','WCT']\n",
    "# tickers = ['MOVE']\n",
    "# tickers = ['GOAT','ETHFI','KAITO','VINE','SONIC','IMX','JELLYJELLY','PARTI','SOL','TRUMP','MOODENG','MOVE','ATOM','NEIRO','ALGO','VIRTUAL','XLM','PENGU','ME','ALCH','WAL','ICX','VANA','NEIROETH','T','BABY','AGLD','SSV','WCT']\n",
    "# tickers = ['SONIC']\n",
    "# tickers = ['PENGU' ,'SONIC','MOVE','IMX']\n",
    "data_dir = '/Users/rayxu/Downloads'\n",
    "records = []\n",
    "\n",
    "cutoff_time_utc = pd.Timestamp('2025-05-06 14:00:00')\n",
    "\n",
    "\n",
    "#美西时间 2025-05-16 22:25把KAITO加到了5000U\n",
    "#美西时间 2025-05-18 23:20把ETHFI加到了5000U\n",
    "#美西时间 2025-05-19 00:50把GOAT加到了5000U\n",
    "#美西时间 2025-05-19 13:38把SONIC加到了5000U\n",
    "#美西时间 2025-05-20 02:19把IMX加到了2500U\n",
    "#美西时间 2025-05-21 03:27把VINE加到了5000U\n",
    "#美西时间 2025-05-21 04:05把WCT加到了2500U\n",
    "#美西时间 2025-05-21 03:27把PARTI加到了2500U\n",
    "#美西时间 2025-05-21 15:19把JELLYJELLY加到了5000U\n",
    "#美西时间 2025-05-23 14:41把MOODENGUSDT加到了5000U\n",
    "#美西时间 2025-05-23 14:44把VIRTUAL加到了5000U\n",
    "#美西时间 2025-05-23 14:49把WCT从2500U加到了5000U\n",
    "#美西时间 2025-05-25 18:03把SOL加到了5000U\n",
    "#美西时间 2025-05-26 00:53把TRUMP加到了5000U\n",
    "#美西时间 2025-05-26 00:53把MOVE加到了5000U\n",
    "#美西时间 2025-05-26 00:53把ATOM加到了1000U\n",
    "#美西时间 2025-05-26 00:53把PENGU加到了2500U\n",
    "\n",
    "\n",
    "start_date_real = pd.Timestamp('2025-05-17 13:25:00')  # KATIO的时间点\n",
    "\n",
    "start_date_real = pd.Timestamp('2025-06-10 05:20:00')   #计算滑点的时间点\n",
    "start_date_real = pd.Timestamp('2025-05-22 06:20:00') \n",
    "# start_date_real = pd.Timestamp('2025-06-01 04:30:00') \n",
    "# end_date = pd.Timestamp('2025-05-31 04:30:00')  # 只保留这之前的数据\n",
    "end_date = pd.Timestamp('2025-06-13 04:30:00')  # 只保留这之前的数据\n",
    "\n",
    "\n",
    "# 为了对应回测曲线\n",
    "# start_date_real = pd.Timestamp('2025-05-21 00:00:00') \n",
    "# end_date = pd.Timestamp('2025-05-21 23:59:59')  \n",
    "\n",
    "\n",
    "def load_and_concat(data_dir, ticker, kind, dropna_subset=None):\n",
    "    \"\"\"\n",
    "    读取并合并：\n",
    "      '{ticker}_USDT.{kind}.csv' 和 '{ticker}_USDT.{kind} (1).csv'。\n",
    "    逻辑：\n",
    "      1. 对每个路径，存在则尝试 pd.read_csv：\n",
    "         - 如果抛 EmptyDataError，认为是空文件，跳过；\n",
    "         - 否则读出 DF，再做 dropna（如果指定），再判断 df.empty。\n",
    "      2. 收集所有非空 df：\n",
    "         - 若列表为空，返回 None；\n",
    "         - 若只有 1 个，reset_index 后返回它；\n",
    "         - 若有 2 个，pd.concat 后返回。\n",
    "    \"\"\"\n",
    "    paths = [\n",
    "        os.path.join(data_dir, f\"{ticker}_USDT.{kind}.csv\"),\n",
    "        os.path.join(data_dir, f\"{ticker}_USDT.{kind} (1).csv\"),\n",
    "        os.path.join(data_dir, f\"{ticker}_USDT.{kind} (2).csv\")\n",
    "    ]\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "        except EmptyDataError:\n",
    "            # 文件存在但无数据，跳过\n",
    "            continue\n",
    "        if dropna_subset:\n",
    "            df = df.dropna(subset=dropna_subset)\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "    if not dfs:\n",
    "        print(f'No {kind} data found for {ticker}')\n",
    "        return None\n",
    "    if len(dfs) == 1:\n",
    "        return dfs[0].reset_index(drop=True)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def remove_outliers_zscore(\n",
    "        s: pd.Series,\n",
    "        z_thresh: float = 3.0,\n",
    "        winsorize: bool = False\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Remove (or optionally winsor‑ize) extreme values from a Series\n",
    "    using the z‑score rule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : pd.Series\n",
    "        Input Series (long_slippage or short_slippage).\n",
    "    z_thresh : float, default 3.0\n",
    "        Absolute z‑score above which a point is considered an outlier.\n",
    "    winsorize : bool, default False\n",
    "        • False  → drop the outliers  \n",
    "        • True   → clamp outliers to ±z_thresh·σ instead of dropping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Outlier‑filtered (or winsor‑ized) Series, index preserved.\n",
    "    \"\"\"\n",
    "    if s.empty:\n",
    "        return s.copy()\n",
    "\n",
    "    mean, std = s.mean(), s.std(ddof=0)\n",
    "    if std == 0:            # avoid division‑by‑zero on degenerate data\n",
    "        return s.copy()\n",
    "\n",
    "    z_scores = (s - mean) / std\n",
    "    mask = z_scores.abs() <= z_thresh\n",
    "\n",
    "    if winsorize:\n",
    "        lower_bound = mean - z_thresh * std\n",
    "        upper_bound = mean + z_thresh * std\n",
    "        return s.clip(lower_bound, upper_bound)\n",
    "    else:\n",
    "        return s[mask]\n",
    "\n",
    "# ---------------- Example ----------------\n",
    "# clean_long  = remove_outliers_zscore(long_slippage,  z_thresh=3.0)\n",
    "# clean_short = remove_outliers_zscore(short_slippage, z_thresh=3.0)\n",
    "\n",
    "    \n",
    "def parse_start_time(start_str):\n",
    "    match = re.match(r'^([0-9\\-:.\\s]+)\\s\\+0800', str(start_str))\n",
    "    if match:\n",
    "        time_str = match.group(1)\n",
    "        try:\n",
    "            return pd.Timestamp(time_str)\n",
    "        except Exception:\n",
    "            return pd.NaT\n",
    "    else:\n",
    "        return pd.NaT\n",
    "    \n",
    "\n",
    "portfolio_cum_pnl = pd.Series(dtype=float)\n",
    "portfolio_trade_pnl = pd.Series(dtype=float)\n",
    "portfolio_funding_pnl = pd.Series(dtype=float)\n",
    "\n",
    "\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "\n",
    "        denominator = (\n",
    "            600 if ticker in ['ehhe']\n",
    "            else 1000 if ticker in ['XLM','ETH']\n",
    "            else 2000 if ticker in ['ME','ALCH','ICX','VANA','NEIROETH','T','BABY','AGLD','SSV','NMR','IOST','JST']\n",
    "            else 10000 if ticker in ['KAITO','GOAT','ETHFI','SONIC','VINE','JELLYJELLY','MOODENG','VIRTUAL','MOVE','TRUMP','WCT','SOL','NEIRO']\n",
    "            else 5000 if ticker in ['PARTI','IMX','ATOM','WAL']\n",
    "            else 200\n",
    "        )\n",
    "\n",
    "        start_date = start_date_real\n",
    "\n",
    "        df_open = load_and_concat(data_dir, ticker, \"open\",   dropna_subset=['pos_id'])\n",
    "        df_close = load_and_concat(data_dir, ticker, \"close\",  dropna_subset=['pos_ids'])\n",
    "        df_funding = load_and_concat(data_dir, ticker, \"funding\")\n",
    "        if df_open is None or df_open.empty:\n",
    "            print(f'Missing open data for {ticker}, skipping...')\n",
    "            continue\n",
    "        # if df_close is None:\n",
    "        #     print(f'Missing data for {ticker}, skipping...')\n",
    "        #     continue\n",
    "        if ticker == 'KAITO':  start_date = pd.Timestamp('2025-05-17 13:25:00')  # 针对KAITO\n",
    "        if ticker == 'ETHFI':  start_date = pd.Timestamp('2025-05-19 14:20:00')  # 针对ETHFI\n",
    "        if ticker == 'GOAT':  start_date = pd.Timestamp('2025-05-19 15:50:00')  # 针对GOAT\n",
    "        if ticker == 'SONIC':  start_date = pd.Timestamp('2025-05-20 04:38:00')  # 针对SONIC\n",
    "        if ticker == 'IMX':  start_date = pd.Timestamp('2025-05-20 17:20:00')  # 针对IMX\n",
    "        if ticker == 'VINE':  start_date = pd.Timestamp('2025-05-21 18:27:00')  \n",
    "        if ticker == 'PARTI':  start_date = pd.Timestamp('2025-05-21 18:32:00')  \n",
    "        if ticker == 'WCT':  \n",
    "            start_date = pd.Timestamp('2025-05-21 19:05:00')  \n",
    "            end_date = pd.Timestamp('2025-06-04 04:40:00')\n",
    "        if ticker == 'JELLYJELLY':  start_date = pd.Timestamp('2025-05-22 06:20:00')  \n",
    "        if ticker == 'MOODENG':  start_date = pd.Timestamp('2025-05-24 05:44:00')  \n",
    "        if ticker == 'VIRTUAL':  start_date = pd.Timestamp('2025-05-24 05:44:00')  \n",
    "\n",
    "        start_date = start_date_real\n",
    "\n",
    "        if df_funding is not None:\n",
    "            df_funding['ts'] = pd.to_datetime(df_funding['ts'])\n",
    "            df_funding = df_funding[(df_funding['ts'] >= start_date)&(df_funding['ts']  <= end_date)]\n",
    "            # print(df_funding)\n",
    "            funding_pnl = df_funding['funding'].sum()\n",
    "            df_funding['ts_rounded'] = df_funding['ts'].dt.floor('1s')\n",
    "            funding_net = df_funding.groupby('ts_rounded')['funding'].sum().reset_index()\n",
    "            funding_pnl_series = funding_net.set_index('ts_rounded')['funding'].cumsum()\n",
    "\n",
    "        else:\n",
    "            funding_pnl = 0\n",
    "\n",
    "\n",
    "        df_open['time'] = df_open['start'].apply(parse_start_time)\n",
    "        df_open = df_open[(df_open['time'] >= start_date) & (df_open['time'] <= end_date)]\n",
    "\n",
    "        s = df_funding['ts'].iloc[0]\n",
    "        if df_open.empty:\n",
    "            s2 = pd.NaT\n",
    "        else:\n",
    "            s2 = df_open['time'].iloc[0]\n",
    "        \n",
    "\n",
    "        if df_close is not None and not df_close.empty:\n",
    "\n",
    "            df_close['time'] = df_close['start'].apply(parse_start_time)\n",
    "            df_close = df_close[(df_close['time'] >= start_date) & (df_close['time'] <= end_date)]\n",
    "            e = df_funding['ts'].iloc[-1]\n",
    "            e2 = df_close['time'].iloc[-1]\n",
    "            e3 = end_date\n",
    "            s = min(pd.to_datetime(s), s2)   \n",
    "            e = max(pd.to_datetime(e), e2,e3)\n",
    "            total_days = (e-s).total_seconds() / 86400\n",
    "\n",
    "            df_open['sign_short'] = np.where(df_open['swap1_side'] == 'buy', 1, 0)\n",
    "            df_close['sign_short'] = np.where(df_close['swap1_side'] == 'closeShort', 1, 0)\n",
    "            short_slippage = pd.concat([(df_open['sr_open_real']  - df_open['sr_open'])[df_open['sign_short'] == 1], (df_close['sr_close_real'] - df_close['sr_close'])[df_close['sign_short'] == 1]], ignore_index=True)\n",
    "\n",
    "\n",
    "            df_open['sign_long'] = np.where(df_open['swap1_side'] == 'sell', 1, 0)\n",
    "            df_close['sign_long'] = np.where(df_close['swap1_side'] == 'closeLong', 1, 0)\n",
    "            long_slippage = pd.concat([(df_open['sr_open']  - df_open['sr_open_real'])[df_open['sign_long'] == 1], (df_close['sr_close'] - df_close['sr_close_real'])[df_close['sign_long'] == 1]], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "            mask_after_cutoff = df_close['time'] >= cutoff_time_utc\n",
    "            earn_after = df_close.loc[mask_after_cutoff, 'real_earn']\n",
    "            earn_before = (df_close.loc[~mask_after_cutoff, 'real_earn'] + 0.00003 * 4)\n",
    "            total_earn_series = pd.concat([earn_before,earn_after]).sort_index()\n",
    "            total_earn = total_earn_series.sum()\n",
    "\n",
    "            spread_pnl = total_earn - df_close['funding1_real'].sum() - df_close['funding2_real'].sum()\n",
    "\n",
    "            spread_series = total_earn_series - df_close['funding1_real'] - df_close['funding2_real']\n",
    "\n",
    "            spread_winning_rate = spread_series[spread_series > 0].count() / len(spread_series) if len(spread_series) > 0 else np.nan\n",
    "        \n",
    "\n",
    "\n",
    "            trade_pnl = (total_earn_series - df_close['funding1_real'] - df_close['funding2_real']).cumsum()\n",
    "\n",
    "\n",
    "\n",
    "            # total_winning_rate = df_close['real_earn'].sum() / df_close['real_earn'].count() if len(df_close) > 0 else np.nan\n",
    "            df_close = df_close.set_index('time')\n",
    "            trade_pnl.index = df_close.index\n",
    "            cum_pnl = df_close['real_earn'].cumsum()-df_close['funding1_real'].cumsum() - df_close['funding2_real'].cumsum()\n",
    "\n",
    "            # funding_pnl_aligned = funding_pnl_series.reindex(cum_pnl.index, method='ffill')\n",
    "            # cum_pnl = cum_pnl + funding_pnl_aligned\n",
    "\n",
    "            # 先获取两个索引的并集（按时间排序）\n",
    "            combined_index = cum_pnl.index.union(funding_pnl_series.index)\n",
    "\n",
    "            # 将两个Series都reindex到合并后的索引（统一填充方式）\n",
    "            cum_pnl_aligned = cum_pnl.reindex(combined_index, method='ffill')\n",
    "            cum_pnl_aligned.fillna(0, inplace=True)\n",
    "            funding_pnl_aligned = funding_pnl_series.reindex(combined_index, method='ffill')\n",
    "            funding_pnl_aligned.fillna(0, inplace=True)\n",
    "            cum_pnl_combined = cum_pnl_aligned + funding_pnl_aligned\n",
    "            clean_long  = remove_outliers_zscore(long_slippage,  z_thresh=3.0)\n",
    "            clean_short = remove_outliers_zscore(short_slippage, z_thresh=3.0)\n",
    "            stats = {\n",
    "                'ticker': ticker,\n",
    "                'short_slippage_mean': clean_short.mean(),\n",
    "                'short_slippage_std': short_slippage.std(),\n",
    "                'short_slippage_max': short_slippage.min(),\n",
    "                'short_slippage_min': short_slippage.max(),\n",
    "                'short_count': len(clean_short),\n",
    "\n",
    "                'long_slippage_mean': clean_long.mean(),\n",
    "                'long_slippage_std': long_slippage.std(),\n",
    "                'long_slippage_max': long_slippage.min(),\n",
    "                'long_slippage_min': long_slippage.max(),\n",
    "                'long_count': len(clean_long),\n",
    "\n",
    "                'spread_winning_rate':spread_winning_rate,\n",
    "                'spread_pnl': spread_pnl,\n",
    "                'funding_pnl': funding_pnl,\n",
    "                'total_pnl': spread_pnl + funding_pnl,\n",
    "                'Total Ret Rate': 365* (spread_pnl + funding_pnl)/(denominator*total_days),\n",
    "                'Spread Ret Rate': 365*spread_pnl/(denominator*total_days),\n",
    "                'Funding Ret Rate': 365*funding_pnl/(denominator*total_days),\n",
    "                'close_count': len(df_close),\n",
    "                'daily_turnover':(len(df_close)/(0.5*denominator/(df_close['deposit1_real'].mean()+df_close['deposit2_real'].mean())))/total_days,\n",
    "                'total_days':total_days\n",
    "            }\n",
    "\n",
    "        else: # 没有close数据 ,但是有funding的数据\n",
    "            print(f'{ticker} has no close data, using only funding curve.')\n",
    "            e = df_funding['ts'].iloc[-1]\n",
    "            e3 = end_date\n",
    "            s = min(pd.to_datetime(s), s2)   \n",
    "            e = max(pd.to_datetime(e), e3)\n",
    "            total_days = (e-s).total_seconds() / 86400\n",
    "            df_open['sign_short'] = np.where(df_open['swap1_side'] == 'buy', 1, 0)\n",
    "\n",
    "            short_slippage = (df_open['sr_open_real']  - df_open['sr_open'])[df_open['sign_short'] == 1]\n",
    "\n",
    "\n",
    "            df_open['sign_long'] = np.where(df_open['swap1_side'] == 'sell', 1, 0)\n",
    "            long_slippage = (df_open['sr_open']  - df_open['sr_open_real'])[df_open['sign_long'] == 1]\n",
    "\n",
    "            spread_pnl = 0\n",
    "            spread_winning_rate = np.nan\n",
    "        \n",
    "            trade_pnl = pd.Series(dtype=float)\n",
    "            # trade_pnl.index = df_close.index\n",
    "            cum_pnl = pd.Series(dtype=float)\n",
    "\n",
    "            # 先获取两个索引的并集（按时间排序）\n",
    "            combined_index = cum_pnl.index.union(funding_pnl_series.index)\n",
    "\n",
    "            # 将两个Series都reindex到合并后的索引（统一填充方式）\n",
    "            cum_pnl_aligned = cum_pnl.reindex(combined_index, method='ffill')\n",
    "            cum_pnl_aligned.fillna(0, inplace=True)\n",
    "            funding_pnl_aligned = funding_pnl_series.reindex(combined_index, method='ffill')\n",
    "            funding_pnl_aligned.fillna(0, inplace=True)\n",
    "            cum_pnl_combined = funding_pnl_aligned\n",
    "\n",
    "            clean_long  = remove_outliers_zscore(long_slippage,  z_thresh=3.0)\n",
    "            clean_short = remove_outliers_zscore(short_slippage, z_thresh=3.0)\n",
    "            stats = {\n",
    "                'ticker': ticker,\n",
    "                'short_slippage_mean': clean_short.mean(),\n",
    "                'short_slippage_std': short_slippage.std(),\n",
    "                'short_slippage_max': short_slippage.min(),\n",
    "                'short_slippage_min': short_slippage.max(),\n",
    "                'short_count': len(clean_short),\n",
    "\n",
    "                'long_slippage_mean': clean_long.mean(),\n",
    "                'long_slippage_std': long_slippage.std(),\n",
    "                'long_slippage_max': long_slippage.min(),\n",
    "                'long_slippage_min': long_slippage.max(),\n",
    "                'long_count': len(clean_long),\n",
    "\n",
    "                'spread_winning_rate':spread_winning_rate,\n",
    "                'spread_pnl': spread_pnl,\n",
    "                'funding_pnl': funding_pnl,\n",
    "                'total_pnl': spread_pnl + funding_pnl,\n",
    "                'Total Ret Rate': 365* (spread_pnl + funding_pnl)/(denominator*total_days),\n",
    "                'Spread Ret Rate': 365*spread_pnl/(denominator*total_days),\n",
    "                'Funding Ret Rate': 365*funding_pnl/(denominator*total_days),\n",
    "                'open_count': len(df_open),\n",
    "                'daily_turnover':(len(df_open)/(0.5*denominator/(df_open['deposit1_real'].mean()+df_open['deposit2_real'].mean())))/total_days,\n",
    "                'total_days':total_days\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        records.append(stats)\n",
    "\n",
    "\n",
    "        # union index + forward fill + fillna(0)\n",
    "        combined_index = cum_pnl_combined.index.union(portfolio_cum_pnl.index)\n",
    "        portfolio_cum_pnl = (\n",
    "            portfolio_cum_pnl.reindex(combined_index, method='ffill').fillna(0)\n",
    "            + cum_pnl_combined.reindex(combined_index, method='ffill').fillna(0)\n",
    "        )\n",
    "\n",
    "        portfolio_trade_pnl = (\n",
    "            portfolio_trade_pnl.reindex(combined_index, method='ffill').fillna(0)\n",
    "            + trade_pnl.reindex(combined_index, method='ffill').fillna(0)\n",
    "        )\n",
    "\n",
    "        portfolio_funding_pnl = (\n",
    "            portfolio_funding_pnl.reindex(combined_index, method='ffill').fillna(0)\n",
    "            + funding_pnl_series.reindex(combined_index, method='ffill').fillna(0)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import plotly.graph_objects as go\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # 1. 总盈亏（含 funding + real_earn - 手动费用）\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=cum_pnl_combined.index, y=cum_pnl_combined.values,\n",
    "            mode='lines+markers',\n",
    "            name='Cumulative PnL',\n",
    "            line=dict(color='royalblue'),\n",
    "            fill='tozeroy',\n",
    "            fillcolor='rgba(65,105,225,0.1)'\n",
    "        ))\n",
    "\n",
    "        # 2. funding 累计盈亏\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=funding_pnl_series.index, y=funding_pnl_series.values,\n",
    "            mode='lines+markers',\n",
    "            name='Funding PnL',\n",
    "            line=dict(dash='dash', color='indianred')\n",
    "        ))\n",
    "\n",
    "        # 3. trade PnL（仅 real_earn）\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=trade_pnl.index, y=trade_pnl.values,\n",
    "            mode='lines+markers',\n",
    "            name='Trade PnL',\n",
    "            line=dict(color='deeppink')\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{ticker} Cumulative PnL from {s} to {e}\",\n",
    "            xaxis_title='Time (Asia/Shanghai)',\n",
    "            yaxis_title='PnL',\n",
    "            legend=dict(font=dict(size=12)),\n",
    "            template='plotly_white',\n",
    "            height=700,\n",
    "            width=1400\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] Error: {e}\")\n",
    "# Portfolio 总体收益图\n",
    "        \n",
    "\n",
    "\n",
    "initial_capital = 100000\n",
    "start_time = portfolio_cum_pnl.index.min()\n",
    "end_time   = portfolio_cum_pnl.index.max()\n",
    "\n",
    "total_days = (end_time - start_time).total_seconds() / 86400\n",
    "total_ret  = portfolio_cum_pnl.iloc[-1]\n",
    "spread_ret = portfolio_trade_pnl.iloc[-1]\n",
    "funding_ret = portfolio_funding_pnl.iloc[-1]\n",
    "\n",
    "total_ret_rate = 365 * total_ret / initial_capital / total_days\n",
    "spread_ret_rate = 365 * spread_ret / initial_capital / total_days\n",
    "funding_ret_rate = 365 * funding_ret / initial_capital / total_days\n",
    "\n",
    "start_str = start_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "end_str = end_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "fig_portfolio = go.Figure()\n",
    "\n",
    "fig_portfolio.add_trace(go.Scatter(\n",
    "    x=portfolio_cum_pnl.index, y=portfolio_cum_pnl.values,\n",
    "    mode='lines',\n",
    "    name='Portfolio Total PnL',\n",
    "    line=dict(color='royalblue'),\n",
    "    fill='tozeroy',\n",
    "    fillcolor='rgba(65,105,225,0.1)'\n",
    "))\n",
    "\n",
    "fig_portfolio.add_trace(go.Scatter(\n",
    "    x=portfolio_trade_pnl.index, y=portfolio_trade_pnl.values,\n",
    "    mode='lines',\n",
    "    name='Portfolio Trade PnL',\n",
    "    line=dict(color='deeppink')\n",
    "))\n",
    "\n",
    "fig_portfolio.add_trace(go.Scatter(\n",
    "    x=portfolio_funding_pnl.index, y=portfolio_funding_pnl.values,\n",
    "    mode='lines',\n",
    "    name='Portfolio Funding PnL',\n",
    "    line=dict(dash='dash', color='indianred')\n",
    "))\n",
    "\n",
    "fig_portfolio.update_layout(\n",
    "    # title=f'Total Portfolio Cumulative PnL from {start_str} to {end_str}, total days: {total_days}. Total annualized Return Rates: {total_ret_rate:.2%} , {spread_ret_rate:.2%} (Spread), {funding_ret_rate:.2%}(Funding)',\n",
    "    title=(\n",
    "        f\"Portfolio PnL from {start_str} to {end_str} ({total_days:.2f} days)  \\n\"\n",
    "        f\"Annualized Return: {total_ret_rate:.2%} | Spread: {spread_ret_rate:.2%} | Funding: {funding_ret_rate:.2%}\"\n",
    "    ),\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='PnL',\n",
    "    template='plotly_white',\n",
    "    legend=dict(font=dict(size=12)),\n",
    "    height=700,\n",
    "    width=1400\n",
    ")\n",
    "\n",
    "fig_portfolio.show()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Portfolio Total Return: {total_ret:.2f} USDT\")\n",
    "print(f\"Portfolio Holding Days: {total_days:.2f} days\")\n",
    "print(f\"Annualized Total Return Rate:   {total_ret_rate:.2%}\")\n",
    "print(f\"Annualized Spread Return Rate:  {spread_ret_rate:.2%}\")\n",
    "print(f\"Annualized Funding Return Rate: {funding_ret_rate:.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(records).sort_values(by = 'Total Ret Rate', ascending=False)\n",
    "result_df\n",
    "# result_df.to_csv('slippage_summary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "portfolio_cum_pnl.index\n",
    "\n",
    "portfolio_trade_pnl.index\n",
    "\n",
    "portfolio_funding_pnl.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把portfolio_cum_pnl, portfolio_trade_pnl, portfolio_funding_pnl 拼成一个dataframe\n",
    "portfolio_df = pd.DataFrame({\n",
    "    'cum_pnl': portfolio_cum_pnl,\n",
    "    'trade_pnl': portfolio_trade_pnl,\n",
    "    'funding_pnl': portfolio_funding_pnl\n",
    "})\n",
    "portfolio_df.to_csv('/Users/rayxu/Desktop/Obentech/0522_0610_10WU_pnl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把portfolio_df保存成excel\n",
    "portfolio_df.to_excel('/Users/rayxu/Desktop/Obentech/0522_0610_10WU_pnl.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['daily_turnover'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_cum_pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['spread_pnl'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['funding_pnl'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#美西时间 2025-05-16 22:25把KAITO加到了5000U\n",
    "#美西时间 2025-05-18 23:20把ETHFI加到了5000U\n",
    "#美西时间 2025-05-19 00:50把GOAT加到了5000U\n",
    "#美西时间 2025-05-19 13:38把SONIC加到了5000U\n",
    "#美西时间 2025-05-20 02:19把IMX加到了2500U\n",
    "#美西时间 2025-05-21 03:27把VINE加到了5000U\n",
    "#美西时间 2025-05-21 03:32把PARTI加到了2500U\n",
    "#美西时间 2025-05-21 04:05把WCT加到了2500U\n",
    "#美西时间 2025-05-21 15:19把JELLYJELLY加到了5000U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#美西时间 2025-05-16 22:25把KAITO加到了5000U\n",
    "#美西时间 2025-05-18 23:20把ETHFI加到了5000U\n",
    "#美西时间 2025-05-19 00:50把GOAT加到了5000U\n",
    "#美西时间 2025-05-19 13:38把SONIC加到了5000U\n",
    "#美西时间 2025-05-20 02:19把IMX加到了2500U\n",
    "#美西时间 2025-05-21 03:27把VINE加到了5000U\n",
    "#美西时间 2025-05-21 04:05把WCT加到了2500U\n",
    "#美西时间 2025-05-21 03:27把PARTI加到了2500U\n",
    "#美西时间 2025-05-21 15:19把JELLYJELLY加到了5000U\n",
    "#美西时间 2025-05-23 14:41把MOODENGUSDT加到了5000U\n",
    "#美西时间 2025-05-23 14:44把VIRTUAL加到了5000U\n",
    "#美西时间 2025-05-23 14:49把WCT从2500U加到了5000U\n",
    "#美西时间 2025-05-25 18:03把SOL加到了5000U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(result_df[result_df.ticker.isin(['GOAT','ETHFI','KAITO','VINE','SONIC','WCT','IMX','JELLYJELLY','MOODENG'])]['total_pnl'].sum()/100000/4)*365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本金映射\n",
    "result_df = result_df[result_df.ticker.isin(['GOAT','ETHFI','KAITO','VINE','SONIC','WCT','IMX','JELLYJELLY'])]\n",
    "capital_dict = {\n",
    "    'KAITO': 10000,\n",
    "    'GOAT': 10000,\n",
    "    'ETHFI': 10000,\n",
    "    'SONIC': 10000,\n",
    "    'VINE': 10000,\n",
    "    'JELLYJELLY': 10000,\n",
    "    'WCT': 5000,\n",
    "    'IMX': 5000\n",
    "}\n",
    "\n",
    "# 添加本金列\n",
    "result_df['capital'] = result_df['ticker'].map(capital_dict)\n",
    "\n",
    "# 年化收益率计算\n",
    "result_df['annualized_return'] = (result_df['total_pnl'] / result_df['capital']) * (365 / result_df['total_days'])\n",
    "\n",
    "# 权重\n",
    "result_df['weight'] = result_df['capital'] / result_df['capital'].sum()\n",
    "\n",
    "# 组合年化收益率\n",
    "portfolio_annualized_return = (result_df['annualized_return'] * result_df['weight']).sum()\n",
    "\n",
    "# 假设10w本金下的年化收益\n",
    "annualized_return_on_100k = portfolio_annualized_return * 100000\n",
    "\n",
    "print(f\"组合年化收益率为: {portfolio_annualized_return:.2%}\")\n",
    "print(f\"在10万元本金下的年化收益为: {annualized_return_on_100k:.2f} U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df.ticker.isin(['GOAT','ETHFI','KAITO','VINE','SONIC','WCT','IMX','JELLYJELLY'])]['Total Ret Rate'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(result_df[result_df.ticker.isin(['GOAT','ETHFI','KAITO','VINE','SONIC','WCT','IMX','JELLYJELLY'])]['total_pnl'].sum()/(5000+10000+10000+5000+10000+10000+10000))*365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df.ticker.isin(['GOAT','ETHFI','KAITO','VINE','SONIC','WCT','IMX'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 滑点数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pandas.errors import EmptyDataError\n",
    "import pytz\n",
    "\n",
    "\n",
    "tickers =  ['BTC','ETH','XRP','SOL','DOGE','TON','ONDO','AAVE','ICP','TRX','XLM','IMX','TRUMP','BNB','ADA','SUI','LINK','AVAX','HBAR','BCH','DOT','LTC','APT','UNI','ETC', 'RENDER', 'POL', 'ATOM', 'ALGO', 'FIL', 'TIA', 'TAO','SONIC','VIRTUAL','MOVE', 'ETHFI' ,'KAITO' ,'MOODENG' ,'VINE' ,'GOAT','PARTI','FARTCOIN','GRASS','JELLYJELLY','EOS', 'NEIRO', 'OP', 'PENGU', 'PEOPLE', 'PNUT', 'WIF', 'WLD','ME','ALCH','WAL','ICX','VANA','NEIROETH','T','BABY','AGLD','SSV','PLUME','NMR','WCT']\n",
    "data_dir = '/Users/rayxu/Downloads'\n",
    "records = []\n",
    "\n",
    "cutoff_time_utc = pd.Timestamp('2025-05-06 14:00:00')\n",
    "\n",
    "start_date = pd.Timestamp('2025-05-22 04:00:00')  # 只保留这之后的数据\n",
    "start_date = pd.Timestamp('2025-05-13 04:30:00')  # 只保留这之后的数据\n",
    "start_date = pd.Timestamp('2025-06-01 15:30:00')  # 只保留这之后的数据\n",
    "end_date = pd.Timestamp('2025-06-15 15:30:00')  # 只保留这之前的数据\n",
    "def load_and_concat(data_dir, ticker, kind, dropna_subset=None):\n",
    "    \"\"\"\n",
    "    读取并合并：\n",
    "      '{ticker}_USDT.{kind}.csv' 和 '{ticker}_USDT.{kind} (1).csv'。\n",
    "    逻辑：\n",
    "      1. 对每个路径，存在则尝试 pd.read_csv：\n",
    "         - 如果抛 EmptyDataError，认为是空文件，跳过；\n",
    "         - 否则读出 DF，再做 dropna（如果指定），再判断 df.empty。\n",
    "      2. 收集所有非空 df：\n",
    "         - 若列表为空，返回 None；\n",
    "         - 若只有 1 个，reset_index 后返回它；\n",
    "         - 若有 2 个，pd.concat 后返回。\n",
    "    \"\"\"\n",
    "    paths = [\n",
    "        os.path.join(data_dir, f\"{ticker}_USDT.{kind}.csv\"),\n",
    "        os.path.join(data_dir, f\"{ticker}_USDT.{kind} (1).csv\"),\n",
    "        os.path.join(data_dir, f\"{ticker}_USDT.{kind} (2).csv\")\n",
    "    ]\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "        except EmptyDataError:\n",
    "            # 文件存在但无数据，跳过\n",
    "            continue\n",
    "        if dropna_subset:\n",
    "            df = df.dropna(subset=dropna_subset)\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "    if not dfs:\n",
    "        print(f'No {kind} data found for {ticker}')\n",
    "        return None\n",
    "    if len(dfs) == 1:\n",
    "        return dfs[0].reset_index(drop=True)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    \n",
    "def parse_start_time(start_str):\n",
    "    match = re.match(r'^([0-9\\-:.\\s]+)\\s\\+0800', str(start_str))\n",
    "    if match:\n",
    "        time_str = match.group(1)\n",
    "        try:\n",
    "            return pd.Timestamp(time_str)\n",
    "        except Exception:\n",
    "            return pd.NaT\n",
    "    else:\n",
    "        return pd.NaT\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        df_open = load_and_concat(data_dir, ticker, \"open\",   dropna_subset=['pos_id'])\n",
    "        df_close = load_and_concat(data_dir, ticker, \"close\",  dropna_subset=['pos_ids'])\n",
    "        df_funding = load_and_concat(data_dir, ticker, \"funding\")\n",
    "\n",
    "        if (df_close is None) and (df_open is None):\n",
    "            print(f'Missing open and close data for {ticker}, skipping...')\n",
    "            continue\n",
    "\n",
    "        df_open['time'] = df_open['start'].apply(parse_start_time)\n",
    "        df_close['time'] = df_close['start'].apply(parse_start_time)\n",
    "\n",
    "        \n",
    "        df_open = df_open[(df_open['time'] >= start_date) & (df_open['time'] <= end_date)]\n",
    "        df_close = df_close[(df_close['time'] >= start_date) & (df_close['time'] <= end_date)]\n",
    "        \n",
    "        df_open['sign_short'] = np.where(df_open['swap1_side'] == 'buy', 1, 0)\n",
    "        df_close['sign_short'] = np.where(df_close['swap1_side'] == 'closeShort', 1, 0)\n",
    "        short_slippage = pd.concat([(df_open['sr_open_real']  - df_open['sr_open'])[df_open['sign_short'] == 1], (df_close['sr_close_real'] - df_close['sr_close'])[df_close['sign_short'] == 1]], ignore_index=True)\n",
    "\n",
    "        df_open['sign_long'] = np.where(df_open['swap1_side'] == 'sell', 1, 0)\n",
    "        df_close['sign_long'] = np.where(df_close['swap1_side'] == 'closeLong', 1, 0)\n",
    "        long_slippage = pd.concat([(df_open['sr_open']  - df_open['sr_open_real'])[df_open['sign_long'] == 1], (df_close['sr_close'] - df_close['sr_close_real'])[df_close['sign_long'] == 1]], ignore_index=True)\n",
    "\n",
    "        overall_slippage = pd.concat([short_slippage, long_slippage], ignore_index=True)\n",
    "        # clean_long_slippage  = remove_outliers_zscore(long_slippage,  z_thresh=2.0)\n",
    "        # clean_short_slippage  = remove_outliers_zscore(short_slippage,  z_thresh=2.0)\n",
    "\n",
    "        # clean_overall_slippage  = remove_outliers_zscore(overall_slippage,  z_thresh=2.0)\n",
    "\n",
    "\n",
    "        stats = {\n",
    "            'ticker': ticker,\n",
    "            'short_slippage_mean': overall_slippage.mean(),\n",
    "            'short_slippage_std': short_slippage.std(),\n",
    "            'short_slippage_max': short_slippage.min(),\n",
    "            'short_slippage_min': short_slippage.max(),\n",
    "\n",
    "            'long_slippage_mean': overall_slippage.mean(),\n",
    "            'long_slippage_std': long_slippage.std(),\n",
    "            'long_slippage_max': long_slippage.min(),\n",
    "            'long_slippage_min': long_slippage.max(),\n",
    "\n",
    "\n",
    "            'overall_slippage_mean': overall_slippage.mean(),\n",
    "            'overall_slippage_std': overall_slippage.std(),\n",
    "            'overall_slippage_max': overall_slippage.min(),\n",
    "            'overall_slippage_min': overall_slippage.max(),\n",
    "\n",
    "            'open_count': len(df_open),\n",
    "            'close_count': len(df_close),\n",
    "            'total_clean_count':len(overall_slippage)\n",
    "        }\n",
    "\n",
    "        records.append(stats)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] Error: {e}\")\n",
    "\n",
    "result_df = pd.DataFrame(records)\n",
    "result_df.to_csv('/Users/rayxu/Desktop/Obentech/slippage_summary_06_15.csv', index=False)\n",
    "result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticker = 'JELLYJELLY'\n",
    "\n",
    "# start_date = pd.Timestamp('2025-05-19 02:30:00') \n",
    "# end_date = pd.Timestamp('2025-05-21 16:10:00')  # 只保留这之前的数据\n",
    "\n",
    "# df_open = load_and_concat(data_dir, ticker, \"open\",   dropna_subset=['pos_id'])\n",
    "# df_close = load_and_concat(data_dir, ticker, \"close\",  dropna_subset=['pos_ids'])\n",
    "# df_funding = load_and_concat(data_dir, ticker, \"funding\")\n",
    "\n",
    "# if df_close is None:\n",
    "#     print(f'Missing data for {ticker}, skipping...')\n",
    "\n",
    "# if ticker == 'KAITO':  start_date = pd.Timestamp('2025-05-17 13:25:00')  # 针对KAITO\n",
    "# if ticker == 'ETHFI':  start_date = pd.Timestamp('2025-05-19 14:20:00')  # 针对ETHFI\n",
    "# if ticker == 'GOAT':  start_date = pd.Timestamp('2025-05-19 15:50:00')  # 针对KAITO\n",
    "# if ticker == 'SONIC':  start_date = pd.Timestamp('2025-05-20 04:38:00')  # 针对SONIC\n",
    "# if ticker == 'IMX':  start_date = pd.Timestamp('2025-05-20 17:20:00')  # 针对SONIC\n",
    "# # funding_path = os.path.join(data_dir, f'{ticker}_USDT.funding (1).csv')\n",
    "# if df_funding is not None:\n",
    "#     df_funding['ts'] = pd.to_datetime(df_funding['ts'])\n",
    "#     df_funding = df_funding[(df_funding['ts'] >= start_date)&(df_funding['ts']  <= end_date)]\n",
    "#     funding_pnl = df_funding['funding'].sum()\n",
    "#     df_funding['ts_rounded'] = df_funding['ts'].dt.floor('1s')\n",
    "#     funding_net = df_funding.groupby('ts_rounded')['funding'].sum().reset_index()\n",
    "#     funding_pnl_series = funding_net.set_index('ts_rounded')['funding'].cumsum()\n",
    "\n",
    "# else:\n",
    "#     funding_pnl = 0\n",
    "\n",
    "\n",
    "\n",
    "# df_open['time'] = df_open['start'].apply(parse_start_time)\n",
    "# df_close['time'] = df_close['start'].apply(parse_start_time)\n",
    "\n",
    "\n",
    "# df_open = df_open[(df_open['time'] >= start_date) & (df_open['time'] <= end_date)]\n",
    "# df_close = df_close[(df_close['time'] >= start_date) & (df_close['time'] <= end_date)]\n",
    "# if df_close.empty:\n",
    "#     print(f'No open or close data for {ticker}, skipping...')\n",
    "\n",
    "\n",
    "# s = df_funding['ts'].iloc[0]\n",
    "# if df_open.empty:\n",
    "#     s2 = pd.NaT\n",
    "# else:\n",
    "#     s2 = df_open['time'].iloc[0]\n",
    "\n",
    "\n",
    "# e = df_funding['ts'].iloc[-1]\n",
    "# e2 = df_close['time'].iloc[-1]\n",
    "# e3 = end_date\n",
    "# # s_main = s.split(' m=')[0].replace(' CST', '').split(' +')[0]     # -> '2025-05-06 16:00:36.204209674 +0800' -》'2025-05-06 16:00:36.204209674'\n",
    "# # s = max(pd.to_datetime(s), start_date)                      \n",
    "# # e = min(pd.to_datetime(e), end_date)\n",
    "# if ticker == 'JELLYJELLY': print(s,s2)   \n",
    "# s = min(pd.to_datetime(s), s2)   \n",
    "        \n",
    "# e = max(pd.to_datetime(e), e2,e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_funding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最新数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pandas.errors import EmptyDataError\n",
    "import pytz\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "tickers =  ['BTC','ETH','XRP','SOL','DOGE','TON','ONDO','AAVE','ICP','TRX','XLM','IMX','TRUMP','BNB','ADA','SUI','LINK','AVAX','HBAR','BCH','DOT','LTC','APT','UNI','ETC', 'RENDER', 'POL', 'ATOM', 'ALGO', 'FIL', 'TIA', 'TAO','SONIC','VIRTUAL','MOVE', 'ETHFI' ,'KAITO' ,'MOODENG' ,'VINE' ,'GOAT','PARTI','FARTCOIN','GRASS','JELLYJELLY','EOS', 'NEIRO', 'OP', 'PENGU', 'PEOPLE', 'PNUT', 'WIF', 'WLD','ME','ALCH','WAL','ICX','VANA','NEIROETH','T','BABY','AGLD','SSV','PLUME','NMR','JST','ANIME','IOST','UMA','WCT']\n",
    "\n",
    "data_dir = '/data/vhosts/cf_dc/manager_maker_dc_pmpro_test/app/'\n",
    "records = []\n",
    "\n",
    "cutoff_time_utc = pd.Timestamp('2025-05-06 14:00:00')\n",
    "\n",
    "\n",
    "\n",
    "start_date_real = pd.Timestamp('2025-05-17 13:25:00')  # KATIO的时间点\n",
    "\n",
    "start_date_real = pd.Timestamp('2025-05-13 10:20:00')   #计算滑点的时间点\n",
    "start_date_real = pd.Timestamp('2025-05-22 06:20:00') \n",
    "# start_date_real = pd.Timestamp('2025-06-09 04:50:00') \n",
    "# end_date = pd.Timestamp('2025-05-20 04:30:00')  # 只保留这之前的数据\n",
    "end_date = pd.Timestamp('2025-06-10 04:50:00')  # 只保留这之前的数据\n",
    "\n",
    "\n",
    "# 为了对应回测曲线\n",
    "# start_date_real = pd.Timestamp('2025-05-21 00:00:00') \n",
    "# end_date = pd.Timestamp('2025-05-21 23:59:59')  \n",
    "\n",
    "\n",
    "def load_and_concat(data_dir, ticker, kind, dropna_subset=None):\n",
    "    \"\"\"\n",
    "    读取并合并：\n",
    "      '{ticker}_USDT.{kind}.csv' 和 '{ticker}_USDT.{kind} (1).csv'。\n",
    "    逻辑：\n",
    "      1. 对每个路径，存在则尝试 pd.read_csv：\n",
    "         - 如果抛 EmptyDataError，认为是空文件，跳过；\n",
    "         - 否则读出 DF，再做 dropna（如果指定），再判断 df.empty。\n",
    "      2. 收集所有非空 df：\n",
    "         - 若列表为空，返回 None；\n",
    "         - 若只有 1 个，reset_index 后返回它；\n",
    "         - 若有 2 个，pd.concat 后返回。\n",
    "    \"\"\"\n",
    "    paths = [\n",
    "        os.path.join(data_dir, f\"{ticker}_USDT.{kind}.csv\"),\n",
    "        os.path.join(data_dir, f\"{ticker}_USDT.{kind} (1).csv\"),\n",
    "        os.path.join(data_dir, f\"{ticker}_USDT.{kind} (2).csv\")\n",
    "    ]\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "        except EmptyDataError:\n",
    "            # 文件存在但无数据，跳过\n",
    "            continue\n",
    "        if dropna_subset:\n",
    "            df = df.dropna(subset=dropna_subset)\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "    if not dfs:\n",
    "        print(f'No {kind} data found for {ticker}')\n",
    "        return None\n",
    "    if len(dfs) == 1:\n",
    "        return dfs[0].reset_index(drop=True)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def remove_outliers_zscore(\n",
    "        s: pd.Series,\n",
    "        z_thresh: float = 3.0,\n",
    "        winsorize: bool = False\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Remove (or optionally winsor‑ize) extreme values from a Series\n",
    "    using the z‑score rule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : pd.Series\n",
    "        Input Series (long_slippage or short_slippage).\n",
    "    z_thresh : float, default 3.0\n",
    "        Absolute z‑score above which a point is considered an outlier.\n",
    "    winsorize : bool, default False\n",
    "        • False  → drop the outliers  \n",
    "        • True   → clamp outliers to ±z_thresh·σ instead of dropping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Outlier‑filtered (or winsor‑ized) Series, index preserved.\n",
    "    \"\"\"\n",
    "    if s.empty:\n",
    "        return s.copy()\n",
    "\n",
    "    mean, std = s.mean(), s.std(ddof=0)\n",
    "    if std == 0:            # avoid division‑by‑zero on degenerate data\n",
    "        return s.copy()\n",
    "\n",
    "    z_scores = (s - mean) / std\n",
    "    mask = z_scores.abs() <= z_thresh\n",
    "\n",
    "    if winsorize:\n",
    "        lower_bound = mean - z_thresh * std\n",
    "        upper_bound = mean + z_thresh * std\n",
    "        return s.clip(lower_bound, upper_bound)\n",
    "    else:\n",
    "        return s[mask]\n",
    "\n",
    "# ---------------- Example ----------------\n",
    "# clean_long  = remove_outliers_zscore(long_slippage,  z_thresh=3.0)\n",
    "# clean_short = remove_outliers_zscore(short_slippage, z_thresh=3.0)\n",
    "\n",
    "    \n",
    "def parse_start_time(start_str):\n",
    "    match = re.match(r'^([0-9\\-:.\\s]+)\\s\\+0800', str(start_str))\n",
    "    if match:\n",
    "        time_str = match.group(1)\n",
    "        try:\n",
    "            return pd.Timestamp(time_str)\n",
    "        except Exception:\n",
    "            return pd.NaT\n",
    "    else:\n",
    "        return pd.NaT\n",
    "    \n",
    "\n",
    "portfolio_cum_pnl = pd.Series(dtype=float)\n",
    "portfolio_trade_pnl = pd.Series(dtype=float)\n",
    "portfolio_funding_pnl = pd.Series(dtype=float)\n",
    "\n",
    "\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "\n",
    "        denominator = (\n",
    "            600 if ticker in ['ehhe']\n",
    "            else 1000 if ticker in ['XLM','ETH']\n",
    "            else 2000 if ticker in ['ME','ALCH','ICX','VANA','NEIROETH','T','BABY','AGLD','SSV','NMR','ICX']\n",
    "            else 10000 if ticker in ['KAITO','GOAT','ETHFI','SONIC','VINE','JELLYJELLY','MOODENG','VIRTUAL','MOVE','TRUMP','WCT','SOL','NEIRO']\n",
    "            else 5000 if ticker in ['PARTI','IMX','ATOM','WAL']\n",
    "            else 200\n",
    "        )\n",
    "\n",
    "        start_date = start_date_real\n",
    "\n",
    "        df_open = load_and_concat(data_dir, ticker, \"open\",   dropna_subset=['pos_id'])\n",
    "        df_close = load_and_concat(data_dir, ticker, \"close\",  dropna_subset=['pos_ids'])\n",
    "        df_funding = load_and_concat(data_dir, ticker, \"funding\")\n",
    "        if df_open is None or df_open.empty:\n",
    "            print(f'Missing open data for {ticker}, skipping...')\n",
    "            continue\n",
    "        # if df_close is None:\n",
    "        #     print(f'Missing data for {ticker}, skipping...')\n",
    "        #     continue\n",
    "        if ticker == 'KAITO':  start_date = pd.Timestamp('2025-05-17 13:25:00')  # 针对KAITO\n",
    "        if ticker == 'ETHFI':  start_date = pd.Timestamp('2025-05-19 14:20:00')  # 针对ETHFI\n",
    "        if ticker == 'GOAT':  start_date = pd.Timestamp('2025-05-19 15:50:00')  # 针对GOAT\n",
    "        if ticker == 'SONIC':  start_date = pd.Timestamp('2025-05-20 04:38:00')  # 针对SONIC\n",
    "        if ticker == 'IMX':  start_date = pd.Timestamp('2025-05-20 17:20:00')  # 针对IMX\n",
    "        if ticker == 'VINE':  start_date = pd.Timestamp('2025-05-21 18:27:00')  \n",
    "        if ticker == 'PARTI':  start_date = pd.Timestamp('2025-05-21 18:32:00')  \n",
    "        if ticker == 'WCT':  \n",
    "            start_date = pd.Timestamp('2025-05-21 19:05:00')  \n",
    "            end_date = pd.Timestamp('2025-06-04 04:40:00')\n",
    "        if ticker == 'JELLYJELLY':  start_date = pd.Timestamp('2025-05-22 06:20:00')  \n",
    "        if ticker == 'MOODENG':  start_date = pd.Timestamp('2025-05-24 05:44:00')  \n",
    "        if ticker == 'VIRTUAL':  start_date = pd.Timestamp('2025-05-24 05:44:00')  \n",
    "\n",
    "        start_date = start_date_real\n",
    "\n",
    "        if df_funding is not None:\n",
    "            df_funding['ts'] = pd.to_datetime(df_funding['ts'])\n",
    "            df_funding = df_funding[(df_funding['ts'] >= start_date)&(df_funding['ts']  <= end_date)]\n",
    "            funding_pnl = df_funding['funding'].sum()\n",
    "            df_funding['ts_rounded'] = df_funding['ts'].dt.floor('1s')\n",
    "            funding_net = df_funding.groupby('ts_rounded')['funding'].sum().reset_index()\n",
    "            funding_pnl_series = funding_net.set_index('ts_rounded')['funding'].cumsum()\n",
    "\n",
    "        else:\n",
    "            funding_pnl = 0\n",
    "\n",
    "\n",
    "        df_open['time'] = df_open['start'].apply(parse_start_time)\n",
    "        df_open = df_open[(df_open['time'] >= start_date) & (df_open['time'] <= end_date)]\n",
    "\n",
    "        s = df_funding['ts'].iloc[0]\n",
    "        if df_open.empty:\n",
    "            s2 = pd.NaT\n",
    "        else:\n",
    "            s2 = df_open['time'].iloc[0]\n",
    "        \n",
    "\n",
    "        if df_close is not None and not df_close.empty:\n",
    "\n",
    "            df_close['time'] = df_close['start'].apply(parse_start_time)\n",
    "            df_close = df_close[(df_close['time'] >= start_date) & (df_close['time'] <= end_date)]\n",
    "            e = df_funding['ts'].iloc[-1]\n",
    "            e2 = df_close['time'].iloc[-1]\n",
    "            e3 = end_date\n",
    "            s = min(pd.to_datetime(s), s2)   \n",
    "            e = max(pd.to_datetime(e), e2,e3)\n",
    "            total_days = (e-s).total_seconds() / 86400\n",
    "\n",
    "            df_open['sign_short'] = np.where(df_open['swap1_side'] == 'buy', 1, 0)\n",
    "            df_close['sign_short'] = np.where(df_close['swap1_side'] == 'closeShort', 1, 0)\n",
    "            short_slippage = pd.concat([(df_open['sr_open_real']  - df_open['sr_open'])[df_open['sign_short'] == 1], (df_close['sr_close_real'] - df_close['sr_close'])[df_close['sign_short'] == 1]], ignore_index=True)\n",
    "\n",
    "\n",
    "            df_open['sign_long'] = np.where(df_open['swap1_side'] == 'sell', 1, 0)\n",
    "            df_close['sign_long'] = np.where(df_close['swap1_side'] == 'closeLong', 1, 0)\n",
    "            long_slippage = pd.concat([(df_open['sr_open']  - df_open['sr_open_real'])[df_open['sign_long'] == 1], (df_close['sr_close'] - df_close['sr_close_real'])[df_close['sign_long'] == 1]], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "            mask_after_cutoff = df_close['time'] >= cutoff_time_utc\n",
    "            earn_after = df_close.loc[mask_after_cutoff, 'real_earn']\n",
    "            earn_before = (df_close.loc[~mask_after_cutoff, 'real_earn'] + 0.00003 * 4)\n",
    "            total_earn_series = pd.concat([earn_before,earn_after]).sort_index()\n",
    "            total_earn = total_earn_series.sum()\n",
    "\n",
    "            spread_pnl = total_earn - df_close['funding1_real'].sum() - df_close['funding2_real'].sum()\n",
    "\n",
    "            spread_series = total_earn_series - df_close['funding1_real'] - df_close['funding2_real']\n",
    "\n",
    "            spread_winning_rate = spread_series[spread_series > 0].count() / len(spread_series) if len(spread_series) > 0 else np.nan\n",
    "        \n",
    "\n",
    "\n",
    "            trade_pnl = (total_earn_series - df_close['funding1_real'] - df_close['funding2_real']).cumsum()\n",
    "\n",
    "\n",
    "\n",
    "            # total_winning_rate = df_close['real_earn'].sum() / df_close['real_earn'].count() if len(df_close) > 0 else np.nan\n",
    "            df_close = df_close.set_index('time')\n",
    "            trade_pnl.index = df_close.index\n",
    "            cum_pnl = df_close['real_earn'].cumsum()-df_close['funding1_real'].cumsum() - df_close['funding2_real'].cumsum()\n",
    "\n",
    "            # funding_pnl_aligned = funding_pnl_series.reindex(cum_pnl.index, method='ffill')\n",
    "            # cum_pnl = cum_pnl + funding_pnl_aligned\n",
    "\n",
    "            # 先获取两个索引的并集（按时间排序）\n",
    "            combined_index = cum_pnl.index.union(funding_pnl_series.index)\n",
    "\n",
    "            # 将两个Series都reindex到合并后的索引（统一填充方式）\n",
    "            cum_pnl_aligned = cum_pnl.reindex(combined_index, method='ffill')\n",
    "            cum_pnl_aligned.fillna(0, inplace=True)\n",
    "            funding_pnl_aligned = funding_pnl_series.reindex(combined_index, method='ffill')\n",
    "            funding_pnl_aligned.fillna(0, inplace=True)\n",
    "            cum_pnl_combined = cum_pnl_aligned + funding_pnl_aligned\n",
    "\n",
    "            stats = {\n",
    "                'ticker': ticker,\n",
    "                'short_slippage_mean': clean_short.mean(),\n",
    "                'short_slippage_std': short_slippage.std(),\n",
    "                'short_slippage_max': short_slippage.min(),\n",
    "                'short_slippage_min': short_slippage.max(),\n",
    "                'short_count': len(clean_short),\n",
    "\n",
    "                'long_slippage_mean': clean_long.mean(),\n",
    "                'long_slippage_std': long_slippage.std(),\n",
    "                'long_slippage_max': long_slippage.min(),\n",
    "                'long_slippage_min': long_slippage.max(),\n",
    "                'long_count': len(clean_long),\n",
    "\n",
    "                'spread_winning_rate':spread_winning_rate,\n",
    "                'spread_pnl': spread_pnl,\n",
    "                'funding_pnl': funding_pnl,\n",
    "                'total_pnl': spread_pnl + funding_pnl,\n",
    "                'Total Ret Rate': 365* (spread_pnl + funding_pnl)/(denominator*total_days),\n",
    "                'Spread Ret Rate': 365*spread_pnl/(denominator*total_days),\n",
    "                'Funding Ret Rate': 365*funding_pnl/(denominator*total_days),\n",
    "                'close_count': len(df_close),\n",
    "                'daily_turnover':(len(df_close)/(0.5*denominator/(df_close['deposit1_real'].mean()+df_close['deposit2_real'].mean())))/total_days,\n",
    "                'total_days':total_days\n",
    "            }\n",
    "\n",
    "        else: # 没有close数据 ,但是有funding的数据\n",
    "            print(f'{ticker} has no close data, using only funding curve.')\n",
    "            e = df_funding['ts'].iloc[-1]\n",
    "            e3 = end_date\n",
    "            s = min(pd.to_datetime(s), s2)   \n",
    "            e = max(pd.to_datetime(e), e3)\n",
    "            total_days = (e-s).total_seconds() / 86400\n",
    "            df_open['sign_short'] = np.where(df_open['swap1_side'] == 'buy', 1, 0)\n",
    "\n",
    "            short_slippage = (df_open['sr_open_real']  - df_open['sr_open'])[df_open['sign_short'] == 1]\n",
    "\n",
    "\n",
    "            df_open['sign_long'] = np.where(df_open['swap1_side'] == 'sell', 1, 0)\n",
    "            long_slippage = (df_open['sr_open']  - df_open['sr_open_real'])[df_open['sign_long'] == 1]\n",
    "\n",
    "            spread_pnl = 0\n",
    "            spread_winning_rate = np.nan\n",
    "        \n",
    "            trade_pnl = pd.Series(dtype=float)\n",
    "            # trade_pnl.index = df_close.index\n",
    "            cum_pnl = pd.Series(dtype=float)\n",
    "\n",
    "            # 先获取两个索引的并集（按时间排序）\n",
    "            combined_index = cum_pnl.index.union(funding_pnl_series.index)\n",
    "\n",
    "            # 将两个Series都reindex到合并后的索引（统一填充方式）\n",
    "            cum_pnl_aligned = cum_pnl.reindex(combined_index, method='ffill')\n",
    "            cum_pnl_aligned.fillna(0, inplace=True)\n",
    "            funding_pnl_aligned = funding_pnl_series.reindex(combined_index, method='ffill')\n",
    "            funding_pnl_aligned.fillna(0, inplace=True)\n",
    "            cum_pnl_combined = funding_pnl_aligned\n",
    "\n",
    "\n",
    "            stats = {\n",
    "                'ticker': ticker,\n",
    "                'short_slippage_mean': clean_short.mean(),\n",
    "                'short_slippage_std': short_slippage.std(),\n",
    "                'short_slippage_max': short_slippage.min(),\n",
    "                'short_slippage_min': short_slippage.max(),\n",
    "                'short_count': len(clean_short),\n",
    "\n",
    "                'long_slippage_mean': clean_long.mean(),\n",
    "                'long_slippage_std': long_slippage.std(),\n",
    "                'long_slippage_max': long_slippage.min(),\n",
    "                'long_slippage_min': long_slippage.max(),\n",
    "                'long_count': len(clean_long),\n",
    "\n",
    "                'spread_winning_rate':spread_winning_rate,\n",
    "                'spread_pnl': spread_pnl,\n",
    "                'funding_pnl': funding_pnl,\n",
    "                'total_pnl': spread_pnl + funding_pnl,\n",
    "                'Total Ret Rate': 365* (spread_pnl + funding_pnl)/(denominator*total_days),\n",
    "                'Spread Ret Rate': 365*spread_pnl/(denominator*total_days),\n",
    "                'Funding Ret Rate': 365*funding_pnl/(denominator*total_days),\n",
    "                'open_count': len(df_open),\n",
    "                'daily_turnover':(len(df_open)/(0.5*denominator/(df_open['deposit1_real'].mean()+df_open['deposit2_real'].mean())))/total_days,\n",
    "                'total_days':total_days\n",
    "            }\n",
    "\n",
    "        clean_long  = remove_outliers_zscore(long_slippage,  z_thresh=3.0)\n",
    "        clean_short = remove_outliers_zscore(short_slippage, z_thresh=3.0)\n",
    "\n",
    "\n",
    "\n",
    "        records.append(stats)\n",
    "\n",
    "\n",
    "        # union index + forward fill + fillna(0)\n",
    "        combined_index = cum_pnl_combined.index.union(portfolio_cum_pnl.index)\n",
    "        portfolio_cum_pnl = (\n",
    "            portfolio_cum_pnl.reindex(combined_index, method='ffill').fillna(0)\n",
    "            + cum_pnl_combined.reindex(combined_index, method='ffill').fillna(0)\n",
    "        )\n",
    "\n",
    "        portfolio_trade_pnl = (\n",
    "            portfolio_trade_pnl.reindex(combined_index, method='ffill').fillna(0)\n",
    "            + trade_pnl.reindex(combined_index, method='ffill').fillna(0)\n",
    "        )\n",
    "\n",
    "        portfolio_funding_pnl = (\n",
    "            portfolio_funding_pnl.reindex(combined_index, method='ffill').fillna(0)\n",
    "            + funding_pnl_series.reindex(combined_index, method='ffill').fillna(0)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        import plotly.graph_objects as go\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # 1. 总盈亏（含 funding + real_earn - 手动费用）\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=cum_pnl_combined.index, y=cum_pnl_combined.values,\n",
    "            mode='lines+markers',\n",
    "            name='Cumulative PnL',\n",
    "            line=dict(color='royalblue'),\n",
    "            fill='tozeroy',\n",
    "            fillcolor='rgba(65,105,225,0.1)'\n",
    "        ))\n",
    "\n",
    "        # 2. funding 累计盈亏\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=funding_pnl_series.index, y=funding_pnl_series.values,\n",
    "            mode='lines+markers',\n",
    "            name='Funding PnL',\n",
    "            line=dict(dash='dash', color='indianred')\n",
    "        ))\n",
    "\n",
    "        # 3. trade PnL（仅 real_earn）\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=trade_pnl.index, y=trade_pnl.values,\n",
    "            mode='lines+markers',\n",
    "            name='Trade PnL',\n",
    "            line=dict(color='deeppink')\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{ticker} Cumulative PnL from {s} to {e}\",\n",
    "            xaxis_title='Time (Asia/Shanghai)',\n",
    "            yaxis_title='PnL',\n",
    "            legend=dict(font=dict(size=12)),\n",
    "            template='plotly_white',\n",
    "            height=700,\n",
    "            width=1400\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] Error: {e}\")\n",
    "# Portfolio 总体收益图\n",
    "        \n",
    "\n",
    "\n",
    "initial_capital = 100000\n",
    "start_time = portfolio_cum_pnl.index.min()\n",
    "end_time   = portfolio_cum_pnl.index.max()\n",
    "\n",
    "total_days = (end_time - start_time).total_seconds() / 86400\n",
    "total_ret  = portfolio_cum_pnl.iloc[-1]\n",
    "spread_ret = portfolio_trade_pnl.iloc[-1]\n",
    "funding_ret = portfolio_funding_pnl.iloc[-1]\n",
    "\n",
    "total_ret_rate = 365 * total_ret / initial_capital / total_days\n",
    "spread_ret_rate = 365 * spread_ret / initial_capital / total_days\n",
    "funding_ret_rate = 365 * funding_ret / initial_capital / total_days\n",
    "\n",
    "start_str = start_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "end_str = end_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "fig_portfolio = go.Figure()\n",
    "\n",
    "fig_portfolio.add_trace(go.Scatter(\n",
    "    x=portfolio_cum_pnl.index, y=portfolio_cum_pnl.values,\n",
    "    mode='lines',\n",
    "    name='Portfolio Total PnL',\n",
    "    line=dict(color='royalblue'),\n",
    "    fill='tozeroy',\n",
    "    fillcolor='rgba(65,105,225,0.1)'\n",
    "))\n",
    "\n",
    "fig_portfolio.add_trace(go.Scatter(\n",
    "    x=portfolio_trade_pnl.index, y=portfolio_trade_pnl.values,\n",
    "    mode='lines',\n",
    "    name='Portfolio Trade PnL',\n",
    "    line=dict(color='deeppink')\n",
    "))\n",
    "\n",
    "fig_portfolio.add_trace(go.Scatter(\n",
    "    x=portfolio_funding_pnl.index, y=portfolio_funding_pnl.values,\n",
    "    mode='lines',\n",
    "    name='Portfolio Funding PnL',\n",
    "    line=dict(dash='dash', color='indianred')\n",
    "))\n",
    "\n",
    "fig_portfolio.update_layout(\n",
    "    # title=f'Total Portfolio Cumulative PnL from {start_str} to {end_str}, total days: {total_days}. Total annualized Return Rates: {total_ret_rate:.2%} , {spread_ret_rate:.2%} (Spread), {funding_ret_rate:.2%}(Funding)',\n",
    "    title=(\n",
    "        f\"Portfolio PnL from {start_str} to {end_str} ({total_days:.2f} days)  \\n\"\n",
    "        f\"Annualized Return: {total_ret_rate:.2%} | Spread: {spread_ret_rate:.2%} | Funding: {funding_ret_rate:.2%}\"\n",
    "    ),\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='PnL',\n",
    "    template='plotly_white',\n",
    "    legend=dict(font=dict(size=12)),\n",
    "    height=700,\n",
    "    width=1400\n",
    ")\n",
    "\n",
    "fig_portfolio.show()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Portfolio Total Return: {total_ret:.2f} USDT\")\n",
    "print(f\"Portfolio Holding Days: {total_days:.2f} days\")\n",
    "print(f\"Annualized Total Return Rate:   {total_ret_rate:.2%}\")\n",
    "print(f\"Annualized Spread Return Rate:  {spread_ret_rate:.2%}\")\n",
    "print(f\"Annualized Funding Return Rate: {funding_ret_rate:.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(records).sort_values(by = 'Total Ret Rate', ascending=False)\n",
    "result_df\n",
    "# result_df.to_csv('slippage_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 给净值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = symbol_pnl_dict_pmtest2['portfolio']['cum_pnl_combined']\n",
    "env = 'pmtest2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index().rename(columns = {0:'NetValue','index':'date'})\n",
    "df['NetValue'] += PORTFOLIO_CONFIG[env]['total_capital']\n",
    "df['Nav'] = df['NetValue'] / PORTFOLIO_CONFIG[env]['total_capital']\n",
    "df.set_index('date').resample('1D').last()['Nav'].dropna()\n",
    "# df.set_index('date').resample('1D').last()['Nav'].dropna().to_excel(f'/Volumes/T7/Obentech/nav/portfolio_nav_{env}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_pnl_dict_pmtest2['portfolio']['cum_pnl_combined']\n",
    "plot_symbol_and_portfolio_in_period(symbol_pnl_dict_pmtest2,ticker, start_date,end_date,type='pmtest2',figure_type = 'plotly',show_portfolio=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
